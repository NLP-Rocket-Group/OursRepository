{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "HANEmotionAnalyze_balance_600000.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-CIbt4LqMiS",
    "colab_type": "text"
   },
   "source": [
    "## 开始"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zBDjArJoT65A",
    "colab_type": "code",
    "outputId": "1c4bdb86-a297-4add-c9fa-3426c4352807",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588175646051,
     "user_tz": -480,
     "elapsed": 23935,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "pycharm": {
     "is_executing": false
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    }
   },
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd /content/drive/My\\ Drive/Colab Notebooks/OursRepository/public-opinion-monitor\n",
    "# \n",
    "# !pip install thulac\n",
    "# !pip install jieba"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "9Eush51XTobh",
    "colab_type": "code",
    "outputId": "6c71ba20-2bfe-43ac-d043-f2c6b71cbdcf",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588175661641,
     "user_tz": -480,
     "elapsed": 4680,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    }
   },
   "source": [
    "from ClassicalHANModel import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import numpy\n",
    "import time, math\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import thulac\n",
    "thulac = thulac.thulac()\n",
    "import jieba"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Model loaded succeed\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "DUFj-U3PTobn",
    "colab_type": "code",
    "outputId": "ef5771e1-b6e3-4128-e340-72db7c4ddfd4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588175667529,
     "user_tz": -480,
     "elapsed": 1796,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    }
   },
   "source": [
    "# import torch.functional as F\n",
    "# \n",
    "embedding = nn.Embedding(10, 3)\n",
    "input = torch.LongTensor([[0, 1,2,4,5],[1, 4,3,2,9]])\n",
    "print(embedding(input))\n",
    "embedding2 = nn.Embedding(10, 3, padding_idx=2)\n",
    "# input = torch.LongTensor([[1,2,0,5, 6,7,8,9]])\n",
    "print(embedding2(input))\n",
    "weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n",
    "embedding3 = nn.Embedding.from_pretrained(weight)\n",
    "input = torch.LongTensor([1])\n",
    "print(embedding3(input))\n",
    "input = torch.LongTensor([0])\n",
    "print(embedding3(input))\n",
    "\n"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([[[-0.4567, -0.2409, -0.8837],\n",
      "         [-0.5701, -0.9491, -2.7943],\n",
      "         [ 1.5182, -0.5346, -0.6047],\n",
      "         [-0.0546, -0.0513, -1.0322],\n",
      "         [-0.5860,  0.0557, -1.3539]],\n",
      "\n",
      "        [[-0.5701, -0.9491, -2.7943],\n",
      "         [-0.0546, -0.0513, -1.0322],\n",
      "         [-1.1007, -1.0440,  0.1437],\n",
      "         [ 1.5182, -0.5346, -0.6047],\n",
      "         [ 1.7443,  0.5313, -0.0597]]], grad_fn=<EmbeddingBackward>)\n",
      "tensor([[[-0.7053,  1.0738, -0.8784],\n",
      "         [-0.5785,  1.6240, -0.4461],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [-0.4914,  0.4058, -2.5656],\n",
      "         [-0.4140,  0.6332,  0.9174]],\n",
      "\n",
      "        [[-0.5785,  1.6240, -0.4461],\n",
      "         [-0.4914,  0.4058, -2.5656],\n",
      "         [-2.1385,  1.1311, -0.5590],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [-0.5943,  0.0329,  0.3941]]], grad_fn=<EmbeddingBackward>)\n",
      "tensor([[4.0000, 5.1000, 6.3000]])\n",
      "tensor([[1.0000, 2.3000, 3.0000]])\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "3HmMThKsTobr",
    "colab_type": "text"
   },
   "source": [
    "## 读取词向量\n",
    "建立词语列表"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "hhmz_drpTobs",
    "colab_type": "code",
    "outputId": "7f1f5928-abf0-4f7a-9c5f-f7e22e212828",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588175692243,
     "user_tz": -480,
     "elapsed": 22553,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    }
   },
   "source": [
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "\n",
    "# file = '../../PretrainedData/Tencent_AILab_ChineseEmbedding/Tencent_AILab_ChineseEmbedding.txt'\n",
    "# file = '../../DataSets/Word2Vect/xingrong_50_thulac/word2vect_50_w5.model'\n",
    "# file = '../../DataSets/Word2Vect/xiejunjie_300_jieba/wiki_han_word2vec_300维度.model'\n",
    "file = '../../DataSets/Word2Vect/Tencent_AILab_ChineseEmbedding/Tencent_AILab_ChineseEmbedding_Min.txt'\n",
    "# word2vec = Word2Vec.load(file)\n",
    "word2vec = KeyedVectors.load_word2vec_format(file, binary=False,limit=100000)\n",
    "# word2vec = KeyedVectors.load_word2vec_format(file, binary=False)\n",
    "word2vec.init_sims(replace=True)  # 神奇，很省内存，可以运算most_similar\n",
    "word2vec.vector_size"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "200"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 7
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "_zCpfSvDTocC",
    "colab_type": "code",
    "outputId": "43edc887-48fc-43b5-beed-263a1f34bf89",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588175693369,
     "user_tz": -480,
     "elapsed": 18430,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    }
   },
   "source": [
    "print(word2vec)\n",
    "# print(word2vec.wv.vocab)\n",
    "# print(len(word2vec.index2word))\n",
    "print(len(word2vec.wv.index2word))\n",
    "print(word2vec.wv.index2word[0])\n",
    "print(word2vec.wv.index2word[1])\n",
    "print(word2vec.wv.index2word[2])\n",
    "print(word2vec.wv.index2word[1522])\n",
    "print(word2vec.wv.index2entity[1522])\n",
    "print(word2vec.similar_by_word('中国'))\n",
    "print(word2vec.similar_by_word('天才'))\n",
    "print(word2vec.wv)\n",
    "print('word2vec.wv.vocab ---- >', word2vec.wv.vocab)\n",
    "print(word2vec.wv.index2word)"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n",
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  import sys\n",
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  del sys.path[0]\n",
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "lzs2-uceTocG",
    "colab_type": "code",
    "outputId": "0aa2edbf-8de3-4024-f243-dbe1e71a9e7a",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588175693369,
     "user_tz": -480,
     "elapsed": 6319,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "wordEmbedding = [word2vec.wv[word]  for word in word2vec.wv.index2word]\n",
    "word2index = { word:i for i, word in enumerate(word2vec.wv.index2word)}\n",
    "# print(wordEmbedding[:2])\n",
    "# print(word2index['中国'])\n",
    "# print(word2index['天才'])\n",
    "print(word2index['喜欢'])\n",
    "print(word2index['不'])\n",
    "print(word2index['爽'])\n",
    "print(word2index['超赞'])\n",
    "\n"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "217\n",
      "64\n",
      "5313\n",
      "41047\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "cy9vhIUoTocL",
    "colab_type": "text"
   },
   "source": [
    "## 读取训练数据"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "gZGmiumwTocN",
    "colab_type": "code",
    "outputId": "5f9df97d-0ee8-4e68-b0ba-6b8fdce08086",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588175697789,
     "user_tz": -480,
     "elapsed": 4172,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    }
   },
   "source": [
    "def compute_ngrams(word, num_min = 1, num_max = 3):\n",
    "    ngrams =[]\n",
    "    for ngram_length in range(num_min, min(len(word), num_max) + 1):\n",
    "        for i in range(len(word) - ngram_length + 1):\n",
    "            # print(i, i + ngram_length)\n",
    "            ngrams.append(word[i : i + ngram_length])\n",
    "    # print(ngrams)\n",
    "    return list(set(ngrams))\n",
    "\n",
    "print(compute_ngrams('you'))\n",
    "print(compute_ngrams('I Think'))\n",
    "print(compute_ngrams('中华人民共和国万岁'))"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['u', 'yo', 'y', 'o', 'ou', 'you']\n",
      "['T', 'n', 'i', 'in', ' T', 'hin', 'Thi', ' ', 'k', 'I ', 'Th', 'hi', 'nk', 'ink', 'h', 'I T', ' Th', 'I']\n",
      "['国万', '共和国', '和国', '岁', '民共', '国', '人民共', '华人', '人', '共和', '中', '中华', '华人民', '中华人', '人民', '华', '共', '国万岁', '和国万', '民共和', '民', '万', '万岁', '和']\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "ulsoTSc2TocT",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# 从词向量文本文件 word2vec 中获取词向量，如果获取到直接返回，若没有获取到，那么把这个词拆开\n",
    "# 成为 ngrams 的新词组，并在 word2vec 中找新词组中的词向量并相加取平均，最后得到平均词向量输出\n",
    "def wordVec(word, word2vec, min_n = 1, max_n = 3):\n",
    "    # 确认词向量维度\n",
    "    word_size = word2vec.wv.syn0[0].shape[0]\n",
    "\n",
    "    # 如果在词典之中，直接返回词向量\n",
    "    if word in word2vec.wv.vocab.keys():\n",
    "        return word2vec[word]\n",
    "    else:\n",
    "        # 计算word的ngrams词组\n",
    "        ngrams = compute_ngrams(word, min_n, max_n)\n",
    "        # 不在词典的情况下\n",
    "        word_vec = numpy.zeros(word_size, dtype=numpy.float32)\n",
    "        ngrams_found = 0\n",
    "        ngrams_single = [ng for ng in ngrams if len(ng) == 1]\n",
    "        ngrams_more = [ng for ng in ngrams if len(ng) > 1]\n",
    "        # 先只接受2个单词长度以上的词向量\n",
    "        for ngram in ngrams_more:\n",
    "            if ngram in word2vec.wv.vocab.keys():\n",
    "                word_vec += word2vec[ngram]\n",
    "                ngrams_found += 1\n",
    "                #print(ngram)\n",
    "        # 如果，没有匹配到，那么最后是考虑单个词向量\n",
    "        if ngrams_found == 0:\n",
    "            for ngram in ngrams_single:\n",
    "                word_vec += word2vec[ngram]\n",
    "                ngrams_found += 1\n",
    "        if word_vec.any():\n",
    "            return word_vec / max(1, ngrams_found)\n",
    "        else:\n",
    "            # 不抛出异常，而是打印提示，并返回0向量。\n",
    "            print(KeyError('all ngrams for word %s absent from model' % word))\n",
    "            return word_vec"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "08tJP4HHTocX",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class DotDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        dict.__init__(self, *args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "        "
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "XGUX_N8KToca",
    "colab_type": "code",
    "outputId": "d997a8da-6552-42c3-98b7-10935a89d157",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588175703604,
     "user_tz": -480,
     "elapsed": 4979,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "dic = {'a':[1, 2, 3, 4], 'b':[5, 6, 7, 8],\n",
    "'c':[9, 10, 11, 12], 'd':[13, 14, 15, 16]}\n",
    "df1=pd.DataFrame(dic)\n",
    "print(df1)\n",
    "df2=df1.sample(frac=0.75)\n",
    "print(df2)\n",
    "# rowlist=[]\n",
    "# for indexs in df2.index:\n",
    "#     rowlist.append(indexs)\n",
    "df3=df1.drop(df2.index.to_list(),axis=0)\n",
    "print(df3)"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "   a  b   c   d\n",
      "0  1  5   9  13\n",
      "1  2  6  10  14\n",
      "2  3  7  11  15\n",
      "3  4  8  12  16\n",
      "   a  b   c   d\n",
      "3  4  8  12  16\n",
      "1  2  6  10  14\n",
      "2  3  7  11  15\n",
      "   a  b  c   d\n",
      "0  1  5  9  13\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lfY5Is_4I34d",
    "colab_type": "code",
    "outputId": "0f027ec8-bd9c-490f-8a75-99a88a3d4b2e",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588175704173,
     "user_tz": -480,
     "elapsed": 1811,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "import re\n",
    "\n",
    "def splitText(text:str, splitChar = '(。|，|,|！|\\!|？|\\?|\\n|\\t)'):\n",
    "  '''\n",
    "  句子切分分隔符\n",
    "  '''\n",
    "  contents = re.split(splitChar, text)\n",
    "  contents = [\"\".join([a, b]) if b != '\\n' and b != '\\t' else \"\" #a + \"。\"\n",
    "              for a, b in zip(contents[0::2], contents[1::2])]\n",
    "  contents = [content for content in contents if content.strip() != '' and content[0] != '。']\n",
    "  # for i, sen in enumerate(contents):\n",
    "  #     print(i, sen)\n",
    "  return contents\n",
    "\n",
    "testStr = \"\"\"\n",
    "深圳，简称“深”，别称鹏城，是广东省副省级市、计划单列市、超大城市，国务院批复确定的中国经济特区、全国性经济中心城市和国际化城市 [1]  。截至2018年末，全市下辖9个区，总面积1997.47平方千米，建成区面积927.96平方千米，常住人口1302.66万人，城镇人口1302.66万人，城镇化率100%，是中国第一个全部城镇化的城市。 [2-5] \n",
    "深圳地处中国华南地区、广东南部、珠江口东岸，东临大亚湾和大鹏湾，西濒珠江口和伶仃洋，南隔深圳河与香港相连，是粤港澳大湾区四大中心城市之一 [6]  、国家物流枢纽、国际性综合交通枢纽 [7]  、国际科技产业创新中心 [8]  、中国三大全国性金融中心之一 [9-10]  ，并全力建设中国特色社会主义先行示范区 [11]  、综合性国家科学中心 [12]  、全球海洋中心城市 [13]  。深圳水陆空铁口岸俱全，是中国拥有口岸数量最多、出入境人员最多、车流量最大的口岸城市。 [14]\n",
    "\"\"\"\n",
    "\n",
    "print(splitText(testStr))"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['深圳，', '简称“深”，', '别称鹏城，', '是广东省副省级市、计划单列市、超大城市，', '国务院批复确定的中国经济特区、全国性经济中心城市和国际化城市 [1]  。', '截至2018年末，', '全市下辖9个区，', '总面积1997.47平方千米，', '建成区面积927.96平方千米，', '常住人口1302.66万人，', '城镇人口1302.66万人，', '城镇化率100%，', '是中国第一个全部城镇化的城市。', '深圳地处中国华南地区、广东南部、珠江口东岸，', '东临大亚湾和大鹏湾，', '西濒珠江口和伶仃洋，', '南隔深圳河与香港相连，', '是粤港澳大湾区四大中心城市之一 [6]  、国家物流枢纽、国际性综合交通枢纽 [7]  、国际科技产业创新中心 [8]  、中国三大全国性金融中心之一 [9-10]  ，', '并全力建设中国特色社会主义先行示范区 [11]  、综合性国家科学中心 [12]  、全球海洋中心城市 [13]  。', '深圳水陆空铁口岸俱全，', '是中国拥有口岸数量最多、出入境人员最多、车流量最大的口岸城市。']\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "1oqKk8CETocd",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def isNan(a):\n",
    "    return a != a\n",
    "\n",
    "\n",
    "class RatingData(data.Dataset):\n",
    "    def __init__(self, path, \n",
    "          word2index, \n",
    "          max_row = -1, \n",
    "          num_word_in_sentence = 10,\n",
    "          num_sentence_in_article = 10,\n",
    "          trainTestRate = 0.85, \n",
    "          isTrain = True, \n",
    "          wordCuter = thulac,\n",
    "          clean_file_name = 'ratings_clean_4_HAN.csv',\n",
    "          ):\n",
    "        self.token_list = []\n",
    "        self.label_list = []\n",
    "        # self.token_positions = torch.tensor([i for i in range(100)])\n",
    "\n",
    "        print(' balance_data.csv 所在path:',path) # 地址不应该包含 ratings.csv\n",
    "\n",
    "        ratings_clean_filename = os.path.join(path, clean_file_name)\n",
    "        ratings_filename = os.path.join(path, 'balance_data_means_600000.csv')\n",
    "        if os.path.isfile(ratings_clean_filename):\n",
    "            clean_pd = pd.read_csv(ratings_clean_filename)\n",
    "        else:\n",
    "            print('没有找到缓存的文件%s, 读取源文件%s'%(ratings_clean_filename, ratings_filename))\n",
    "            ratings_pd = pd.read_csv(ratings_filename)\n",
    "            print('开始生成缓存文件%s'%(ratings_clean_filename))\n",
    "            clean_pd = pd.DataFrame({\n",
    "                'labels':[],\n",
    "                'data':[],\n",
    "            })\n",
    "            nonRatingCount = 0\n",
    "            for i, row in ratings_pd.iterrows():\n",
    "                if max_row != -1 and i > max_row:\n",
    "                    break\n",
    "                if not isinstance(row['data'], str) or row['data'] == '':\n",
    "                    # print(i + 1, row['comment'])\n",
    "                    nonRatingCount += 1\n",
    "                    continue\n",
    "                r0 = row['labels']\n",
    "                if r0 == -1:\n",
    "                    r0 = 2\n",
    "                    \n",
    "                if i % 10000 == 9999:\n",
    "                    print(i + 1, r0)\n",
    "\n",
    "                # 把文章切割成句子\n",
    "                sentences = splitText(row['data'])\n",
    "                tokens = []\n",
    "                for i in range(num_sentence_in_article):\n",
    "                  if i < len(sentences):\n",
    "                    # 把句子切割成词\n",
    "                    words = list(wordCuter.cut(sentences[i]))\n",
    "                    # 把词转成词向量中的index，不足时补全\n",
    "                    token = [ word2index[words[j]] if j < len(words) and words[j] in word2index else 0 \n",
    "                          for j in range(num_word_in_sentence)] \n",
    "                    tokens += token\n",
    "                  else:\n",
    "                    # 不足时补全\n",
    "                    tokens += [0] * num_word_in_sentence\n",
    "                \n",
    "                newRow = DotDict()\n",
    "\n",
    "                newRow.labels = [r0]\n",
    "                newRow.data = [json.dumps(tokens)]\n",
    "\n",
    "                clean_pd = clean_pd.append(pd.DataFrame(newRow), ignore_index=True)\n",
    "            print('空的评论数量： %d'%(nonRatingCount))\n",
    "            clean_pd.to_csv(ratings_clean_filename)\n",
    "\n",
    "        # 读取\n",
    "        if isTrain:\n",
    "            temp_pd = clean_pd.sample(frac=trainTestRate)\n",
    "        else:\n",
    "            temp_pd = clean_pd.sample(frac=trainTestRate)\n",
    "            temp_pd = clean_pd.drop(temp_pd.index.tolist(), axis=0)\n",
    "\n",
    "        for i, row in temp_pd.iterrows():\n",
    "            if max_row != -1 and i > max_row:\n",
    "                break\n",
    "\n",
    "            self.label_list.append(torch.tensor(row['labels']).long())\n",
    "            self.token_list.append(torch.from_numpy(numpy.array( json.loads(row['data']) ) ).long())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.token_list[index], self.label_list[index]#, self.token_positions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_list)\n",
    "    "
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      " balance_data.csv 所在path: ../../DataSets/yf_dianping\n",
      " balance_data.csv 所在path: ../../DataSets/yf_dianping\n",
      "443310\n",
      "110828\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "ratingData = RatingData('../../DataSets/yf_dianping',\n",
    "                             word2index = word2index,\n",
    "                            #  max_row= 5,\n",
    "                             isTrain=True,\n",
    "                             trainTestRate = 0.8,\n",
    "                             wordCuter= jieba,\n",
    "                             clean_file_name='balance_data_4_HAN_means_600000.csv',\n",
    "                             )\n",
    "trainLoader = torch.utils.data.DataLoader(dataset=ratingData,\n",
    "                                          batch_size=128,\n",
    "                                          shuffle = True,\n",
    "                                          # num_workers = 8,\n",
    "                                          )\n",
    "ratingData2 = RatingData('../../DataSets/yf_dianping',\n",
    "                             word2index = word2index,\n",
    "                             # max_row= 200000,\n",
    "                             isTrain=False,\n",
    "                             trainTestRate = 0.8,\n",
    "                             wordCuter= jieba,\n",
    "                             clean_file_name='balance_data_4_HAN_means_600000.csv',\n",
    "                             )\n",
    "testLoader = torch.utils.data.DataLoader(dataset=ratingData2,\n",
    "                                          batch_size=128,\n",
    "                                          shuffle = True,\n",
    "                                          # num_workers = 8,\n",
    "                                          )\n",
    "print(len(ratingData.label_list))\n",
    "print(len(ratingData2.label_list))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxwtXncXqESB",
    "colab_type": "text"
   },
   "source": [
    "## 创建模型\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "pQYgCURnTocn",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "54e89b88-efab-46e2-c4dd-ae4c3d776e96",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588176212969,
     "user_tz": -480,
     "elapsed": 5397,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    }
   },
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, num_input, num_hidden):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.W = nn.Linear(num_input, num_hidden)\n",
    "        self.U = nn.Linear(num_hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = F.tanh(self.W(x))\n",
    "        a = F.softmax(self.U(u), dim=1)\n",
    "        return torch.mul(a, x).sum(dim=1)\n",
    "\n",
    "\n",
    "class HAN(nn.Module):\n",
    "    def __init__(self, num_embeddings = 5845,\n",
    "                 num_classes = 10,\n",
    "                 num_words = 20,        # 每句话最多多少个词\n",
    "                 num_sentence = 10,     # 一篇文章多少个句子\n",
    "                 embedding_dim = 200,\n",
    "                 hidden_size_gru = 50,\n",
    "                 hidden_size_att = 100,\n",
    "                 ):\n",
    "        super(HAN, self).__init__()\n",
    "\n",
    "        self.num_words = num_words\n",
    "        self.num_sentence = num_sentence\n",
    "        self.embed = nn.Embedding(num_embeddings, embedding_dim, 0)\n",
    "\n",
    "        self.GRU1 = nn.GRU(embedding_dim,\n",
    "                           hidden_size_gru,\n",
    "                           bidirectional=True,  # 双向  Default: ``False``\n",
    "                           batch_first=True,    # : If ``True``, then the input and output tensors are provided as (batch, seq, feature). Default: ``False``\n",
    "                           )\n",
    "        self.self_attention1 = SelfAttention(hidden_size_gru * 2, hidden_size_att)\n",
    "\n",
    "\n",
    "        self.GRU2 = nn.GRU(hidden_size_gru * 2,\n",
    "                           hidden_size_gru,\n",
    "                           bidirectional=True,  # 双向  Default: ``False``\n",
    "                           batch_first=True,    # : If ``True``, then the input and output tensors are provided as (batch, seq, feature). Default: ``False``\n",
    "                           )\n",
    "        self.self_attention2 = SelfAttention(hidden_size_gru * 2, hidden_size_att)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size_att, num_classes)\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        # print(\"x:\", x.shape, self.num_words)\n",
    "        sentences = []\n",
    "\n",
    "        for i in range(self.num_sentence):\n",
    "            sentence = x[:, i * self.num_words: (i + 1) * self.num_words]\n",
    "            # print(\"view 后 x:\", x.shape)\n",
    "            sentence = self.embed(sentence)\n",
    "            # print(\"embed 后 x:\", x.shape)\n",
    "            sentence, _ = self.GRU1(sentence)\n",
    "            # print(\"GRU1 后 x:\", x.shape)\n",
    "            sentence = self.self_attention1(sentence)\n",
    "            # print(\"self_attention1 后 x:\", x.shape)\n",
    "            sentences.append(sentence)\n",
    "        sentences = torch.cat(sentences)\n",
    "\n",
    "        x = sentences.view(sentences.size(0) // self.num_sentence, self.num_words, -1)\n",
    "        # print(\"view2 后 x:\", x.shape)\n",
    "        x, _ = self.GRU2(x)\n",
    "        # print(\"GRU2 后 x:\", x.shape)\n",
    "        x = self.self_attention2(x)\n",
    "        # print(\"self_attention2 后 x:\", x.shape)\n",
    "        x = self.fc(x)\n",
    "        # print(\"fc 后 x:\", x.shape)\n",
    "        return F.softmax(x, dim=1)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "HAN(\n",
      "  (embed): Embedding(99999, 200, padding_idx=0)\n",
      "  (GRU1): GRU(200, 200, batch_first=True, bidirectional=True)\n",
      "  (self_attention1): SelfAttention(\n",
      "    (W): Linear(in_features=400, out_features=400, bias=True)\n",
      "    (U): Linear(in_features=400, out_features=1, bias=True)\n",
      "  )\n",
      "  (GRU2): GRU(400, 200, batch_first=True, bidirectional=True)\n",
      "  (self_attention2): SelfAttention(\n",
      "    (W): Linear(in_features=400, out_features=400, bias=True)\n",
      "    (U): Linear(in_features=400, out_features=1, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=400, out_features=3, bias=True)\n",
      ")\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "Embedding(99999, 200)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 28
    }
   ],
   "source": [
    "\n",
    "#创建模型\n",
    "wordEmbedding = torch.FloatTensor(wordEmbedding)\n",
    "num_embeddings = len(word2vec.wv.index2word)\n",
    "model = HAN(num_embeddings, \n",
    "            num_classes = 3,\n",
    "            embedding_dim = word2vec.wv.vector_size, \n",
    "            num_words = 10,\n",
    "            num_sentence = 10,\n",
    "            hidden_size_gru = 200,\n",
    "            hidden_size_att = 400,\n",
    "            )\n",
    "print(model)\n",
    "\n",
    "modelParams = model.parameters()\n",
    "for param in modelParams:\n",
    "    if len(param.data.shape) > 1:\n",
    "        # print('---', param.data.shape, param.data)\n",
    "        torch.nn.init.kaiming_normal(param.data)\n",
    "        # print('--->', param.data)\n",
    "        \n",
    "model.embed.from_pretrained(wordEmbedding)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 开始训练过程"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def trainOneEpoch(epoch, model:HAN, trainLoader, optimizer:Optimizer, lossFunc):\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        lossFunc = lossFunc.cuda()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    startTime = time.time()\n",
    "    for i, (x, y) in enumerate(trainLoader):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        outputs = model(x)\n",
    "        loss = lossFunc(outputs, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 99:\n",
    "            print('Epoch %d, %d/%d, loss:%f ' % (epoch, i, len(trainLoader), loss))\n",
    "        # if i > 2000:\n",
    "        #     break \n",
    "    print('Epoch %d cost time: %.3fs' % (epoch, time.time() - startTime))\n",
    "\n",
    "\n",
    "def testModel(epoch, model:HAN, testLoader):\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    startTime = time.time()\n",
    "    for i, (x, y) in enumerate(testLoader):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        outputs = model(x)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += len(y)\n",
    "        correct += predicted.data.eq(y.data).cpu().sum().numpy()\n",
    "        \n",
    "\n",
    "        if i % 50 == 49:\n",
    "            print('Epoch Test %d, %d/%d, \\npredicted:%s, \\ntarget:%s' % (epoch, i, len(testLoader), \n",
    "                                                                     str(predicted),\n",
    "                                                                     str(y)))\n",
    "        # if i > 2000:\n",
    "        #     break \n",
    "    print('Epoch Test %d cost time: %.3fs' % (epoch, time.time() - startTime))\n",
    "    print('准确率： %.3f' % (correct / total))\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def train(epoch, model, modelSavePath, isLoad, lr=0.0002):\n",
    "    last_acc = 0\n",
    "    if isLoad: \n",
    "        model.load_state_dict(torch.load(modelSavePath))\n",
    "        last_acc = testModel(epoch, model, testLoader)\n",
    "    # optimizer=torch.optim.SGD(model.parameters(), momentum=0.9, lr=0.001)\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.0002)\n",
    "    lossFunc =torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(epoch):\n",
    "        trainOneEpoch(epoch, model, trainLoader, optimizer, lossFunc)\n",
    "        acc = testModel(epoch, model, testLoader)\n",
    "        if last_acc < acc:\n",
    "          torch.save(model.state_dict(), modelSavePath)\n",
    "        last_acc = acc\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-d9d9d87bc9da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'EmotionAnalyzeModelData_ClassicalHAN.model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-99fe22fc6218>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch, model, modelSavePath, isLoad, lr)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[0mlossFunc\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mtrainOneEpoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainLoader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlossFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestLoader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlast_acc\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-99fe22fc6218>\u001b[0m in \u001b[0;36mtrainOneEpoch\u001b[1;34m(epoch, model, trainLoader, optimizer, lossFunc)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlossFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 4.00 GiB total capacity; 2.63 GiB already allocated; 52.75 MiB free; 2.76 GiB reserved in total by PyTorch)"
     ],
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 4.00 GiB total capacity; 2.63 GiB already allocated; 52.75 MiB free; 2.76 GiB reserved in total by PyTorch)",
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "train(10, model, 'EmotionAnalyzeModelData_ClassicalHAN.model', False, lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gzvump0AVdJY",
    "colab_type": "code",
    "pycharm": {
     "is_executing": false
    },
    "colab": {}
   },
   "source": [
    "\n",
    "def eval(model, modelSavePath, isLoad = True):\n",
    "    if isLoad: model.load_state_dict(torch.load(modelSavePath))\n",
    "    testModel(0, model, testLoader)\n",
    "\n",
    "eval(model, 'EmotionAnalyzeModelData_300_600.model', isLoad = True)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "wKXjE4UBTodM",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "lossFunc =torch.nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "_, predicted = torch.max(input.data, 1)\n",
    "output = lossFunc(input, target)\n",
    "print('input',input, '\\n target',target, '\\n output', output)\n",
    "output = lossFunc(input, predicted)\n",
    "print('predicted',predicted, '\\n target',target, '\\n output', output)\n",
    "\n",
    "print(predicted.data.eq(target).cpu().sum())\n",
    "print(target.data.eq(predicted).cpu().sum())"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "Z1zQWh6DTodQ",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# 何凯明初始化\n",
    "\n",
    "w = torch.Tensor(3, 5, 2)\n",
    "print(w)\n",
    "print(nn.init.kaiming_uniform(w))\n",
    "print(w)\n",
    "w = torch.Tensor(3, 5, 2)\n",
    "print(w)\n",
    "print(torch.nn.init.kaiming_normal(w))\n",
    "print(w)"
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}