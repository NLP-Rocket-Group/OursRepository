{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "Why.ipynb",
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "CuLODj5PUNKT",
    "colab_type": "code",
    "outputId": "c95ca14a-58f6-445d-ee7a-c65f79b68406",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1587106954242,
     "user_tz": -480,
     "elapsed": 20960,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    }
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/My\\ Drive/Colab Notebooks/OursRepository/public-opinion-monitor"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n",
      "/content/drive/My Drive/Colab Notebooks/OursRepository/public-opinion-monitor\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wTq2D_C1UKpK",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([3, 10]) tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n",
      "        [10., 11., 12., 13., 14., 15., 16., 17., 18., 19.],\n",
      "        [20., 21., 22., 23., 24., 25., 26., 27., 28., 29.]])\n",
      "li Linear(in_features=10, out_features=5, bias=True)\n",
      "pre torch.Size([3, 5]) tensor([[ 0.4324,  1.1549,  3.4924, -0.8478,  4.0849],\n",
      "        [ 0.1247,  1.1175,  7.9667, -5.1108, 12.1356],\n",
      "        [-0.1830,  1.0800, 12.4411, -9.3739, 20.1863]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "x1 torch.Size([3, 1, 10]) tensor([[[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.]],\n",
      "\n",
      "        [[10., 11., 12., 13., 14., 15., 16., 17., 18., 19.]],\n",
      "\n",
      "        [[20., 21., 22., 23., 24., 25., 26., 27., 28., 29.]]])\n",
      "pre torch.Size([3, 1, 5]) tensor([[[ 0.4324,  1.1549,  3.4924, -0.8478,  4.0849]],\n",
      "\n",
      "        [[ 0.1247,  1.1175,  7.9667, -5.1108, 12.1356]],\n",
      "\n",
      "        [[-0.1830,  1.0800, 12.4411, -9.3739, 20.1863]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "x torch.Size([3, 5, 10]) tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],\n",
      "         [ 10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.],\n",
      "         [ 20.,  21.,  22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.],\n",
      "         [ 30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,  39.],\n",
      "         [ 40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.]],\n",
      "\n",
      "        [[ 50.,  51.,  52.,  53.,  54.,  55.,  56.,  57.,  58.,  59.],\n",
      "         [ 60.,  61.,  62.,  63.,  64.,  65.,  66.,  67.,  68.,  69.],\n",
      "         [ 70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,  78.,  79.],\n",
      "         [ 80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,  88.,  89.],\n",
      "         [ 90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,  99.]],\n",
      "\n",
      "        [[100., 101., 102., 103., 104., 105., 106., 107., 108., 109.],\n",
      "         [110., 111., 112., 113., 114., 115., 116., 117., 118., 119.],\n",
      "         [120., 121., 122., 123., 124., 125., 126., 127., 128., 129.],\n",
      "         [130., 131., 132., 133., 134., 135., 136., 137., 138., 139.],\n",
      "         [140., 141., 142., 143., 144., 145., 146., 147., 148., 149.]]])\n",
      "pre torch.Size([3, 5, 5]) tensor([[[  0.4324,   1.1549,   3.4924,  -0.8478,   4.0849],\n",
      "         [  0.1247,   1.1175,   7.9667,  -5.1108,  12.1356],\n",
      "         [ -0.1830,   1.0800,  12.4411,  -9.3739,  20.1863],\n",
      "         [ -0.4908,   1.0426,  16.9155, -13.6369,  28.2370],\n",
      "         [ -0.7985,   1.0052,  21.3899, -17.8999,  36.2877]],\n",
      "\n",
      "        [[ -1.1063,   0.9678,  25.8643, -22.1629,  44.3383],\n",
      "         [ -1.4140,   0.9303,  30.3387, -26.4259,  52.3890],\n",
      "         [ -1.7217,   0.8929,  34.8131, -30.6890,  60.4397],\n",
      "         [ -2.0295,   0.8555,  39.2875, -34.9520,  68.4904],\n",
      "         [ -2.3372,   0.8181,  43.7619, -39.2150,  76.5411]],\n",
      "\n",
      "        [[ -2.6450,   0.7807,  48.2362, -43.4780,  84.5918],\n",
      "         [ -2.9527,   0.7432,  52.7106, -47.7410,  92.6424],\n",
      "         [ -3.2604,   0.7058,  57.1850, -52.0041, 100.6931],\n",
      "         [ -3.5682,   0.6684,  61.6594, -56.2671, 108.7438],\n",
      "         [ -3.8759,   0.6310,  66.1338, -60.5301, 116.7945]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 测试模型输入维度的有效性\n",
    "x = torch.Tensor(range(0,30)).reshape(3, -1)\n",
    "print('x',x.shape, x)\n",
    "li = nn.Linear(10,5)\n",
    "print('li',li)\n",
    "pre = li(x)\n",
    "print('pre', pre.shape, pre)\n",
    "x1 = x.unsqueeze(1)\n",
    "print('x1',x1.shape, x1)\n",
    "pre = li(x1)\n",
    "print('pre', pre.shape, pre)\n",
    "print()\n",
    "\n",
    "x = torch.Tensor(range(0,150)).reshape(3, -1, 10)\n",
    "print('x',x.shape, x)\n",
    "pre = li(x)\n",
    "print('pre', pre.shape, pre)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([3, 4, 5]) tensor([[[ 0.,  1.,  2.,  3.,  4.],\n",
      "         [ 5.,  6.,  7.,  8.,  9.],\n",
      "         [10., 11., 12., 13., 14.],\n",
      "         [15., 16., 17., 18., 19.]],\n",
      "\n",
      "        [[20., 21., 22., 23., 24.],\n",
      "         [25., 26., 27., 28., 29.],\n",
      "         [30., 31., 32., 33., 34.],\n",
      "         [35., 36., 37., 38., 39.]],\n",
      "\n",
      "        [[40., 41., 42., 43., 44.],\n",
      "         [45., 46., 47., 48., 49.],\n",
      "         [50., 51., 52., 53., 54.],\n",
      "         [55., 56., 57., 58., 59.]]])\n",
      "F.softmax(x,dim=0) tensor([[[4.2484e-18, 4.2484e-18, 4.2484e-18, 4.2484e-18, 4.2484e-18],\n",
      "         [4.2484e-18, 4.2484e-18, 4.2484e-18, 4.2484e-18, 4.2484e-18],\n",
      "         [4.2484e-18, 4.2484e-18, 4.2484e-18, 4.2484e-18, 4.2484e-18],\n",
      "         [4.2484e-18, 4.2484e-18, 4.2484e-18, 4.2484e-18, 4.2484e-18]],\n",
      "\n",
      "        [[2.0612e-09, 2.0612e-09, 2.0612e-09, 2.0612e-09, 2.0612e-09],\n",
      "         [2.0612e-09, 2.0612e-09, 2.0612e-09, 2.0612e-09, 2.0612e-09],\n",
      "         [2.0612e-09, 2.0612e-09, 2.0612e-09, 2.0612e-09, 2.0612e-09],\n",
      "         [2.0612e-09, 2.0612e-09, 2.0612e-09, 2.0612e-09, 2.0612e-09]],\n",
      "\n",
      "        [[1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]]]) torch.Size([3, 4, 5])\n",
      "F.softmax(x,dim=1) tensor([[[3.0384e-07, 3.0384e-07, 3.0384e-07, 3.0384e-07, 3.0384e-07],\n",
      "         [4.5094e-05, 4.5094e-05, 4.5094e-05, 4.5094e-05, 4.5094e-05],\n",
      "         [6.6925e-03, 6.6925e-03, 6.6925e-03, 6.6925e-03, 6.6925e-03],\n",
      "         [9.9326e-01, 9.9326e-01, 9.9326e-01, 9.9326e-01, 9.9326e-01]],\n",
      "\n",
      "        [[3.0384e-07, 3.0384e-07, 3.0384e-07, 3.0384e-07, 3.0384e-07],\n",
      "         [4.5094e-05, 4.5094e-05, 4.5094e-05, 4.5094e-05, 4.5094e-05],\n",
      "         [6.6925e-03, 6.6925e-03, 6.6925e-03, 6.6925e-03, 6.6925e-03],\n",
      "         [9.9326e-01, 9.9326e-01, 9.9326e-01, 9.9326e-01, 9.9326e-01]],\n",
      "\n",
      "        [[3.0384e-07, 3.0384e-07, 3.0384e-07, 3.0384e-07, 3.0384e-07],\n",
      "         [4.5094e-05, 4.5094e-05, 4.5094e-05, 4.5094e-05, 4.5094e-05],\n",
      "         [6.6925e-03, 6.6925e-03, 6.6925e-03, 6.6925e-03, 6.6925e-03],\n",
      "         [9.9326e-01, 9.9326e-01, 9.9326e-01, 9.9326e-01, 9.9326e-01]]]) torch.Size([3, 4, 5])\n",
      "F.softmax(x,dim=2) tensor([[[0.0117, 0.0317, 0.0861, 0.2341, 0.6364],\n",
      "         [0.0117, 0.0317, 0.0861, 0.2341, 0.6364],\n",
      "         [0.0117, 0.0317, 0.0861, 0.2341, 0.6364],\n",
      "         [0.0117, 0.0317, 0.0861, 0.2341, 0.6364]],\n",
      "\n",
      "        [[0.0117, 0.0317, 0.0861, 0.2341, 0.6364],\n",
      "         [0.0117, 0.0317, 0.0861, 0.2341, 0.6364],\n",
      "         [0.0117, 0.0317, 0.0861, 0.2341, 0.6364],\n",
      "         [0.0117, 0.0317, 0.0861, 0.2341, 0.6364]],\n",
      "\n",
      "        [[0.0117, 0.0317, 0.0861, 0.2341, 0.6364],\n",
      "         [0.0117, 0.0317, 0.0861, 0.2341, 0.6364],\n",
      "         [0.0117, 0.0317, 0.0861, 0.2341, 0.6364],\n",
      "         [0.0117, 0.0317, 0.0861, 0.2341, 0.6364]]]) torch.Size([3, 4, 5])\n",
      "\n",
      "x torch.Size([3, 1, 20]) tensor([[[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "          14., 15., 16., 17., 18., 19.]],\n",
      "\n",
      "        [[20., 21., 22., 23., 24., 25., 26., 27., 28., 29., 30., 31., 32., 33.,\n",
      "          34., 35., 36., 37., 38., 39.]],\n",
      "\n",
      "        [[40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51., 52., 53.,\n",
      "          54., 55., 56., 57., 58., 59.]]])\n",
      "F.softmax(x,dim=0) tensor([[[4.2484e-18, 4.2484e-18, 4.2484e-18, 4.2484e-18, 4.2484e-18,\n",
      "          4.2484e-18, 4.2484e-18, 4.2484e-18, 4.2484e-18, 4.2484e-18,\n",
      "          4.2484e-18, 4.2484e-18, 4.2484e-18, 4.2484e-18, 4.2484e-18,\n",
      "          4.2484e-18, 4.2484e-18, 4.2484e-18, 4.2484e-18, 4.2484e-18]],\n",
      "\n",
      "        [[2.0612e-09, 2.0612e-09, 2.0612e-09, 2.0612e-09, 2.0612e-09,\n",
      "          2.0612e-09, 2.0612e-09, 2.0612e-09, 2.0612e-09, 2.0612e-09,\n",
      "          2.0612e-09, 2.0612e-09, 2.0612e-09, 2.0612e-09, 2.0612e-09,\n",
      "          2.0612e-09, 2.0612e-09, 2.0612e-09, 2.0612e-09, 2.0612e-09]],\n",
      "\n",
      "        [[1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]]]) torch.Size([3, 1, 20])\n",
      "F.softmax(x,dim=1) tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1.]]]) torch.Size([3, 1, 20])\n",
      "F.softmax(x,dim=2) tensor([[[3.5416e-09, 9.6272e-09, 2.6169e-08, 7.1136e-08, 1.9337e-07,\n",
      "          5.2563e-07, 1.4288e-06, 3.8839e-06, 1.0557e-05, 2.8698e-05,\n",
      "          7.8010e-05, 2.1205e-04, 5.7642e-04, 1.5669e-03, 4.2592e-03,\n",
      "          1.1578e-02, 3.1471e-02, 8.5548e-02, 2.3254e-01, 6.3212e-01]],\n",
      "\n",
      "        [[3.5416e-09, 9.6272e-09, 2.6169e-08, 7.1136e-08, 1.9337e-07,\n",
      "          5.2563e-07, 1.4288e-06, 3.8839e-06, 1.0557e-05, 2.8698e-05,\n",
      "          7.8010e-05, 2.1205e-04, 5.7642e-04, 1.5669e-03, 4.2592e-03,\n",
      "          1.1578e-02, 3.1471e-02, 8.5548e-02, 2.3254e-01, 6.3212e-01]],\n",
      "\n",
      "        [[3.5416e-09, 9.6272e-09, 2.6169e-08, 7.1136e-08, 1.9337e-07,\n",
      "          5.2563e-07, 1.4288e-06, 3.8839e-06, 1.0557e-05, 2.8698e-05,\n",
      "          7.8010e-05, 2.1205e-04, 5.7642e-04, 1.5669e-03, 4.2592e-03,\n",
      "          1.1578e-02, 3.1471e-02, 8.5548e-02, 2.3254e-01, 6.3212e-01]]]) torch.Size([3, 1, 20])\n"
     ]
    }
   ],
   "source": [
    "# 测试softmax对指定维度的计算效果\n",
    "\n",
    "x = torch.Tensor(range(0,60)).reshape(3, 4, -1)\n",
    "print(\"x\", x.shape, x)\n",
    "print(\"F.softmax(x,dim=0)\", F.softmax(x,dim=0), F.softmax(x,dim=0).size())\n",
    "print(\"F.softmax(x,dim=1)\", F.softmax(x,dim=1), F.softmax(x,dim=1).size())\n",
    "print(\"F.softmax(x,dim=2)\", F.softmax(x,dim=2), F.softmax(x,dim=1).size())\n",
    "print()\n",
    "x = torch.Tensor(range(0,60)).reshape(3, 1, -1)\n",
    "print(\"x\", x.shape, x)\n",
    "print(\"F.softmax(x,dim=0)\", F.softmax(x,dim=0), F.softmax(x,dim=0).size())\n",
    "print(\"F.softmax(x,dim=1)\", F.softmax(x,dim=1), F.softmax(x,dim=1).size())\n",
    "print(\"F.softmax(x,dim=2)\", F.softmax(x,dim=2), F.softmax(x,dim=1).size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LTGLLcuaVnMi",
    "colab_type": "code",
    "outputId": "d51464f6-a08d-4330-8815-28ba7e34cc92",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1587106968065,
     "user_tz": -480,
     "elapsed": 34765,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 701
    }
   },
   "source": [
    "!pip install transformers"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
      "\r\u001B[K     |▋                               | 10kB 20.0MB/s eta 0:00:01\r\u001B[K     |█▏                              | 20kB 4.3MB/s eta 0:00:01\r\u001B[K     |█▊                              | 30kB 5.5MB/s eta 0:00:01\r\u001B[K     |██▎                             | 40kB 5.7MB/s eta 0:00:01\r\u001B[K     |███                             | 51kB 4.8MB/s eta 0:00:01\r\u001B[K     |███▌                            | 61kB 5.3MB/s eta 0:00:01\r\u001B[K     |████                            | 71kB 5.6MB/s eta 0:00:01\r\u001B[K     |████▋                           | 81kB 6.3MB/s eta 0:00:01\r\u001B[K     |█████▎                          | 92kB 6.6MB/s eta 0:00:01\r\u001B[K     |█████▉                          | 102kB 6.6MB/s eta 0:00:01\r\u001B[K     |██████▍                         | 112kB 6.6MB/s eta 0:00:01\r\u001B[K     |███████                         | 122kB 6.6MB/s eta 0:00:01\r\u001B[K     |███████▋                        | 133kB 6.6MB/s eta 0:00:01\r\u001B[K     |████████▏                       | 143kB 6.6MB/s eta 0:00:01\r\u001B[K     |████████▊                       | 153kB 6.6MB/s eta 0:00:01\r\u001B[K     |█████████▎                      | 163kB 6.6MB/s eta 0:00:01\r\u001B[K     |█████████▉                      | 174kB 6.6MB/s eta 0:00:01\r\u001B[K     |██████████▌                     | 184kB 6.6MB/s eta 0:00:01\r\u001B[K     |███████████                     | 194kB 6.6MB/s eta 0:00:01\r\u001B[K     |███████████▋                    | 204kB 6.6MB/s eta 0:00:01\r\u001B[K     |████████████▏                   | 215kB 6.6MB/s eta 0:00:01\r\u001B[K     |████████████▉                   | 225kB 6.6MB/s eta 0:00:01\r\u001B[K     |█████████████▍                  | 235kB 6.6MB/s eta 0:00:01\r\u001B[K     |██████████████                  | 245kB 6.6MB/s eta 0:00:01\r\u001B[K     |██████████████▌                 | 256kB 6.6MB/s eta 0:00:01\r\u001B[K     |███████████████▏                | 266kB 6.6MB/s eta 0:00:01\r\u001B[K     |███████████████▊                | 276kB 6.6MB/s eta 0:00:01\r\u001B[K     |████████████████▎               | 286kB 6.6MB/s eta 0:00:01\r\u001B[K     |████████████████▉               | 296kB 6.6MB/s eta 0:00:01\r\u001B[K     |█████████████████▍              | 307kB 6.6MB/s eta 0:00:01\r\u001B[K     |██████████████████              | 317kB 6.6MB/s eta 0:00:01\r\u001B[K     |██████████████████▋             | 327kB 6.6MB/s eta 0:00:01\r\u001B[K     |███████████████████▏            | 337kB 6.6MB/s eta 0:00:01\r\u001B[K     |███████████████████▊            | 348kB 6.6MB/s eta 0:00:01\r\u001B[K     |████████████████████▍           | 358kB 6.6MB/s eta 0:00:01\r\u001B[K     |█████████████████████           | 368kB 6.6MB/s eta 0:00:01\r\u001B[K     |█████████████████████▌          | 378kB 6.6MB/s eta 0:00:01\r\u001B[K     |██████████████████████          | 389kB 6.6MB/s eta 0:00:01\r\u001B[K     |██████████████████████▊         | 399kB 6.6MB/s eta 0:00:01\r\u001B[K     |███████████████████████▎        | 409kB 6.6MB/s eta 0:00:01\r\u001B[K     |███████████████████████▉        | 419kB 6.6MB/s eta 0:00:01\r\u001B[K     |████████████████████████▍       | 430kB 6.6MB/s eta 0:00:01\r\u001B[K     |█████████████████████████       | 440kB 6.6MB/s eta 0:00:01\r\u001B[K     |█████████████████████████▋      | 450kB 6.6MB/s eta 0:00:01\r\u001B[K     |██████████████████████████▏     | 460kB 6.6MB/s eta 0:00:01\r\u001B[K     |██████████████████████████▊     | 471kB 6.6MB/s eta 0:00:01\r\u001B[K     |███████████████████████████▎    | 481kB 6.6MB/s eta 0:00:01\r\u001B[K     |████████████████████████████    | 491kB 6.6MB/s eta 0:00:01\r\u001B[K     |████████████████████████████▌   | 501kB 6.6MB/s eta 0:00:01\r\u001B[K     |█████████████████████████████   | 512kB 6.6MB/s eta 0:00:01\r\u001B[K     |█████████████████████████████▋  | 522kB 6.6MB/s eta 0:00:01\r\u001B[K     |██████████████████████████████▎ | 532kB 6.6MB/s eta 0:00:01\r\u001B[K     |██████████████████████████████▉ | 542kB 6.6MB/s eta 0:00:01\r\u001B[K     |███████████████████████████████▍| 552kB 6.6MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 563kB 6.6MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 573kB 6.6MB/s \n",
      "\u001B[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
      "Collecting tokenizers==0.5.2\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
      "\u001B[K     |████████████████████████████████| 3.7MB 9.9MB/s \n",
      "\u001B[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
      "Collecting sacremoses\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n",
      "\u001B[K     |████████████████████████████████| 890kB 43.0MB/s \n",
      "\u001B[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.39)\n",
      "Collecting sentencepiece\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001B[K     |████████████████████████████████| 1.0MB 38.3MB/s \n",
      "\u001B[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.39 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.39)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (2.8.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=4219b94ce5f71b7559dd14d8deca6225f2d83697d7d11d377709cb915afa03d8\n",
      "  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
      "Successfully installed sacremoses-0.0.41 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.8.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "9d3rnisBUKpO",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# import argparse\n",
    "# def get_train_args():\n",
    "#     parser=argparse.ArgumentParser()\n",
    "#     parser.add_argument('--batch_size',type=int,default=10,help = '每批数据的数量')\n",
    "#     parser.add_argument('--nepoch',type=int,default=3,help = '训练的轮次')\n",
    "#     parser.add_argument('--lr',type=float,default=0.001,help = '学习率')\n",
    "#     parser.add_argument('--gpu',type=bool,default=True,help = '是否使用gpu')\n",
    "#     parser.add_argument('--num_workers',type=int,default=2,help='dataloader使用的线程数量')\n",
    "#     parser.add_argument('--num_labels',type=int,default=3,help='分类类数')\n",
    "#     parser.add_argument('--data_path',type=str,default='./data',help='数据路径')\n",
    "#     opt=parser.parse_args()\n",
    "#     print(opt)\n",
    "#     return opt\n",
    "\n",
    "# opt = get_train_args()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "4JUvzavPUKpS",
    "colab_type": "code",
    "outputId": "863f3aa2-6f39-4113-a40d-bb01be1ce61f",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1587106968068,
     "user_tz": -480,
     "elapsed": 34753,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    }
   },
   "source": [
    "class DotDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        dict.__init__(self, *args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "d = DotDict()\n",
    "d.foo = 'bar'\n",
    "d.foo1 = 'bar1'\n",
    "print(d['foo'])\n",
    "print(d.foo)\n",
    "\n",
    "for k,v in d.items():\n",
    "    print(k,v)\n",
    "\n",
    "opt1 = DotDict({'num_labels' : 3})\n",
    "print(opt1.num_labels)\n",
    "\n",
    "opt2 = DotDict(num_labels = 3,)\n",
    "print(opt2.num_labels)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "bar\n",
      "bar\n",
      "foo bar\n",
      "foo1 bar1\n",
      "3\n",
      "3\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "3k028vZlUKpV",
    "colab_type": "code",
    "outputId": "64d3dd21-704c-45ef-978b-91aef4fc2dbd",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1587106970432,
     "user_tz": -480,
     "elapsed": 37108,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    }
   },
   "source": [
    "from transformers import *\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "opt3 = DotDict(\n",
    "    num_labels = 3,\n",
    "    batch_size = 10,\n",
    "    num_workers = 0,\n",
    "    cache_dir = '../../PretrainedData/Transformers/bert-base-chinese',\n",
    "    data_path = 'data',\n",
    "    nepoch = 30,\n",
    "               )\n",
    "\n",
    "print('cuda.is_available',torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  opt3.gpu = True\n",
    "else:\n",
    "  opt3.gpu = False"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "cuda.is_available True\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "FAz05pOAUKpY",
    "colab_type": "code",
    "outputId": "fdec2465-3bc1-403b-df2c-b01ac58ce835",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1587107000748,
     "user_tz": -480,
     "elapsed": 67416,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "# num_labels是分类的类数\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-chinese',\n",
    "                                                         num_labels=opt3.num_labels,\n",
    "                                                         cache_dir = opt3.cache_dir,\n",
    "                                                         )\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese',\n",
    "                                          cache_dir = opt3.cache_dir,)\n",
    "print(tokenizer)\n",
    "\n",
    "if opt3.gpu:\n",
    "  model.cuda()"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json from cache at ../../PretrainedData/Transformers/bert-base-chinese/8a3b1cfe5da58286e12a0f5d7d182b8d6eca88c08e26c332ee3817548cf7e60a.3767c74c8ed285531d04153fe84a0791672aff52f7249b27df341dbce09b8305\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin from cache at ../../PretrainedData/Transformers/bert-base-chinese/b1b5e295889f2d0979ede9a78ad9cb5dc6a0e25ab7f9417b315f0a2c22f4683d.929717ca66a3ba9eb9ec2f85973c6398c54c38a4faa464636a491d7a705f7eb6\n",
      "INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at ../../PretrainedData/Transformers/bert-base-chinese/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "<transformers.tokenization_bert.BertTokenizer object at 0x7f03654fab00>\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "-pR49i_EUKpb",
    "colab_type": "code",
    "outputId": "b8a11092-7463-41db-e14b-d866e05fd158",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1587107006061,
     "user_tz": -480,
     "elapsed": 72722,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "class NewsData(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, is_train=1):\n",
    "        self.data_num = 7346\n",
    "        self.x_list = []\n",
    "        self.y_list = []\n",
    "        self.posi = []\n",
    "\n",
    "        with open(root + '/Train_DataSet.csv', encoding='UTF-8') as f:\n",
    "            for i in range(self.data_num + 1):\n",
    "                line = f.readline()[:-1] + '这是一个中性的数据'\n",
    "\n",
    "                data_one_str = line.split(',')[len(line.split(',')) - 2]\n",
    "                data_two_str = line.split(',')[len(line.split(',')) - 1]\n",
    "\n",
    "                if len(data_one_str) < 6:\n",
    "                    z = len(data_one_str)\n",
    "                    data_one_str = data_one_str + '，' + data_two_str[0:min(200, len(data_two_str))]\n",
    "                else:\n",
    "                    data_one_str = data_one_str\n",
    "                if i == 0:\n",
    "                    continue\n",
    "\n",
    "                word_l = tokenizer.encode(data_one_str, add_special_tokens=False)\n",
    "                if len(word_l) < 100:\n",
    "                    while (len(word_l) != 100):\n",
    "                        word_l.append(0)\n",
    "                else:\n",
    "                    word_l = word_l[0:100]\n",
    "\n",
    "                word_l.append(102)\n",
    "                l = word_l\n",
    "                word_l = [101]\n",
    "                word_l.extend(l)\n",
    "\n",
    "                self.x_list.append(torch.tensor(word_l))\n",
    "\n",
    "                self.posi.append(torch.tensor([i for i in range(102)]))\n",
    "\n",
    "        with open(root + '/Train_DataSet_Label.csv', encoding='UTF-8') as f:\n",
    "            for i in range(self.data_num + 1):\n",
    "                tempStr = f.readline()\n",
    "                if i % 500 == 499:\n",
    "                    print(i + 1, tempStr)\n",
    "                label_one = tempStr[-2]\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                label_one = int(label_one)\n",
    "                self.y_list.append(torch.tensor(label_one))\n",
    "\n",
    "        # 训练集或者是测试集\n",
    "        if is_train == 1:\n",
    "            self.x_list = self.x_list[0:6000]\n",
    "            self.y_list = self.y_list[0:6000]\n",
    "            self.posi = self.posi[0:6000]\n",
    "        else:\n",
    "            self.x_list = self.x_list[6000:]\n",
    "            self.y_list = self.y_list[6000:]\n",
    "            self.posi = self.posi[6000:]\n",
    "\n",
    "        self.len = len(self.x_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_list[index], self.y_list[index], self.posi[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "def get_data(opt):\n",
    "    #NewsData继承于pytorch的Dataset类\n",
    "    trainset = NewsData(opt.data_path,is_train = 1)\n",
    "    trainloader=torch.utils.data.DataLoader(trainset,\n",
    "                                            batch_size=opt.batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=opt.num_workers)\n",
    "    testset = NewsData(opt.data_path,is_train = 0)\n",
    "    testloader=torch.utils.data.DataLoader(testset,\n",
    "                                           batch_size=opt.batch_size,\n",
    "                                           shuffle=False,\n",
    "                                           num_workers=opt.num_workers)\n",
    "    return trainloader, testloader\n",
    "trainloader, testloader = get_data(opt3)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "500 69782363f8264714b60ba699aff87e61,2\n",
      "\n",
      "1000 d6b520b3a93643c7855cc9ca4262bd0f,1\n",
      "\n",
      "1500 4c4990a462b24b2787ddb75d69d68beb,1\n",
      "\n",
      "2000 c67b1254e8a746a78df818c54001ee8d,1\n",
      "\n",
      "2500 45e0fe90c3e845c397e8e7c8a021019d,2\n",
      "\n",
      "3000 80aea805d1064025864224914ae28949,1\n",
      "\n",
      "3500 77486e8fafa94f42985c20a46c0b5b0a,2\n",
      "\n",
      "4000 21e36b376b2e4b4988736494c995e57f,1\n",
      "\n",
      "4500 728cbcbcecf84cfe87ded2b2f2801616,2\n",
      "\n",
      "5000 8ad5c7afb24948b0b3d1aae7ec6311c0,1\n",
      "\n",
      "5500 4e3c9fa942d9425b90397f4a0b008180,1\n",
      "\n",
      "6000 c7d498d2bac14644b7f8cfb67a3fb3ed,2\n",
      "\n",
      "6500 35f7358beb72448095ffe1579128d1d4,2\n",
      "\n",
      "7000 86486e5c075c4534ab7a0e2c3240108c,2\n",
      "\n",
      "500 69782363f8264714b60ba699aff87e61,2\n",
      "\n",
      "1000 d6b520b3a93643c7855cc9ca4262bd0f,1\n",
      "\n",
      "1500 4c4990a462b24b2787ddb75d69d68beb,1\n",
      "\n",
      "2000 c67b1254e8a746a78df818c54001ee8d,1\n",
      "\n",
      "2500 45e0fe90c3e845c397e8e7c8a021019d,2\n",
      "\n",
      "3000 80aea805d1064025864224914ae28949,1\n",
      "\n",
      "3500 77486e8fafa94f42985c20a46c0b5b0a,2\n",
      "\n",
      "4000 21e36b376b2e4b4988736494c995e57f,1\n",
      "\n",
      "4500 728cbcbcecf84cfe87ded2b2f2801616,2\n",
      "\n",
      "5000 8ad5c7afb24948b0b3d1aae7ec6311c0,1\n",
      "\n",
      "5500 4e3c9fa942d9425b90397f4a0b008180,1\n",
      "\n",
      "6000 c7d498d2bac14644b7f8cfb67a3fb3ed,2\n",
      "\n",
      "6500 35f7358beb72448095ffe1579128d1d4,2\n",
      "\n",
      "7000 86486e5c075c4534ab7a0e2c3240108c,2\n",
      "\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "yWnKSvvtUKpj",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "WM7B-0hZUKpl",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def train(epoch, model, trainloader, testloader, optimizer, opt):\n",
    "    print('\\ntrain-Epoch: %d' % (epoch + 1))\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    print_step = int(len(trainloader) / 10)\n",
    "    for batch_idx, (sue, label, posi) in enumerate(trainloader):\n",
    "        if opt.gpu:\n",
    "            sue = sue.cuda()\n",
    "            posi = posi.cuda()\n",
    "            label = label.unsqueeze(1).cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # 输入参数为词列表、位置列表、标签\n",
    "        outputs = model(sue, position_ids=posi, labels=label)\n",
    "\n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % print_step == 0:\n",
    "            print(\"Epoch:%d [%d|%d] loss:%f\" % (epoch + 1, batch_idx, len(trainloader), loss.mean()))\n",
    "    print(\"time:%.3f\" % (time.time() - start_time))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "1k1nE7CPUKpq",
    "colab_type": "code",
    "outputId": "692894cc-5e04-4ca4-dc83-1fce32f87076",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1587113804397,
     "user_tz": -480,
     "elapsed": 3898286,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "def testFunc(epoch, model, trainloader, testloader, opt):\n",
    "    print('\\ntest-Epoch: %d' % (epoch + 1))\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (sue, label, posi) in enumerate(testloader):\n",
    "            if opt.gpu:\n",
    "                sue = sue.cuda()\n",
    "                posi = posi.cuda()\n",
    "                labels = label.unsqueeze(1).cuda()\n",
    "                label = label.cuda()\n",
    "            else:\n",
    "                labels = label.unsqueeze(1)\n",
    "\n",
    "            outputs = model(sue, labels=labels)\n",
    "            loss, logits = outputs[:2]\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "\n",
    "            total += sue.size(0)\n",
    "            correct += predicted.data.eq(label.data).cpu().sum()\n",
    "\n",
    "    s = (\"Acc:%.3f\" % ((1.0 * correct.numpy()) / total))\n",
    "    print(s)\n",
    "\n",
    "if not os.path.exists('./model.pth'):\n",
    "  for epoch in range(opt3.nepoch):\n",
    "    train(epoch, model, trainloader, testloader, optimizer, opt3)\n",
    "    testFunc(epoch, model, trainloader, testloader, opt3)\n",
    "  torch.save(model.state_dict(), './model.pth')\n",
    "else:\n",
    "  print('加载模型')\n",
    "  model.load_state_dict(torch.load('model.pth'))\n",
    "  print('继续训练模型')\n",
    "  for epoch in range(opt3.nepoch):\n",
    "    train(epoch, model, trainloader, testloader, optimizer, opt3)\n",
    "    testFunc(epoch, model, trainloader, testloader, opt3)\n",
    "  torch.save(model.state_dict(), './model.pth')\n",
    "  \n",
    "print('评估模型：')\n",
    "testFunc(0, model, trainloader, testloader, opt3)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "加载模型\n",
      "继续训练模型\n",
      "\n",
      "train-Epoch: 1\n",
      "Epoch:1 [0|600] loss:0.239784\n",
      "Epoch:1 [60|600] loss:0.013531\n",
      "Epoch:1 [120|600] loss:0.010328\n",
      "Epoch:1 [180|600] loss:0.070856\n",
      "Epoch:1 [240|600] loss:0.233773\n",
      "Epoch:1 [300|600] loss:0.020154\n",
      "Epoch:1 [360|600] loss:0.092855\n",
      "Epoch:1 [420|600] loss:0.019567\n",
      "Epoch:1 [480|600] loss:0.004509\n",
      "Epoch:1 [540|600] loss:0.011776\n",
      "time:209.344\n",
      "\n",
      "test-Epoch: 1\n",
      "Acc:0.672\n",
      "\n",
      "train-Epoch: 2\n",
      "Epoch:2 [0|600] loss:0.017937\n",
      "Epoch:2 [60|600] loss:0.018717\n",
      "Epoch:2 [120|600] loss:0.005666\n",
      "Epoch:2 [180|600] loss:0.031009\n",
      "Epoch:2 [240|600] loss:0.042000\n",
      "Epoch:2 [300|600] loss:0.032604\n",
      "Epoch:2 [360|600] loss:0.075369\n",
      "Epoch:2 [420|600] loss:0.071426\n",
      "Epoch:2 [480|600] loss:0.004053\n",
      "Epoch:2 [540|600] loss:0.029277\n",
      "time:209.717\n",
      "\n",
      "test-Epoch: 2\n",
      "Acc:0.672\n",
      "\n",
      "train-Epoch: 3\n",
      "Epoch:3 [0|600] loss:0.033012\n",
      "Epoch:3 [60|600] loss:0.002780\n",
      "Epoch:3 [120|600] loss:0.001836\n",
      "Epoch:3 [180|600] loss:0.379085\n",
      "Epoch:3 [240|600] loss:0.011814\n",
      "Epoch:3 [300|600] loss:0.004146\n",
      "Epoch:3 [360|600] loss:0.062081\n",
      "Epoch:3 [420|600] loss:0.160812\n",
      "Epoch:3 [480|600] loss:0.059474\n",
      "Epoch:3 [540|600] loss:0.030752\n",
      "time:209.718\n",
      "\n",
      "test-Epoch: 3\n",
      "Acc:0.668\n",
      "\n",
      "train-Epoch: 4\n",
      "Epoch:4 [0|600] loss:0.119621\n",
      "Epoch:4 [60|600] loss:0.067336\n",
      "Epoch:4 [120|600] loss:0.098577\n",
      "Epoch:4 [180|600] loss:0.021730\n",
      "Epoch:4 [240|600] loss:0.004486\n",
      "Epoch:4 [300|600] loss:0.003347\n",
      "Epoch:4 [360|600] loss:0.409209\n",
      "Epoch:4 [420|600] loss:0.034962\n",
      "Epoch:4 [480|600] loss:0.043377\n",
      "Epoch:4 [540|600] loss:0.005045\n",
      "time:209.632\n",
      "\n",
      "test-Epoch: 4\n",
      "Acc:0.652\n",
      "\n",
      "train-Epoch: 5\n",
      "Epoch:5 [0|600] loss:0.004372\n",
      "Epoch:5 [60|600] loss:0.001335\n",
      "Epoch:5 [120|600] loss:0.118844\n",
      "Epoch:5 [180|600] loss:0.000635\n",
      "Epoch:5 [240|600] loss:0.005563\n",
      "Epoch:5 [300|600] loss:0.002601\n",
      "Epoch:5 [360|600] loss:0.045104\n",
      "Epoch:5 [420|600] loss:0.002607\n",
      "Epoch:5 [480|600] loss:0.003403\n",
      "Epoch:5 [540|600] loss:0.014622\n",
      "time:209.771\n",
      "\n",
      "test-Epoch: 5\n",
      "Acc:0.663\n",
      "\n",
      "train-Epoch: 6\n",
      "Epoch:6 [0|600] loss:0.005661\n",
      "Epoch:6 [60|600] loss:0.007598\n",
      "Epoch:6 [120|600] loss:0.036784\n",
      "Epoch:6 [180|600] loss:0.005904\n",
      "Epoch:6 [240|600] loss:0.378700\n",
      "Epoch:6 [300|600] loss:0.009086\n",
      "Epoch:6 [360|600] loss:0.219979\n",
      "Epoch:6 [420|600] loss:0.003465\n",
      "Epoch:6 [480|600] loss:0.176629\n",
      "Epoch:6 [540|600] loss:0.053431\n",
      "time:210.104\n",
      "\n",
      "test-Epoch: 6\n",
      "Acc:0.643\n",
      "\n",
      "train-Epoch: 7\n",
      "Epoch:7 [0|600] loss:0.002281\n",
      "Epoch:7 [60|600] loss:0.489022\n",
      "Epoch:7 [120|600] loss:0.001418\n",
      "Epoch:7 [180|600] loss:0.002344\n",
      "Epoch:7 [240|600] loss:0.016388\n",
      "Epoch:7 [300|600] loss:0.060730\n",
      "Epoch:7 [360|600] loss:0.001380\n",
      "Epoch:7 [420|600] loss:0.021094\n",
      "Epoch:7 [480|600] loss:0.013743\n",
      "Epoch:7 [540|600] loss:0.018795\n",
      "time:209.820\n",
      "\n",
      "test-Epoch: 7\n",
      "Acc:0.636\n",
      "\n",
      "train-Epoch: 8\n",
      "Epoch:8 [0|600] loss:0.000856\n",
      "Epoch:8 [60|600] loss:0.110449\n",
      "Epoch:8 [120|600] loss:0.001111\n",
      "Epoch:8 [180|600] loss:0.011155\n",
      "Epoch:8 [240|600] loss:0.001121\n",
      "Epoch:8 [300|600] loss:0.297739\n",
      "Epoch:8 [360|600] loss:0.020158\n",
      "Epoch:8 [420|600] loss:0.008713\n",
      "Epoch:8 [480|600] loss:0.000825\n",
      "Epoch:8 [540|600] loss:0.007383\n",
      "time:209.786\n",
      "\n",
      "test-Epoch: 8\n",
      "Acc:0.671\n",
      "\n",
      "train-Epoch: 9\n",
      "Epoch:9 [0|600] loss:0.012284\n",
      "Epoch:9 [60|600] loss:0.000451\n",
      "Epoch:9 [120|600] loss:0.005242\n",
      "Epoch:9 [180|600] loss:0.009770\n",
      "Epoch:9 [240|600] loss:0.005005\n",
      "Epoch:9 [300|600] loss:0.001493\n",
      "Epoch:9 [360|600] loss:0.002064\n",
      "Epoch:9 [420|600] loss:0.047023\n",
      "Epoch:9 [480|600] loss:0.002068\n",
      "Epoch:9 [540|600] loss:0.001729\n",
      "time:209.727\n",
      "\n",
      "test-Epoch: 9\n",
      "Acc:0.649\n",
      "\n",
      "train-Epoch: 10\n",
      "Epoch:10 [0|600] loss:0.000719\n",
      "Epoch:10 [60|600] loss:0.007199\n",
      "Epoch:10 [120|600] loss:0.001328\n",
      "Epoch:10 [180|600] loss:0.008347\n",
      "Epoch:10 [240|600] loss:0.050089\n",
      "Epoch:10 [300|600] loss:0.002391\n",
      "Epoch:10 [360|600] loss:0.012226\n",
      "Epoch:10 [420|600] loss:0.004273\n",
      "Epoch:10 [480|600] loss:0.002315\n",
      "Epoch:10 [540|600] loss:0.004057\n",
      "time:210.017\n",
      "\n",
      "test-Epoch: 10\n",
      "Acc:0.653\n",
      "\n",
      "train-Epoch: 11\n",
      "Epoch:11 [0|600] loss:0.002133\n",
      "Epoch:11 [60|600] loss:0.004599\n",
      "Epoch:11 [120|600] loss:0.001637\n",
      "Epoch:11 [180|600] loss:0.002447\n",
      "Epoch:11 [240|600] loss:0.064326\n",
      "Epoch:11 [300|600] loss:0.002071\n",
      "Epoch:11 [360|600] loss:0.003402\n",
      "Epoch:11 [420|600] loss:0.191462\n",
      "Epoch:11 [480|600] loss:0.011106\n",
      "Epoch:11 [540|600] loss:0.133300\n",
      "time:210.170\n",
      "\n",
      "test-Epoch: 11\n",
      "Acc:0.647\n",
      "\n",
      "train-Epoch: 12\n",
      "Epoch:12 [0|600] loss:0.020290\n",
      "Epoch:12 [60|600] loss:0.018234\n",
      "Epoch:12 [120|600] loss:0.002983\n",
      "Epoch:12 [180|600] loss:0.001086\n",
      "Epoch:12 [240|600] loss:0.000768\n",
      "Epoch:12 [300|600] loss:0.038676\n",
      "Epoch:12 [360|600] loss:0.041152\n",
      "Epoch:12 [420|600] loss:0.002248\n",
      "Epoch:12 [480|600] loss:0.005024\n",
      "Epoch:12 [540|600] loss:0.024663\n",
      "time:210.048\n",
      "\n",
      "test-Epoch: 12\n",
      "Acc:0.634\n",
      "\n",
      "train-Epoch: 13\n",
      "Epoch:13 [0|600] loss:0.001730\n",
      "Epoch:13 [60|600] loss:0.004053\n",
      "Epoch:13 [120|600] loss:0.091884\n",
      "Epoch:13 [180|600] loss:0.002761\n",
      "Epoch:13 [240|600] loss:0.039751\n",
      "Epoch:13 [300|600] loss:0.018236\n",
      "Epoch:13 [360|600] loss:0.000562\n",
      "Epoch:13 [420|600] loss:0.000577\n",
      "Epoch:13 [480|600] loss:0.096175\n",
      "Epoch:13 [540|600] loss:0.079070\n",
      "time:210.098\n",
      "\n",
      "test-Epoch: 13\n",
      "Acc:0.658\n",
      "\n",
      "train-Epoch: 14\n",
      "Epoch:14 [0|600] loss:0.672527\n",
      "Epoch:14 [60|600] loss:0.006343\n",
      "Epoch:14 [120|600] loss:0.001856\n",
      "Epoch:14 [180|600] loss:0.006815\n",
      "Epoch:14 [240|600] loss:0.003829\n",
      "Epoch:14 [300|600] loss:0.000231\n",
      "Epoch:14 [360|600] loss:0.064910\n",
      "Epoch:14 [420|600] loss:0.070168\n",
      "Epoch:14 [480|600] loss:0.006119\n",
      "Epoch:14 [540|600] loss:0.029731\n",
      "time:209.996\n",
      "\n",
      "test-Epoch: 14\n",
      "Acc:0.669\n",
      "\n",
      "train-Epoch: 15\n",
      "Epoch:15 [0|600] loss:0.007027\n",
      "Epoch:15 [60|600] loss:0.000813\n",
      "Epoch:15 [120|600] loss:0.005358\n",
      "Epoch:15 [180|600] loss:0.000820\n",
      "Epoch:15 [240|600] loss:0.000900\n",
      "Epoch:15 [300|600] loss:0.153502\n",
      "Epoch:15 [360|600] loss:0.003029\n",
      "Epoch:15 [420|600] loss:0.007057\n",
      "Epoch:15 [480|600] loss:0.001514\n",
      "Epoch:15 [540|600] loss:0.000955\n",
      "time:210.209\n",
      "\n",
      "test-Epoch: 15\n",
      "Acc:0.669\n",
      "\n",
      "train-Epoch: 16\n",
      "Epoch:16 [0|600] loss:0.002167\n",
      "Epoch:16 [60|600] loss:0.172995\n",
      "Epoch:16 [120|600] loss:0.002565\n",
      "Epoch:16 [180|600] loss:0.000638\n",
      "Epoch:16 [240|600] loss:0.003859\n",
      "Epoch:16 [300|600] loss:0.000256\n",
      "Epoch:16 [360|600] loss:0.003833\n",
      "Epoch:16 [420|600] loss:0.012838\n",
      "Epoch:16 [480|600] loss:0.005283\n",
      "Epoch:16 [540|600] loss:0.001008\n",
      "time:210.102\n",
      "\n",
      "test-Epoch: 16\n",
      "Acc:0.670\n",
      "\n",
      "train-Epoch: 17\n",
      "Epoch:17 [0|600] loss:0.000866\n",
      "Epoch:17 [60|600] loss:0.001368\n",
      "Epoch:17 [120|600] loss:0.107265\n",
      "Epoch:17 [180|600] loss:0.020325\n",
      "Epoch:17 [240|600] loss:0.000531\n",
      "Epoch:17 [300|600] loss:0.001432\n",
      "Epoch:17 [360|600] loss:0.000757\n",
      "Epoch:17 [420|600] loss:0.000598\n",
      "Epoch:17 [480|600] loss:0.001069\n",
      "Epoch:17 [540|600] loss:0.009082\n",
      "time:209.874\n",
      "\n",
      "test-Epoch: 17\n",
      "Acc:0.671\n",
      "\n",
      "train-Epoch: 18\n",
      "Epoch:18 [0|600] loss:0.287113\n",
      "Epoch:18 [60|600] loss:0.021032\n",
      "Epoch:18 [120|600] loss:0.084768\n",
      "Epoch:18 [180|600] loss:0.044047\n",
      "Epoch:18 [240|600] loss:0.025942\n",
      "Epoch:18 [300|600] loss:0.054366\n",
      "Epoch:18 [360|600] loss:0.111200\n",
      "Epoch:18 [420|600] loss:0.087803\n",
      "Epoch:18 [480|600] loss:0.000242\n",
      "Epoch:18 [540|600] loss:0.001556\n",
      "time:209.889\n",
      "\n",
      "test-Epoch: 18\n",
      "Acc:0.654\n",
      "\n",
      "train-Epoch: 19\n",
      "Epoch:19 [0|600] loss:0.004182\n",
      "Epoch:19 [60|600] loss:0.001186\n",
      "Epoch:19 [120|600] loss:0.006499\n",
      "Epoch:19 [180|600] loss:0.000714\n",
      "Epoch:19 [240|600] loss:0.070698\n",
      "Epoch:19 [300|600] loss:0.001600\n",
      "Epoch:19 [360|600] loss:0.000873\n",
      "Epoch:19 [420|600] loss:0.004501\n",
      "Epoch:19 [480|600] loss:0.008360\n",
      "Epoch:19 [540|600] loss:0.371046\n",
      "time:209.967\n",
      "\n",
      "test-Epoch: 19\n",
      "Acc:0.666\n",
      "\n",
      "train-Epoch: 20\n",
      "Epoch:20 [0|600] loss:0.019657\n",
      "Epoch:20 [60|600] loss:0.000345\n",
      "Epoch:20 [120|600] loss:0.001928\n",
      "Epoch:20 [180|600] loss:0.001562\n",
      "Epoch:20 [240|600] loss:0.001740\n",
      "Epoch:20 [300|600] loss:0.000684\n",
      "Epoch:20 [360|600] loss:0.009860\n",
      "Epoch:20 [420|600] loss:0.001315\n",
      "Epoch:20 [480|600] loss:0.002089\n",
      "Epoch:20 [540|600] loss:0.032033\n",
      "time:210.412\n",
      "\n",
      "test-Epoch: 20\n",
      "Acc:0.674\n",
      "\n",
      "train-Epoch: 21\n",
      "Epoch:21 [0|600] loss:0.001589\n",
      "Epoch:21 [60|600] loss:0.000542\n",
      "Epoch:21 [120|600] loss:0.000227\n",
      "Epoch:21 [180|600] loss:0.000879\n",
      "Epoch:21 [240|600] loss:0.000481\n",
      "Epoch:21 [300|600] loss:0.001472\n",
      "Epoch:21 [360|600] loss:0.035954\n",
      "Epoch:21 [420|600] loss:0.001132\n",
      "Epoch:21 [480|600] loss:0.006755\n",
      "Epoch:21 [540|600] loss:0.003283\n",
      "time:210.029\n",
      "\n",
      "test-Epoch: 21\n",
      "Acc:0.677\n",
      "\n",
      "train-Epoch: 22\n",
      "Epoch:22 [0|600] loss:0.000154\n",
      "Epoch:22 [60|600] loss:0.337715\n",
      "Epoch:22 [120|600] loss:0.010952\n",
      "Epoch:22 [180|600] loss:0.007597\n",
      "Epoch:22 [240|600] loss:0.075128\n",
      "Epoch:22 [300|600] loss:0.007525\n",
      "Epoch:22 [360|600] loss:0.062423\n",
      "Epoch:22 [420|600] loss:0.004074\n",
      "Epoch:22 [480|600] loss:0.001447\n",
      "Epoch:22 [540|600] loss:0.000919\n",
      "time:209.684\n",
      "\n",
      "test-Epoch: 22\n",
      "Acc:0.646\n",
      "\n",
      "train-Epoch: 23\n",
      "Epoch:23 [0|600] loss:0.000621\n",
      "Epoch:23 [60|600] loss:0.001038\n",
      "Epoch:23 [120|600] loss:0.107649\n",
      "Epoch:23 [180|600] loss:0.022427\n",
      "Epoch:23 [240|600] loss:0.000216\n",
      "Epoch:23 [300|600] loss:0.002499\n",
      "Epoch:23 [360|600] loss:0.000329\n",
      "Epoch:23 [420|600] loss:0.327071\n",
      "Epoch:23 [480|600] loss:0.010755\n",
      "Epoch:23 [540|600] loss:0.000781\n",
      "time:209.914\n",
      "\n",
      "test-Epoch: 23\n",
      "Acc:0.666\n",
      "\n",
      "train-Epoch: 24\n",
      "Epoch:24 [0|600] loss:0.000058\n",
      "Epoch:24 [60|600] loss:0.000937\n",
      "Epoch:24 [120|600] loss:0.000257\n",
      "Epoch:24 [180|600] loss:0.003226\n",
      "Epoch:24 [240|600] loss:0.021492\n",
      "Epoch:24 [300|600] loss:0.000522\n",
      "Epoch:24 [360|600] loss:0.003280\n",
      "Epoch:24 [420|600] loss:0.023001\n",
      "Epoch:24 [480|600] loss:0.011100\n",
      "Epoch:24 [540|600] loss:0.004127\n",
      "time:210.035\n",
      "\n",
      "test-Epoch: 24\n",
      "Acc:0.647\n",
      "\n",
      "train-Epoch: 25\n",
      "Epoch:25 [0|600] loss:0.003285\n",
      "Epoch:25 [60|600] loss:0.160237\n",
      "Epoch:25 [120|600] loss:0.000372\n",
      "Epoch:25 [180|600] loss:0.010902\n",
      "Epoch:25 [240|600] loss:0.030260\n",
      "Epoch:25 [300|600] loss:0.002185\n",
      "Epoch:25 [360|600] loss:0.000643\n",
      "Epoch:25 [420|600] loss:0.022402\n",
      "Epoch:25 [480|600] loss:0.002061\n",
      "Epoch:25 [540|600] loss:0.000617\n",
      "time:210.083\n",
      "\n",
      "test-Epoch: 25\n",
      "Acc:0.675\n",
      "\n",
      "train-Epoch: 26\n",
      "Epoch:26 [0|600] loss:0.003705\n",
      "Epoch:26 [60|600] loss:0.023501\n",
      "Epoch:26 [120|600] loss:0.000787\n",
      "Epoch:26 [180|600] loss:0.008358\n",
      "Epoch:26 [240|600] loss:0.002015\n",
      "Epoch:26 [300|600] loss:0.000344\n",
      "Epoch:26 [360|600] loss:0.001662\n",
      "Epoch:26 [420|600] loss:0.003708\n",
      "Epoch:26 [480|600] loss:0.002396\n",
      "Epoch:26 [540|600] loss:0.048996\n",
      "time:210.105\n",
      "\n",
      "test-Epoch: 26\n",
      "Acc:0.679\n",
      "\n",
      "train-Epoch: 27\n",
      "Epoch:27 [0|600] loss:0.000806\n",
      "Epoch:27 [60|600] loss:0.199387\n",
      "Epoch:27 [120|600] loss:0.253139\n",
      "Epoch:27 [180|600] loss:0.107286\n",
      "Epoch:27 [240|600] loss:0.000478\n",
      "Epoch:27 [300|600] loss:0.000386\n",
      "Epoch:27 [360|600] loss:0.005452\n",
      "Epoch:27 [420|600] loss:0.125357\n",
      "Epoch:27 [480|600] loss:0.029323\n",
      "Epoch:27 [540|600] loss:0.111938\n",
      "time:210.087\n",
      "\n",
      "test-Epoch: 27\n",
      "Acc:0.668\n",
      "\n",
      "train-Epoch: 28\n",
      "Epoch:28 [0|600] loss:0.000566\n",
      "Epoch:28 [60|600] loss:0.042058\n",
      "Epoch:28 [120|600] loss:0.000394\n",
      "Epoch:28 [180|600] loss:0.000291\n",
      "Epoch:28 [240|600] loss:0.007451\n",
      "Epoch:28 [300|600] loss:0.003494\n",
      "Epoch:28 [360|600] loss:0.000824\n",
      "Epoch:28 [420|600] loss:0.050074\n",
      "Epoch:28 [480|600] loss:0.143943\n",
      "Epoch:28 [540|600] loss:0.028823\n",
      "time:210.088\n",
      "\n",
      "test-Epoch: 28\n",
      "Acc:0.663\n",
      "\n",
      "train-Epoch: 29\n",
      "Epoch:29 [0|600] loss:0.022250\n",
      "Epoch:29 [60|600] loss:0.002833\n",
      "Epoch:29 [120|600] loss:0.000673\n",
      "Epoch:29 [180|600] loss:0.000874\n",
      "Epoch:29 [240|600] loss:0.001987\n",
      "Epoch:29 [300|600] loss:0.000670\n",
      "Epoch:29 [360|600] loss:0.000573\n",
      "Epoch:29 [420|600] loss:0.000613\n",
      "Epoch:29 [480|600] loss:0.031515\n",
      "Epoch:29 [540|600] loss:0.000243\n",
      "time:209.888\n",
      "\n",
      "test-Epoch: 29\n",
      "Acc:0.647\n",
      "\n",
      "train-Epoch: 30\n",
      "Epoch:30 [0|600] loss:0.006292\n",
      "Epoch:30 [60|600] loss:0.001633\n",
      "Epoch:30 [120|600] loss:0.002211\n",
      "Epoch:30 [180|600] loss:0.030555\n",
      "Epoch:30 [240|600] loss:0.000165\n",
      "Epoch:30 [300|600] loss:0.004391\n",
      "Epoch:30 [360|600] loss:0.001388\n",
      "Epoch:30 [420|600] loss:0.007636\n",
      "Epoch:30 [480|600] loss:0.001412\n",
      "Epoch:30 [540|600] loss:0.000429\n",
      "time:210.207\n",
      "\n",
      "test-Epoch: 30\n",
      "Acc:0.658\n",
      "评估模型：\n",
      "\n",
      "test-Epoch: 1\n",
      "Acc:0.658\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "bYMjE9HOUKpt",
    "colab_type": "code",
    "outputId": "23029e86-884d-437a-d466-6d6c1977e5e5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1587113820623,
     "user_tz": -480,
     "elapsed": 16244,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    }
   },
   "source": [
    "testFunc(0, model, trainloader, testloader, opt3)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "\n",
      "test-Epoch: 1\n",
      "Acc:0.658\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[10 11 12 13 14 15 16 17 18 19]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "a = numpy.arange(0, 100)\n",
    "print(a)\n",
    "b = 10\n",
    "i = 0\n",
    "print(a[i * b : (i + 1) * b])\n",
    "i = 1\n",
    "print(a[i * b : (i + 1) * b])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n"
     ]
    },
    {
     "data": {
      "text/plain": "['设置',\n '用户',\n '词典',\n '，',\n '用户',\n '词典',\n '中',\n '的',\n '词',\n '会',\n '被',\n '打',\n '上',\n 'uw',\n '标签',\n '。',\n '词典',\n '中',\n '每',\n '一个',\n '词',\n '一行',\n '，',\n 'UTF8编码']"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import thulac\n",
    "\n",
    "thulacObj = thulac.thulac(seg_only=True)\n",
    "class thulac_cutor:\n",
    "    def cut(self,sentence:str, text=False):\n",
    "        return [w for [w, _] in thulacObj.cut(sentence, text=text)]\n",
    "thulac_cutor = thulac_cutor()\n",
    "\n",
    "thulac_cutor.cut(\"设置用户词典，用户词典中的词会被打上uw标签。词典中每一个词一行，UTF8编码\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "callFuncValues ('Thread-1', 2, 6)\n",
      "callFuncKVValues {}\n",
      "callFuncValues ('Thread-2', 2, 6)\n",
      "callFuncKVValues {}\n",
      "开启线程： Thread-1\n",
      "开启线程： Thread-2\n",
      "Thread-1: Fri May  1 22:17:57 2020\n",
      "Thread-2: Fri May  1 22:17:57 2020\n",
      "Thread-1: Fri May  1 22:17:59 2020\n",
      "Thread-2: Fri May  1 22:17:59 2020\n",
      "Thread-1: Fri May  1 22:18:01 2020\n",
      "Thread-2: Fri May  1 22:18:01 2020\n",
      "Thread-1: Fri May  1 22:18:03 2020\n",
      "Thread-2: Fri May  1 22:18:03 2020\n",
      "Thread-1: Fri May  1 22:18:05 2020\n",
      "Thread-2: Fri May  1 22:18:05 2020\n",
      "Thread-1: Fri May  1 22:18:07 2020\n",
      "This is Callback!\n",
      "Thread-2: Fri May  1 22:18:07 2020\n",
      "This is Callback!\n",
      "退出主线程\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "def print_time(name, delay, counter):\n",
    "    while counter:\n",
    "        time.sleep(delay)\n",
    "        print (\"%s: %s\" % (name, time.ctime(time.time())))\n",
    "        counter -= 1\n",
    "def callback(*callFuncValues, **callFuncKVValues):\n",
    "    print(\"This is Callback!\")\n",
    "\n",
    "class myThread (threading.Thread):\n",
    "    def __init__(self, threadID, name, callFunc, callback, *callFuncValues, **callFuncKVValues):\n",
    "        threading.Thread.__init__(self)\n",
    "        print(\"callFuncValues\", callFuncValues)\n",
    "        print(\"callFuncKVValues\", callFuncKVValues)\n",
    "        self.threadID = threadID\n",
    "        self.name = name\n",
    "        self.callFunc = callFunc\n",
    "        self.callback = callback\n",
    "        self.callFuncValues = callFuncValues\n",
    "        self.callFuncKVValues = callFuncKVValues\n",
    "\n",
    "    def run(self):\n",
    "        print (\"开启线程： \" + self.name)\n",
    "        # 获取锁，用于线程同步\n",
    "        # threadLock.acquire()\n",
    "        if self.callback:\n",
    "            self.callback(self.callFunc(*self.callFuncValues, **self.callFuncKVValues))\n",
    "        else:\n",
    "            self.callFunc(*self.callFuncValues, **self.callFuncKVValues)\n",
    "        # 释放锁，开启下一个线程\n",
    "        # threadLock.release()\n",
    "\n",
    "threadLock = threading.Lock()\n",
    "# threads = []\n",
    "\n",
    "# 创建新线程\n",
    "thread1 = myThread(1, \"Thread-1\", print_time, callback, \"Thread-1\", 2, 6)\n",
    "thread2 = myThread(2, \"Thread-2\", print_time, callback, \"Thread-2\", 2, 6)\n",
    "\n",
    "# 开启新线程\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "\n",
    "# 添加线程到线程列表\n",
    "# threads.append(thread1)\n",
    "# threads.append(thread2)\n",
    "\n",
    "# 等待所有线程完成\n",
    "# for t in threads:\n",
    "#     t.join()\n",
    "\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "print (\"退出主线程\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}