{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time, math\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd /content/drive/My\\ Drive/Colab Notebooks/OursRepository/public-opinion-monitor\n",
    "\n",
    "# !pip install transformers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification as Model, BertTokenizer as Tokenizer\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class DotDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        dict.__init__(self, *args, **kwargs)\n",
    "        self.__dict__ = self"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda.is_available True\n"
     ]
    }
   ],
   "source": [
    "opt = DotDict(\n",
    "    num_labels = 6,\n",
    "    batch_size = 10,\n",
    "    num_workers = 0,\n",
    "    cache_dir = '../../PretrainedData/Transformers/bert-base-chinese',\n",
    "    data_path = 'data',\n",
    "               )\n",
    "\n",
    "print('cuda.is_available',torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  opt.gpu = True\n",
    "else:\n",
    "  opt.gpu = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json from cache at ../../PretrainedData/Transformers/bert-base-chinese\\8a3b1cfe5da58286e12a0f5d7d182b8d6eca88c08e26c332ee3817548cf7e60a.3767c74c8ed285531d04153fe84a0791672aff52f7249b27df341dbce09b8305\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"_num_labels\": 6,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin from cache at ../../PretrainedData/Transformers/bert-base-chinese\\b1b5e295889f2d0979ede9a78ad9cb5dc6a0e25ab7f9417b315f0a2c22f4683d.929717ca66a3ba9eb9ec2f85973c6398c54c38a4faa464636a491d7a705f7eb6\n",
      "INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at ../../PretrainedData/Transformers/bert-base-chinese\\8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.tokenization_bert.BertTokenizer object at 0x000001D114FFC648>\n"
     ]
    }
   ],
   "source": [
    "# num_labels是分类的类数\n",
    "\n",
    "model = Model.from_pretrained('bert-base-chinese',\n",
    "                                         num_labels=opt.num_labels,\n",
    "                                         cache_dir = opt.cache_dir,\n",
    "                                         )\n",
    "tokenizer = Tokenizer.from_pretrained('bert-base-chinese',\n",
    "                                          cache_dir = opt.cache_dir,)\n",
    "print(tokenizer)\n",
    "\n",
    "if opt.gpu:\n",
    "    model = model.cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\n",
    "df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))\n",
    "df = df.append(df2)\n",
    "print(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def isNan(a):\n",
    "    return a != a\n",
    "\n",
    "class RatingData4Bert(data.Dataset):\n",
    "    def __init__(self, path, tokenizer:Tokenizer, max_row = -1, trainTestRate = 0.85, isTrain = True):\n",
    "        self.token_list = []\n",
    "        self.label_list = []\n",
    "        self.token_positions = torch.tensor([i for i in range(100)])\n",
    "\n",
    "        print('地址不应该包含 ratings.csv   path:',path) # 地址不应该包含 ratings.csv\n",
    "\n",
    "        ratings_clean_filename = os.path.join(path, 'ratings_clean.csv')\n",
    "        ratings_filename = os.path.join(path, 'ratings.csv')\n",
    "        if os.path.isfile(ratings_clean_filename):\n",
    "            clean_pd = pd.read_csv(ratings_clean_filename)\n",
    "        else:\n",
    "            print('没有找到缓存的文件%s, 读取源文件%s'%(ratings_clean_filename, ratings_filename))\n",
    "            ratings_pd = pd.read_csv(ratings_filename)\n",
    "            print('开始生成缓存文件%s'%(ratings_clean_filename))\n",
    "            clean_pd = pd.DataFrame({\n",
    "                'userId':[],\n",
    "                'restId':[],\n",
    "                'rating':[],\n",
    "                'comment':[],\n",
    "            })\n",
    "            nonRatingCount = 0\n",
    "            for i, row in ratings_pd.iterrows():\n",
    "                if max_row != -1 and i > max_row:\n",
    "                    break\n",
    "                if not isinstance(row['comment'], str) or row['comment'] == '':\n",
    "                    # print(i + 1, row['comment'])\n",
    "                    nonRatingCount += 1\n",
    "                    continue\n",
    "                r0 = row['rating']\n",
    "                r1 = row['rating_env']\n",
    "                r2 = row['rating_flavor']\n",
    "                r3 = row['rating_service']\n",
    "                if r0 == '' or isNan(r0): r0 = 0 # 假设总评分为 0 表示未评分\n",
    "                if r1 == '' or isNan(r1): r1 = 3\n",
    "                if r2 == '' or isNan(r2): r2 = 3\n",
    "                if r3 == '' or isNan(r3): r3 = 3\n",
    "                r0 = round(r0 * 0.5 + (r1 + r2 + r3) * 0.1666666)\n",
    "                if i % 10000 == 9999:\n",
    "                    print(i + 1, r0)\n",
    "\n",
    "                token = tokenizer.encode(text=str(row['comment']), max_length=100, pad_to_max_length = True)\n",
    "                # print('token', token)\n",
    "                # token = [101] + token + [102]\n",
    "\n",
    "                newRow = DotDict()\n",
    "                newRow.userId = [row['userId']]\n",
    "                newRow.restId = [row['restId']]\n",
    "                newRow.rating = [r0]\n",
    "                newRow.comment = [json.dumps(token)]\n",
    "\n",
    "                clean_pd = clean_pd.append(pd.DataFrame(newRow), ignore_index=True)\n",
    "            print('空的评论数量： %d'%(nonRatingCount))\n",
    "            clean_pd.to_csv(ratings_clean_filename)\n",
    "\n",
    "        # 读取\n",
    "        if isTrain:\n",
    "            temp_pd = clean_pd[ : int(len(clean_pd) * trainTestRate)]\n",
    "        else:\n",
    "            temp_pd = clean_pd[int(len(clean_pd) * trainTestRate) : ]\n",
    "\n",
    "        for i, row in temp_pd.iterrows():\n",
    "            if max_row != -1 and i > max_row:\n",
    "                break\n",
    "\n",
    "            self.label_list.append(torch.tensor(row['rating']).long())\n",
    "            self.token_list.append(torch.from_numpy(numpy.array( json.loads(row['comment']) ) ).long())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # print(self.token_list[index], self.label_list[index], self.token_positions)\n",
    "        return self.token_list[index], self.label_list[index], self.token_positions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "地址不应该包含 ratings.csv   path: ../../DataSets/yf_dianping\n"
     ]
    }
   ],
   "source": [
    "ratingData = RatingData4Bert('../../DataSets/yf_dianping',\n",
    "                             tokenizer=tokenizer,\n",
    "                             max_row= 200000,\n",
    "                             isTrain=True,\n",
    "                             )\n",
    "trainLoader = torch.utils.data.DataLoader(dataset=ratingData,\n",
    "                                          batch_size=opt.batch_size,\n",
    "                                          shuffle = True,\n",
    "                                          # num_workers = 0,\n",
    "                                          )\n",
    "ratingData2 = RatingData4Bert('../../DataSets/yf_dianping',\n",
    "                             tokenizer=tokenizer,\n",
    "                             max_row= 200000,\n",
    "                             isTrain=False,\n",
    "                             )\n",
    "testLoader = torch.utils.data.DataLoader(dataset=ratingData2,\n",
    "                                          batch_size=opt.batch_size,\n",
    "                                          shuffle = True,\n",
    "                                          # num_workers = 0,\n",
    "                                          )\n",
    "print(len(ratingData.label_list))\n",
    "print(len(ratingData2.label_list))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.nn.modules.loss import MSELoss as Loss\n",
    "\n",
    "def trainOneEpoch(epoch, model:Model, trainLoader, optimizer:Optimizer, opt):\n",
    "    model.train()\n",
    "    \n",
    "    startTime = time.time()\n",
    "    for i, (x, y, p) in enumerate(trainLoader):\n",
    "        if opt.gpu:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            p = p.cuda()\n",
    "\n",
    "        outputs = model(input_ids = x, labels = y, position_ids = p)\n",
    "        loss = outputs[0]\n",
    "        # logits = outputs[1]\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 1000 == 999:\n",
    "            print('Epoch %d, %d/%d, loss:%f ' % (epoch, i, len(trainLoader), loss.means()))\n",
    "    print('Epoch %d cost time: %.3fs' % (epoch, time.time() - startTime))\n",
    "\n",
    "\n",
    "def testModel(epoch, model:Model, testLoader, opt):\n",
    "    model.eval()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    startTime = time.time()\n",
    "    for i, (x, y, p) in enumerate(testLoader):\n",
    "        if opt.gpu:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            p = p.cuda()\n",
    "\n",
    "        outputs = model(input_ids = x, labels = y, position_ids = p)\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "\n",
    "        total += x.size(0)\n",
    "        correct += predicted.data.eq(y.data).cpu().sum()\n",
    "\n",
    "        if i % 1000 == 999:\n",
    "            print('Epoch Test %d, %d/%d, loss:%f ' % (epoch, i, len(testLoader), loss))\n",
    "    print('Epoch Test %d cost time: %.3fs' % (epoch, time.time() - startTime))\n",
    "    print('准确率： %.3f' % (correct / total))\n",
    "\n",
    "\n",
    "def train(nepoch, modelSavePath):\n",
    "    optimizer=torch.optim.SGD(model.parameters(), momentum=0.9, lr=0.001)\n",
    "    for epoch in range(nepoch):\n",
    "        trainOneEpoch(epoch, model, trainLoader, optimizer, opt)\n",
    "        testModel(epoch, model, testLoader, opt)\n",
    "    torch.save(model.state_dict(), modelSavePath)\n",
    "\n",
    "def eval(modelSavePath, isLoad = True):\n",
    "    if isLoad: model.load_state_dict(torch.load(modelSavePath))\n",
    "    testModel(0, model, testLoader, opt)\n",
    "\n",
    "train(10, 'EmotionAnalyzeModelData.model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print( *torch.max(torch.tensor([[5, 33, 2, 65, 4]]), 1))  # 1是维度，用*来取值可得到两个值:最大值和索引\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}