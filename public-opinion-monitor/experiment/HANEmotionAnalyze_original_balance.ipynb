{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "HANEmotionAnalyze_balance_600000.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-CIbt4LqMiS",
    "colab_type": "text"
   },
   "source": [
    "## 开始"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zBDjArJoT65A",
    "colab_type": "code",
    "outputId": "1c4bdb86-a297-4add-c9fa-3426c4352807",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588175646051,
     "user_tz": -480,
     "elapsed": 23935,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    }
   },
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd /content/drive/My\\ Drive/Colab Notebooks/OursRepository/public-opinion-monitor\n",
    "# \n",
    "# !pip install thulac\n",
    "# !pip install jieba"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "9Eush51XTobh",
    "colab_type": "code",
    "outputId": "6c71ba20-2bfe-43ac-d043-f2c6b71cbdcf",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588175661641,
     "user_tz": -480,
     "elapsed": 4680,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    }
   },
   "source": [
    "# from ClassicalHANModel import *\n",
    "from HANModel import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import numpy\n",
    "import time, math\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import jieba\n",
    "import thulac\n",
    "thulacObj = thulac.thulac(seg_only=True)\n",
    "class thulac_cutor:\n",
    "    def cut(self,sentence:str):\n",
    "        return thulacObj.fast_cut(sentence, text=True)\n",
    "thulac_cutor = thulac_cutor()\n"
   ],
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "DUFj-U3PTobn",
    "colab_type": "code",
    "outputId": "ef5771e1-b6e3-4128-e340-72db7c4ddfd4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588175667529,
     "user_tz": -480,
     "elapsed": 1796,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    }
   },
   "source": [
    "# import torch.functional as F\n",
    "# \n",
    "embedding = nn.Embedding(10, 3)\n",
    "input = torch.LongTensor([[0, 1,2,4,5],[1, 4,3,2,9]])\n",
    "print(embedding(input))\n",
    "embedding2 = nn.Embedding(10, 3, padding_idx=2)\n",
    "# input = torch.LongTensor([[1,2,0,5, 6,7,8,9]])\n",
    "print(embedding2(input))\n",
    "weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n",
    "embedding3 = nn.Embedding.from_pretrained(weight)\n",
    "input = torch.LongTensor([1])\n",
    "print(embedding3(input))\n",
    "input = torch.LongTensor([0])\n",
    "print(embedding3(input))\n",
    "\n"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2861, -0.3206, -0.4947],\n",
      "         [ 0.1511, -0.8703, -1.1428],\n",
      "         [ 0.4613, -0.4741,  0.2637],\n",
      "         [ 1.3662, -2.7073,  0.1566],\n",
      "         [ 0.9701,  0.2239, -0.0524]],\n",
      "\n",
      "        [[ 0.1511, -0.8703, -1.1428],\n",
      "         [ 1.3662, -2.7073,  0.1566],\n",
      "         [ 0.0638, -0.2631,  1.9001],\n",
      "         [ 0.4613, -0.4741,  0.2637],\n",
      "         [-1.7546,  1.2931,  0.3676]]], grad_fn=<EmbeddingBackward>)\n",
      "tensor([[[-1.0643, -0.6969, -1.1636],\n",
      "         [-0.1451,  0.0369, -0.3593],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [-2.2114,  0.4338,  0.5247],\n",
      "         [-0.2669, -0.9779, -0.6148]],\n",
      "\n",
      "        [[-0.1451,  0.0369, -0.3593],\n",
      "         [-2.2114,  0.4338,  0.5247],\n",
      "         [-0.7927, -0.6899,  0.6048],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [-0.2294,  0.8857, -0.6943]]], grad_fn=<EmbeddingBackward>)\n",
      "tensor([[4.0000, 5.1000, 6.3000]])\n",
      "tensor([[1.0000, 2.3000, 3.0000]])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "3HmMThKsTobr",
    "colab_type": "text"
   },
   "source": [
    "## 读取词向量\n",
    "建立词语列表"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "hhmz_drpTobs",
    "colab_type": "code",
    "outputId": "7f1f5928-abf0-4f7a-9c5f-f7e22e212828",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588175692243,
     "user_tz": -480,
     "elapsed": 22553,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    }
   },
   "source": [
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "\n",
    "# file = '../../PretrainedData/Tencent_AILab_ChineseEmbedding/Tencent_AILab_ChineseEmbedding.txt'\n",
    "# file = '../../DataSets/Word2Vect/xingrong_50_thulac/word2vect_50_w5.model'\n",
    "# file = '../../DataSets/Word2Vect/xiejunjie_300_jieba/wiki_han_word2vec_300维度.model'\n",
    "file = '../../DataSets/Word2Vect/Tencent_AILab_ChineseEmbedding/Tencent_AILab_ChineseEmbedding_Min.txt'\n",
    "# word2vec = Word2Vec.load(file)\n",
    "word2vec = KeyedVectors.load_word2vec_format(file, binary=False,limit=100000)\n",
    "# word2vec = KeyedVectors.load_word2vec_format(file, binary=False)\n",
    "word2vec.init_sims(replace=True)  # 神奇，很省内存，可以运算most_similar\n",
    "word2vec.vector_size"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "200"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "_zCpfSvDTocC",
    "colab_type": "code",
    "outputId": "43edc887-48fc-43b5-beed-263a1f34bf89",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588175693369,
     "user_tz": -480,
     "elapsed": 18430,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    }
   },
   "source": [
    "print(word2vec)\n",
    "# print(word2vec.wv.vocab)\n",
    "# print(len(word2vec.index2word))\n",
    "print(len(word2vec.wv.index2word))\n",
    "print(word2vec.wv.index2word[0])\n",
    "print(word2vec.wv.index2word[1])\n",
    "print(word2vec.wv.index2word[2])\n",
    "print(word2vec.wv.index2word[1522])\n",
    "print(word2vec.wv.index2entity[1522])\n",
    "print(word2vec.similar_by_word('中国'))\n",
    "print(word2vec.similar_by_word('天才'))\n",
    "print(word2vec.wv)\n",
    "print('word2vec.wv.vocab ---- >', word2vec.wv.vocab)\n",
    "print(word2vec.wv.index2word)"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  import sys\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if sys.path[0] == '':\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  del sys.path[0]\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "lzs2-uceTocG",
    "colab_type": "code",
    "outputId": "0aa2edbf-8de3-4024-f243-dbe1e71a9e7a",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588175693369,
     "user_tz": -480,
     "elapsed": 6319,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "wordEmbedding = [word2vec.wv[word]  for word in word2vec.wv.index2word]\n",
    "word2index = { word:i for i, word in enumerate(word2vec.wv.index2word)}\n",
    "# print(wordEmbedding[:2])\n",
    "# print(word2index['中国'])\n",
    "# print(word2index['天才'])\n",
    "print(word2index['喜欢'])\n",
    "print(word2index['不'])\n",
    "print(word2index['爽'])\n",
    "print(word2index['超赞'])\n",
    "\n"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217\n",
      "64\n",
      "5313\n",
      "41047\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "cy9vhIUoTocL",
    "colab_type": "text"
   },
   "source": [
    "## 读取训练数据"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "gZGmiumwTocN",
    "colab_type": "code",
    "outputId": "5f9df97d-0ee8-4e68-b0ba-6b8fdce08086",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588175697789,
     "user_tz": -480,
     "elapsed": 4172,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    }
   },
   "source": [
    "def compute_ngrams(word, num_min = 1, num_max = 3):\n",
    "    ngrams =[]\n",
    "    for ngram_length in range(num_min, min(len(word), num_max) + 1):\n",
    "        for i in range(len(word) - ngram_length + 1):\n",
    "            # print(i, i + ngram_length)\n",
    "            ngrams.append(word[i : i + ngram_length])\n",
    "    # print(ngrams)\n",
    "    return list(set(ngrams))\n",
    "\n",
    "print(compute_ngrams('you'))\n",
    "print(compute_ngrams('I Think'))\n",
    "print(compute_ngrams('中华人民共和国万岁'))"
   ],
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['y', 'ou', 'u', 'o', 'yo', 'you']\n",
      "['h', ' T', 'T', 'I ', 'in', 'k', 'hi', 'Th', 'I T', 'n', ' ', 'nk', ' Th', 'I', 'Thi', 'ink', 'hin', 'i']\n",
      "['民共', '共和国', '国万', '中华人', '国万岁', '共', '民', '万', '华人民', '岁', '和', '民共和', '华', '共和', '人民', '华人', '中华', '国', '人民共', '人', '万岁', '和国', '和国万', '中']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ulsoTSc2TocT",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# 从词向量文本文件 word2vec 中获取词向量，如果获取到直接返回，若没有获取到，那么把这个词拆开\n",
    "# 成为 ngrams 的新词组，并在 word2vec 中找新词组中的词向量并相加取平均，最后得到平均词向量输出\n",
    "def wordVec(word, word2vec, min_n = 1, max_n = 3):\n",
    "    # 确认词向量维度\n",
    "    word_size = word2vec.wv.syn0[0].shape[0]\n",
    "\n",
    "    # 如果在词典之中，直接返回词向量\n",
    "    if word in word2vec.wv.vocab.keys():\n",
    "        return word2vec[word]\n",
    "    else:\n",
    "        # 计算word的ngrams词组\n",
    "        ngrams = compute_ngrams(word, min_n, max_n)\n",
    "        # 不在词典的情况下\n",
    "        word_vec = numpy.zeros(word_size, dtype=numpy.float32)\n",
    "        ngrams_found = 0\n",
    "        ngrams_single = [ng for ng in ngrams if len(ng) == 1]\n",
    "        ngrams_more = [ng for ng in ngrams if len(ng) > 1]\n",
    "        # 先只接受2个单词长度以上的词向量\n",
    "        for ngram in ngrams_more:\n",
    "            if ngram in word2vec.wv.vocab.keys():\n",
    "                word_vec += word2vec[ngram]\n",
    "                ngrams_found += 1\n",
    "                #print(ngram)\n",
    "        # 如果，没有匹配到，那么最后是考虑单个词向量\n",
    "        if ngrams_found == 0:\n",
    "            for ngram in ngrams_single:\n",
    "                word_vec += word2vec[ngram]\n",
    "                ngrams_found += 1\n",
    "        if word_vec.any():\n",
    "            return word_vec / max(1, ngrams_found)\n",
    "        else:\n",
    "            # 不抛出异常，而是打印提示，并返回0向量。\n",
    "            print(KeyError('all ngrams for word %s absent from model' % word))\n",
    "            return word_vec"
   ],
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "08tJP4HHTocX",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class DotDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        dict.__init__(self, *args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "        "
   ],
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "XGUX_N8KToca",
    "colab_type": "code",
    "outputId": "d997a8da-6552-42c3-98b7-10935a89d157",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588175703604,
     "user_tz": -480,
     "elapsed": 4979,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "dic = {'a':[1, 2, 3, 4], 'b':[5, 6, 7, 8],\n",
    "'c':[9, 10, 11, 12], 'd':[13, 14, 15, 16]}\n",
    "df1=pd.DataFrame(dic)\n",
    "print(df1)\n",
    "df2=df1.sample(frac=0.75)\n",
    "print(df2)\n",
    "# rowlist=[]\n",
    "# for indexs in df2.index:\n",
    "#     rowlist.append(indexs)\n",
    "df3=df1.drop(df2.index.to_list(),axis=0)\n",
    "print(df3)"
   ],
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a  b   c   d\n",
      "0  1  5   9  13\n",
      "1  2  6  10  14\n",
      "2  3  7  11  15\n",
      "3  4  8  12  16\n",
      "   a  b   c   d\n",
      "1  2  6  10  14\n",
      "3  4  8  12  16\n",
      "2  3  7  11  15\n",
      "   a  b  c   d\n",
      "0  1  5  9  13\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lfY5Is_4I34d",
    "colab_type": "code",
    "outputId": "0f027ec8-bd9c-490f-8a75-99a88a3d4b2e",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588175704173,
     "user_tz": -480,
     "elapsed": 1811,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    }
   },
   "source": [
    "import re\n",
    "\n",
    "def splitText(text:str, splitChar = '(。|，|,|！|\\!|？|\\?|\\n|\\t)'):\n",
    "  '''\n",
    "  句子切分分隔符\n",
    "  '''\n",
    "  contents = re.split(splitChar, text)\n",
    "  contents = [\"\".join([a, b]) if b != '\\n' and b != '\\t' else \"\" #a + \"。\"\n",
    "              for a, b in zip(contents[0::2], contents[1::2])]\n",
    "  contents = [content for content in contents if content.strip() != '' and content[0] != '。']\n",
    "  # for i, sen in enumerate(contents):\n",
    "  #     print(i, sen)\n",
    "  return contents\n",
    "\n",
    "testStr = \"\"\"\n",
    "深圳，简称“深”，别称鹏城，是广东省副省级市、计划单列市、超大城市，国务院批复确定的中国经济特区、全国性经济中心城市和国际化城市 [1]  。截至2018年末，全市下辖9个区，总面积1997.47平方千米，建成区面积927.96平方千米，常住人口1302.66万人，城镇人口1302.66万人，城镇化率100%，是中国第一个全部城镇化的城市。 [2-5] \n",
    "深圳地处中国华南地区、广东南部、珠江口东岸，东临大亚湾和大鹏湾，西濒珠江口和伶仃洋，南隔深圳河与香港相连，是粤港澳大湾区四大中心城市之一 [6]  、国家物流枢纽、国际性综合交通枢纽 [7]  、国际科技产业创新中心 [8]  、中国三大全国性金融中心之一 [9-10]  ，并全力建设中国特色社会主义先行示范区 [11]  、综合性国家科学中心 [12]  、全球海洋中心城市 [13]  。深圳水陆空铁口岸俱全，是中国拥有口岸数量最多、出入境人员最多、车流量最大的口岸城市。 [14]\n",
    "\"\"\"\n",
    "\n",
    "print(splitText(testStr))"
   ],
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['深圳，', '简称“深”，', '别称鹏城，', '是广东省副省级市、计划单列市、超大城市，', '国务院批复确定的中国经济特区、全国性经济中心城市和国际化城市 [1]  。', '截至2018年末，', '全市下辖9个区，', '总面积1997.47平方千米，', '建成区面积927.96平方千米，', '常住人口1302.66万人，', '城镇人口1302.66万人，', '城镇化率100%，', '是中国第一个全部城镇化的城市。', '深圳地处中国华南地区、广东南部、珠江口东岸，', '东临大亚湾和大鹏湾，', '西濒珠江口和伶仃洋，', '南隔深圳河与香港相连，', '是粤港澳大湾区四大中心城市之一 [6]  、国家物流枢纽、国际性综合交通枢纽 [7]  、国际科技产业创新中心 [8]  、中国三大全国性金融中心之一 [9-10]  ，', '并全力建设中国特色社会主义先行示范区 [11]  、综合性国家科学中心 [12]  、全球海洋中心城市 [13]  。', '深圳水陆空铁口岸俱全，', '是中国拥有口岸数量最多、出入境人员最多、车流量最大的口岸城市。']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "1oqKk8CETocd",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def isNan(a):\n",
    "    return a != a\n",
    "\n",
    "\n",
    "class RatingData(data.Dataset):\n",
    "    def __init__(self, path, \n",
    "          word2index, \n",
    "          max_row = -1, \n",
    "          num_word_in_sentence = 10,\n",
    "          num_sentence_in_article = 10,\n",
    "          trainTestRate = 0.85, \n",
    "          isTrain = True, \n",
    "          wordCuter = thulac,\n",
    "          clean_file_name = 'ratings_clean_4_HAN.csv',\n",
    "          ):\n",
    "        self.token_list = []\n",
    "        self.label_list = []\n",
    "        # self.token_positions = torch.tensor([i for i in range(100)])\n",
    "\n",
    "        print(' balance_data.csv 所在path:',path) # 地址不应该包含 ratings.csv\n",
    "\n",
    "        ratings_clean_filename = os.path.join(path, clean_file_name)\n",
    "        ratings_filename = os.path.join(path, 'balance_data.csv')\n",
    "        if os.path.isfile(ratings_clean_filename):\n",
    "            clean_pd = pd.read_csv(ratings_clean_filename)\n",
    "        else:\n",
    "            print('没有找到缓存的文件%s, 读取源文件%s'%(ratings_clean_filename, ratings_filename))\n",
    "            ratings_pd = pd.read_csv(ratings_filename)\n",
    "            print('开始生成缓存文件%s'%(ratings_clean_filename))\n",
    "            clean_pd = pd.DataFrame({\n",
    "                'labels':[],\n",
    "                'data':[],\n",
    "            })\n",
    "            nonRatingCount = 0\n",
    "            for i, row in ratings_pd.iterrows():\n",
    "                if max_row != -1 and i > max_row:\n",
    "                    break\n",
    "                if not isinstance(row['data'], str) or row['data'] == '':\n",
    "                    # print(i + 1, row['comment'])\n",
    "                    nonRatingCount += 1\n",
    "                    continue\n",
    "                r0 = row['labels']\n",
    "                if r0 == -1:\n",
    "                    r0 = 2\n",
    "                    \n",
    "                if i % 10000 == 9999:\n",
    "                    print(i + 1, r0)\n",
    "\n",
    "                # 把文章切割成句子\n",
    "                sentences = splitText(row['data'])\n",
    "                tokens = []\n",
    "                for i in range(num_sentence_in_article):\n",
    "                  if i < len(sentences):\n",
    "                    # 把句子切割成词\n",
    "                    words = list(wordCuter.cut(sentences[i]))\n",
    "                    # 把词转成词向量中的index，不足时补全\n",
    "                    token = [ word2index[words[j]] if j < len(words) and words[j] in word2index else 0 \n",
    "                          for j in range(num_word_in_sentence)] \n",
    "                    tokens += token\n",
    "                  else:\n",
    "                    # 不足时补全\n",
    "                    tokens += [0] * num_word_in_sentence\n",
    "                \n",
    "                newRow = DotDict()\n",
    "\n",
    "                newRow.labels = [r0]\n",
    "                newRow.data = [json.dumps(tokens)]\n",
    "\n",
    "                clean_pd = clean_pd.append(pd.DataFrame(newRow), ignore_index=True)\n",
    "            print('空的评论数量： %d'%(nonRatingCount))\n",
    "            clean_pd.to_csv(ratings_clean_filename)\n",
    "\n",
    "        # 读取\n",
    "        if isTrain:\n",
    "            temp_pd = clean_pd.sample(frac=trainTestRate)\n",
    "        else:\n",
    "            temp_pd = clean_pd.sample(frac=trainTestRate)\n",
    "            temp_pd = clean_pd.drop(temp_pd.index.tolist(), axis=0)\n",
    "\n",
    "        for i, row in temp_pd.iterrows():\n",
    "            if max_row != -1 and i > max_row:\n",
    "                break\n",
    "\n",
    "            self.label_list.append(torch.tensor(row['labels']).long())\n",
    "            self.token_list.append(torch.from_numpy(numpy.array( json.loads(row['data']) ) ).long())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.token_list[index], self.label_list[index]#, self.token_positions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_list)\n",
    "    \n",
    "\n",
    "##%%\n",
    "\n",
    "ratingData = RatingData('../../DataSets/yf_dianping',\n",
    "                             word2index = word2index,\n",
    "                            #  max_row= 5,\n",
    "                             isTrain=True,\n",
    "                             trainTestRate = 0.8,\n",
    "                             wordCuter= jieba,\n",
    "                             clean_file_name='balance_data_4_HAN_original_balance.csv',\n",
    "                             # clean_file_name='balance_data_4_HAN_TWV.csv',\n",
    "                             )\n",
    "trainLoader = torch.utils.data.DataLoader(dataset=ratingData,\n",
    "                                          batch_size=512,\n",
    "                                          shuffle = True,\n",
    "                                          # num_workers = 8,\n",
    "                                          )\n",
    "ratingData2 = RatingData('../../DataSets/yf_dianping',\n",
    "                             word2index = word2index,\n",
    "                             # max_row= 200000,\n",
    "                             isTrain=False,\n",
    "                             trainTestRate = 0.8,\n",
    "                             wordCuter= jieba,\n",
    "                             clean_file_name='balance_data_4_HAN_original_balance.csv',\n",
    "                             # clean_file_name='balance_data_4_HAN_TWV.csv',\n",
    "                             )\n",
    "testLoader = torch.utils.data.DataLoader(dataset=ratingData2,\n",
    "                                          batch_size=128,\n",
    "                                          shuffle = True,\n",
    "                                          # num_workers = 8,\n",
    "                                          )\n",
    "print(len(ratingData.label_list))\n",
    "print(len(ratingData2.label_list))\n"
   ],
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " balance_data.csv 所在path: ../../DataSets/yf_dianping\n",
      "没有找到缓存的文件../../DataSets/yf_dianping\\balance_data_4_HAN_original_balance.csv, 读取源文件../../DataSets/yf_dianping\\balance_data.csv\n",
      "开始生成缓存文件../../DataSets/yf_dianping\\balance_data_4_HAN_original_balance.csv\n",
      "10000 2\n",
      "20000 2\n",
      "30000 2\n",
      "40000 2\n",
      "50000 2\n",
      "60000 2\n",
      "70000 2\n",
      "80000 2\n",
      "90000 2\n",
      "100000 2\n",
      "110000 2\n",
      "120000 2\n",
      "130000 2\n",
      "140000 2\n",
      "150000 2\n",
      "160000 2\n",
      "170000 2\n",
      "180000 2\n",
      "190000 2\n",
      "200000 2\n",
      "210000 2\n",
      "220000 2\n",
      "230000 1\n",
      "240000 1\n",
      "250000 1\n",
      "260000 1\n",
      "270000 1\n",
      "280000 1\n",
      "290000 1\n",
      "300000 1\n",
      "310000 1\n",
      "320000 1\n",
      "330000 1\n",
      "340000 1\n",
      "350000 1\n",
      "360000 1\n",
      "370000 1\n",
      "380000 1\n",
      "390000 1\n",
      "400000 1\n",
      "410000 1\n",
      "420000 1\n",
      "430000 1\n",
      "440000 1\n",
      "450000 0\n",
      "460000 0\n",
      "470000 0\n",
      "480000 0\n",
      "490000 0\n",
      "500000 0\n",
      "510000 0\n",
      "520000 0\n",
      "530000 0\n",
      "540000 0\n",
      "550000 0\n",
      "560000 0\n",
      "570000 0\n",
      "580000 0\n",
      "590000 0\n",
      "600000 0\n",
      "610000 0\n",
      "620000 0\n",
      "630000 0\n",
      "640000 0\n",
      "650000 0\n",
      "660000 0\n",
      "空的评论数量： 0\n",
      " balance_data.csv 所在path: ../../DataSets/yf_dianping\n",
      "530311\n",
      "132578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\FULIU-~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.779 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxwtXncXqESB",
    "colab_type": "text"
   },
   "source": [
    "## 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "isPrint = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "pQYgCURnTocn",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "54e89b88-efab-46e2-c4dd-ae4c3d776e96",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588176212969,
     "user_tz": -480,
     "elapsed": 5397,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    }
   },
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, num_input):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.W = nn.Linear(num_input, num_input)\n",
    "        self.U = nn.Linear(num_input, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isPrint : print(\"SelfAttention 前 x.shape：\", x.shape)\n",
    "        u = F.tanh(self.W(x))\n",
    "        if isPrint : print(\"--> F.tanh(self.W(x)) 后 u.shape：\", u.shape, \"self.W：\", self.W)\n",
    "        a = F.softmax(self.U(u), dim=1)\n",
    "        if isPrint : print(\"--> F.softmax(self.U(u), dim=1) 后 a.shape：\", a.shape, \"self.U：\", self.U)\n",
    "        res = torch.mul(a, x).sum(dim=1)\n",
    "        if isPrint : print(\"--> torch.mul(a, x).sum(dim=1) 后 res.shape：\", res.shape)\n",
    "        return res\n",
    "\n",
    "\n",
    "class HAN(nn.Module):\n",
    "    def __init__(self, num_embeddings = 5845,\n",
    "                 num_classes = 10,\n",
    "                 num_words = 20,        # 每句话最多多少个词\n",
    "                 num_sentence = 10,     # 一篇文章多少个句子\n",
    "                 embedding_dim = 200,\n",
    "                 hidden_size_gru = 50,\n",
    "                 hidden_size_att = 100,\n",
    "                 ):\n",
    "        super(HAN, self).__init__()\n",
    "\n",
    "        self.num_words = num_words\n",
    "        self.num_sentence = num_sentence\n",
    "        self.embed = nn.Embedding(num_embeddings, embedding_dim, 0)\n",
    "\n",
    "        self.GRU1 = nn.GRU(embedding_dim,\n",
    "                           hidden_size_gru,\n",
    "                           bidirectional=True,  # 双向  Default: ``False``\n",
    "                           batch_first=True,    # : If ``True``, then the input and output tensors are provided as (batch, seq, feature). Default: ``False``\n",
    "                           )\n",
    "        self.self_attention1 = SelfAttention(hidden_size_gru * 2)\n",
    "\n",
    "\n",
    "        self.GRU2 = nn.GRU(hidden_size_gru * 2,\n",
    "                           hidden_size_gru * 2,\n",
    "                           bidirectional=True,  # 双向  Default: ``False``\n",
    "                           batch_first=True,    # : If ``True``, then the input and output tensors are provided as (batch, seq, feature). Default: ``False``\n",
    "                           )\n",
    "        self.self_attention2 = SelfAttention(hidden_size_gru * 4)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size_att * 2, num_classes)\n",
    "\n",
    "    def forward(self, x:torch.Tensor, isSentenceSplit:bool=True):\n",
    "        if isPrint : print()\n",
    "        if isPrint : print()\n",
    "        if isPrint : print(\"x:\", x.shape)\n",
    "        if isSentenceSplit:\n",
    "            sentences = []\n",
    "\n",
    "            for i in range(self.num_sentence):\n",
    "                sentence = x[:, i * self.num_words: (i + 1) * self.num_words]\n",
    "                if isPrint : print(\"-> sentence:\", sentence.shape)\n",
    "                sentence = self.embed(sentence)\n",
    "                if isPrint : print(\"-> embed 后 sentence:\", sentence.shape)\n",
    "                sentence, _ = self.GRU1(sentence)\n",
    "                if isPrint : print(\"-> GRU1 后 sentence:\", sentence.shape)\n",
    "                sentence = self.self_attention1(sentence)\n",
    "                if isPrint : print(\"-> self_attention1 后 sentence:\", sentence.shape)\n",
    "                sentences.append(sentence)\n",
    "            sentences = torch.cat(sentences, dim=1)\n",
    "            if isPrint : print()\n",
    "            if isPrint : print(\"-> torch.cat(sentences, dim=1) 后 sentences:\", sentences.shape)\n",
    "            x = sentences.view(sentences.size(0), self.num_sentence, -1)\n",
    "            if isPrint : print(\"-> sentences.view 后 x:\", x.shape)\n",
    "        else:\n",
    "            sentences = self.embed(x)\n",
    "            if isPrint : print(\"-> embed 后 sentences:\", sentences.shape)\n",
    "            sentences, _ = self.GRU1(sentences)\n",
    "            if isPrint : print(\"-> GRU1 后 sentences:\", sentences.shape)\n",
    "            sentences = self.self_attention1(sentences)\n",
    "            if isPrint : print(\"-> self_attention1 后 sentences:\", sentences.shape)\n",
    "            x = sentences\n",
    "        if isPrint : print()\n",
    "        if isPrint : print(\"view2 后 x:\", x.shape)\n",
    "        x, _ = self.GRU2(x)\n",
    "        if isPrint : print(\"GRU2 后 x:\", x.shape)\n",
    "        x = self.self_attention2(x)\n",
    "        if isPrint : print(\"self_attention2 后 x:\", x.shape)\n",
    "        x = self.fc(x)\n",
    "        if isPrint : print(\"fc 后 x:\", x.shape)\n",
    "        return F.softmax(x, dim=1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for i in range(4):\n",
    "    newSentence = torch.range(i * 8, (i + 1) * 8 - 1).reshape(2,-1)\n",
    "    sentences.append(newSentence)\n",
    "    print(' -> ',newSentence)\n",
    "\n",
    "sentences = torch.cat(sentences)\n",
    "print('sentences ',sentences.size())\n",
    "print('sentences.size(0) ',sentences.size(0))\n",
    "x = sentences.view(sentences.size(0) * 2 , -1)\n",
    "print(\"x\", x, x.shape)\n",
    "# x = sentences.view(sentences.size(0) // 2 , -1)\n",
    "# print(\"x\", x, x.size())\n",
    "# x = sentences.view(x.size(0) * 2 , 2, -1)\n",
    "# print(\"x\", x, x.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "isSentenceSplit = True\n",
    "\n",
    "#创建模型\n",
    "wordEmbedding = torch.FloatTensor(wordEmbedding)\n",
    "num_embeddings = len(word2vec.wv.index2word)\n",
    "# Classical\n",
    "model = HAN(num_embeddings,\n",
    "            num_classes = 3,\n",
    "            embedding_dim = word2vec.wv.vector_size,\n",
    "            num_words = 10,\n",
    "            num_sentence = 10,\n",
    "            hidden_size_gru = 200,\n",
    "            hidden_size_att = 400,\n",
    "            )\n",
    "# model = HAN(num_embeddings,\n",
    "#             num_classes = 3,\n",
    "#             embedding_dim = word2vec.wv.vector_size,\n",
    "#             num_words = 100,\n",
    "#             hidden_size_gru = 200,\n",
    "#             hidden_size_att = 400,\n",
    "#             )\n",
    "print(model)\n",
    "\n",
    "modelParams = model.parameters()\n",
    "for param in modelParams:\n",
    "    if len(param.data.shape) > 1:\n",
    "        # print('---', param.data.shape, param.data)\n",
    "        torch.nn.init.kaiming_normal(param.data)\n",
    "        # print('--->', param.data)\n",
    "        \n",
    "model.embed.from_pretrained(wordEmbedding)\n",
    "\n",
    "\n",
    "##%% md\n",
    "\n",
    "## 开始训练过程\n",
    "\n",
    "##%%\n",
    "\n",
    "def trainOneEpoch(epoch, model:HAN, trainLoader, optimizer:Optimizer, lossFunc):\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        lossFunc = lossFunc.cuda()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    startTime = time.time()\n",
    "    for i, (x, y) in enumerate(trainLoader):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        outputs = model(x, isSentenceSplit)\n",
    "        loss = lossFunc(outputs, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 99:\n",
    "            print('Epoch %d, %d/%d, loss:%f ' % (epoch, i, len(trainLoader), loss))\n",
    "        # if i > 2000:\n",
    "        #     break \n",
    "    print('Epoch %d cost time: %.3fs' % (epoch, time.time() - startTime))\n",
    "\n",
    "\n",
    "def testModel(epoch, model:HAN, testLoader):\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    startTime = time.time()\n",
    "    for i, (x, y) in enumerate(testLoader):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        outputs = model(x, isSentenceSplit)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += len(y)\n",
    "        correct += predicted.data.eq(y.data).cpu().sum().numpy()\n",
    "        \n",
    "\n",
    "        if i % 50 == 49:\n",
    "            print('Epoch Test %d, %d/%d' % (epoch, i, len(testLoader)))\n",
    "        # if i > 2000:\n",
    "        #     break \n",
    "    print('Epoch Test %d cost time: %.3fs' % (epoch, time.time() - startTime))\n",
    "    print('准确率： %.3f' % (correct / total))\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def train(epoch, model, modelSavePath, isLoad, lr=0.0002):\n",
    "    last_acc = 0\n",
    "    if isLoad: \n",
    "        model.load_state_dict(torch.load(modelSavePath))\n",
    "        last_acc = testModel(epoch, model, testLoader)\n",
    "    # optimizer=torch.optim.SGD(model.parameters(), momentum=0.9, lr=0.001)\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.0002)\n",
    "    lossFunc =torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(epoch):\n",
    "        trainOneEpoch(epoch, model, trainLoader, optimizer, lossFunc)\n",
    "        acc = testModel(epoch, model, testLoader)\n",
    "        if last_acc < acc:\n",
    "          torch.save(model.state_dict(), modelSavePath)\n",
    "        last_acc = acc\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(20, model, 'EmotionAnalyzeModelData_ClassicalHAN_OB.model', False, lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(20, model, 'EmotionAnalyzeModelData_ClassicalHAN_OB.model', True, lr=0.0005)\n",
    "train(20, model, 'EmotionAnalyzeModelData_ClassicalHAN_OB.model', True, lr=0.0001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gzvump0AVdJY",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "\n",
    "def eval(model, modelSavePath, isLoad = True):\n",
    "    if isLoad: model.load_state_dict(torch.load(modelSavePath))\n",
    "    testModel(0, model, testLoader)\n",
    "\n",
    "eval(model, 'EmotionAnalyzeModelData_ClassicalHAN_OB.model', isLoad = True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "wKXjE4UBTodM",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "lossFunc =torch.nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "_, predicted = torch.max(input.data, 1)\n",
    "output = lossFunc(input, target)\n",
    "print('input',input, '\\n target',target, '\\n output', output)\n",
    "output = lossFunc(input, predicted)\n",
    "print('predicted',predicted, '\\n target',target, '\\n output', output)\n",
    "\n",
    "print(predicted.data.eq(target).cpu().sum())\n",
    "print(target.data.eq(predicted).cpu().sum())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Z1zQWh6DTodQ",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# 何凯明初始化\n",
    "\n",
    "w = torch.Tensor(3, 5, 2)\n",
    "print(w)\n",
    "print(nn.init.kaiming_uniform(w))\n",
    "print(w)\n",
    "w = torch.Tensor(3, 5, 2)\n",
    "print(w)\n",
    "print(torch.nn.init.kaiming_normal(w))\n",
    "print(w)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}