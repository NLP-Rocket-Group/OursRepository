{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"name":"HAN_original_balance_TWV.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"k-CIbt4LqMiS","colab_type":"text"},"source":["## 开始"]},{"cell_type":"code","metadata":{"id":"zBDjArJoT65A","colab_type":"code","outputId":"318af7be-304a-4c73-f5c9-89f7f17e36bf","executionInfo":{"status":"ok","timestamp":1588437292027,"user_tz":-480,"elapsed":95516,"user":{"displayName":"fuliu fuliu","photoUrl":"","userId":"17240005970423735187"}},"colab":{"base_uri":"https://localhost:8080/","height":343}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/My\\ Drive/Colab Notebooks/OursRepository/public-opinion-monitor\n","\n","!pip install thulac\n","!pip install jieba"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content/drive/My Drive/Colab Notebooks/OursRepository/public-opinion-monitor\n","Collecting thulac\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/f2/f5893d06e744fe228f06ea1f340c90d15f55b0e3b0148762ab234af4573c/thulac-0.2.1.tar.gz (52.9MB)\n","\u001b[K     |████████████████████████████████| 52.9MB 58kB/s \n","\u001b[?25hBuilding wheels for collected packages: thulac\n","  Building wheel for thulac (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for thulac: filename=thulac-0.2.1-cp36-none-any.whl size=53141671 sha256=def17a12758b3354350a38b19039e981a23e982f8de8148abe20362a162623f4\n","  Stored in directory: /root/.cache/pip/wheels/db/36/4a/1ac1e9b9ce727a9dfc7fa20092992707d7da162df871c8488f\n","Successfully built thulac\n","Installing collected packages: thulac\n","Successfully installed thulac-0.2.1\n","Requirement already satisfied: jieba in /usr/local/lib/python3.6/dist-packages (0.42.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"9Eush51XTobh","colab_type":"code","outputId":"251aa135-950f-4641-c7a9-293d312eac2a","executionInfo":{"status":"ok","timestamp":1588441600002,"user_tz":-480,"elapsed":4412,"user":{"displayName":"fuliu fuliu","photoUrl":"","userId":"17240005970423735187"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# from ClassicalHANModel import *\n","# from HANModel import *\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim.optimizer import Optimizer\n","import numpy\n","import time, math\n","import torch.utils.data as data\n","import os\n","import pandas as pd\n","import json\n","\n","import jieba\n","import thulac\n","thulacObj = thulac.thulac(seg_only=True)\n","class thulac_cutor:\n","    def cut(self,sentence:str):\n","        return thulacObj.fast_cut(sentence, text=True)\n","thulac_cutor = thulac_cutor()\n"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Model loaded succeed\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"3HmMThKsTobr","colab_type":"text"},"source":["## 读取词向量\n","建立词语列表"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"hhmz_drpTobs","colab_type":"code","outputId":"8c0316c0-9f0e-410d-80da-eaf8bacc26a5","executionInfo":{"status":"ok","timestamp":1588437320377,"user_tz":-480,"elapsed":123846,"user":{"displayName":"fuliu fuliu","photoUrl":"","userId":"17240005970423735187"}},"colab":{"base_uri":"https://localhost:8080/","height":91}},"source":["from gensim.models import KeyedVectors, Word2Vec\n","\n","file = '../../DataSets/Word2Vect/Tencent_AILab_ChineseEmbedding/Tencent_AILab_ChineseEmbedding_Min.txt'\n","# word2vec = Word2Vec.load(file)\n","word2vec = KeyedVectors.load_word2vec_format(file, binary=False,limit=100000)\n","# word2vec = KeyedVectors.load_word2vec_format(file, binary=False)\n","word2vec.init_sims(replace=True)  # 神奇，很省内存，可以运算most_similar\n","word2vec.vector_size"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["200"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"_zCpfSvDTocC","colab_type":"code","outputId":"d6092944-9226-4ef5-b071-8adf91e8460f","executionInfo":{"status":"ok","timestamp":1588437320378,"user_tz":-480,"elapsed":123838,"user":{"displayName":"fuliu fuliu","photoUrl":"","userId":"17240005970423735187"}},"colab":{"base_uri":"https://localhost:8080/","height":577}},"source":["print(word2vec)\n","print(len(word2vec.wv.index2word))\n","print(word2vec.wv.index2word[0])\n","print(word2vec.wv.index2word[1])\n","print(word2vec.wv.index2word[2])\n","print(word2vec.wv.index2word[1522])\n","print(word2vec.wv.index2entity[1522])\n","print(word2vec.similar_by_word('中国'))\n","print(word2vec.similar_by_word('天才'))\n","print(word2vec.wv)\n","print('word2vec.wv.vocab ---- >', word2vec.wv.vocab)\n","print(word2vec.wv.index2word)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["IOPub data rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_data_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  This is separate from the ipykernel package so we can avoid doing imports until\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  after removing the cwd from sys.path.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  \"\"\"\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  import sys\n","/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n","  if np.issubdtype(vec.dtype, np.int):\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  # Remove the CWD from sys.path while we load stuff.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  # This is added back by InteractiveShellApp.init_path()\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  if sys.path[0] == '':\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"lzs2-uceTocG","colab_type":"code","outputId":"24310c16-d109-4554-baaa-3db006bdab61","executionInfo":{"status":"ok","timestamp":1588437320379,"user_tz":-480,"elapsed":123831,"user":{"displayName":"fuliu fuliu","photoUrl":"","userId":"17240005970423735187"}},"colab":{"base_uri":"https://localhost:8080/","height":181}},"source":["wordEmbedding = [word2vec.wv[word]  for word in word2vec.wv.index2word]\n","word2index = { word:i for i, word in enumerate(word2vec.wv.index2word)}\n","# print(wordEmbedding[:2])\n","# print(word2index['中国'])\n","# print(word2index['天才'])\n","print(word2index['喜欢'])\n","print(word2index['不'])\n","print(word2index['爽'])\n","print(word2index['超赞'])\n","\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  \"\"\"Entry point for launching an IPython kernel.\n"],"name":"stderr"},{"output_type":"stream","text":["217\n","64\n","5313\n","41047\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  \n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"cy9vhIUoTocL","colab_type":"text"},"source":["## 读取训练数据"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"gZGmiumwTocN","colab_type":"code","colab":{}},"source":["# def compute_ngrams(word, num_min = 1, num_max = 3):\n","#     ngrams =[]\n","#     for ngram_length in range(num_min, min(len(word), num_max) + 1):\n","#         for i in range(len(word) - ngram_length + 1):\n","#             # print(i, i + ngram_length)\n","#             ngrams.append(word[i : i + ngram_length])\n","#     # print(ngrams)\n","#     return list(set(ngrams))\n","\n","# print(compute_ngrams('you'))\n","# print(compute_ngrams('I Think'))\n","# print(compute_ngrams('中华人民共和国万岁'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"ulsoTSc2TocT","colab_type":"code","colab":{}},"source":["# # 从词向量文本文件 word2vec 中获取词向量，如果获取到直接返回，若没有获取到，那么把这个词拆开\n","# # 成为 ngrams 的新词组，并在 word2vec 中找新词组中的词向量并相加取平均，最后得到平均词向量输出\n","# def wordVec(word, word2vec, min_n = 1, max_n = 3):\n","#     # 确认词向量维度\n","#     word_size = word2vec.wv.syn0[0].shape[0]\n","\n","#     # 如果在词典之中，直接返回词向量\n","#     if word in word2vec.wv.vocab.keys():\n","#         return word2vec[word]\n","#     else:\n","#         # 计算word的ngrams词组\n","#         ngrams = compute_ngrams(word, min_n, max_n)\n","#         # 不在词典的情况下\n","#         word_vec = numpy.zeros(word_size, dtype=numpy.float32)\n","#         ngrams_found = 0\n","#         ngrams_single = [ng for ng in ngrams if len(ng) == 1]\n","#         ngrams_more = [ng for ng in ngrams if len(ng) > 1]\n","#         # 先只接受2个单词长度以上的词向量\n","#         for ngram in ngrams_more:\n","#             if ngram in word2vec.wv.vocab.keys():\n","#                 word_vec += word2vec[ngram]\n","#                 ngrams_found += 1\n","#                 #print(ngram)\n","#         # 如果，没有匹配到，那么最后是考虑单个词向量\n","#         if ngrams_found == 0:\n","#             for ngram in ngrams_single:\n","#                 word_vec += word2vec[ngram]\n","#                 ngrams_found += 1\n","#         if word_vec.any():\n","#             return word_vec / max(1, ngrams_found)\n","#         else:\n","#             # 不抛出异常，而是打印提示，并返回0向量。\n","#             print(KeyError('all ngrams for word %s absent from model' % word))\n","#             return word_vec"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"08tJP4HHTocX","colab_type":"code","colab":{}},"source":["class DotDict(dict):\n","    def __init__(self, *args, **kwargs):\n","        dict.__init__(self, *args, **kwargs)\n","        self.__dict__ = self\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"XGUX_N8KToca","colab_type":"code","outputId":"febabd25-4fb0-450e-e4c2-646d5cdf4d6a","executionInfo":{"status":"ok","timestamp":1588437320381,"user_tz":-480,"elapsed":123798,"user":{"displayName":"fuliu fuliu","photoUrl":"","userId":"17240005970423735187"}},"colab":{"base_uri":"https://localhost:8080/","height":215}},"source":["import pandas as pd\n","\n","dic = {'a':[1, 2, 3, 4], 'b':[5, 6, 7, 8],\n","'c':[9, 10, 11, 12], 'd':[13, 14, 15, 16]}\n","df1=pd.DataFrame(dic)\n","print(df1)\n","df2=df1.sample(frac=0.75)\n","print(df2)\n","# rowlist=[]\n","# for indexs in df2.index:\n","#     rowlist.append(indexs)\n","df3=df1.drop(df2.index.to_list(),axis=0)\n","print(df3)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["   a  b   c   d\n","0  1  5   9  13\n","1  2  6  10  14\n","2  3  7  11  15\n","3  4  8  12  16\n","   a  b   c   d\n","3  4  8  12  16\n","1  2  6  10  14\n","2  3  7  11  15\n","   a  b  c   d\n","0  1  5  9  13\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lfY5Is_4I34d","colab_type":"code","colab":{}},"source":["# import re\n","\n","# def splitText(text:str, splitChar = '(。|，|,|！|\\!|？|\\?|\\n|\\t)'):\n","#   '''\n","#   句子切分分隔符\n","#   '''\n","#   contents = re.split(splitChar, text)\n","#   contents = [\"\".join([a, b]) if b != '\\n' and b != '\\t' else \"\" #a + \"。\"\n","#               for a, b in zip(contents[0::2], contents[1::2])]\n","#   contents = [content for content in contents if content.strip() != '' and content[0] != '。']\n","#   # for i, sen in enumerate(contents):\n","#   #     print(i, sen)\n","#   return contents\n","\n","# testStr = \"\"\"\n","# 深圳，简称“深”，别称鹏城，是广东省副省级市、计划单列市、超大城市，国务院批复确定的中国经济特区、全国性经济中心城市和国际化城市 [1]  。截至2018年末，全市下辖9个区，总面积1997.47平方千米，建成区面积927.96平方千米，常住人口1302.66万人，城镇人口1302.66万人，城镇化率100%，是中国第一个全部城镇化的城市。 [2-5] \n","# 深圳地处中国华南地区、广东南部、珠江口东岸，东临大亚湾和大鹏湾，西濒珠江口和伶仃洋，南隔深圳河与香港相连，是粤港澳大湾区四大中心城市之一 [6]  、国家物流枢纽、国际性综合交通枢纽 [7]  、国际科技产业创新中心 [8]  、中国三大全国性金融中心之一 [9-10]  ，并全力建设中国特色社会主义先行示范区 [11]  、综合性国家科学中心 [12]  、全球海洋中心城市 [13]  。深圳水陆空铁口岸俱全，是中国拥有口岸数量最多、出入境人员最多、车流量最大的口岸城市。 [14]\n","# \"\"\"\n","\n","# print(splitText(testStr))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"1oqKk8CETocd","colab_type":"code","colab":{}},"source":["def isNan(a):\n","    return a != a\n","\n","class RatingData(data.Dataset):\n","    def __init__(self, path, \n","                 word2index, \n","                 max_row = -1, \n","                 trainTestRate = 0.85, \n","                 isTrain = True, \n","                 wordCuter = thulac,\n","                 dataFile = 'balance_data.csv',\n","                 clean_file_name = 'ratings_clean_4_HAN.csv',\n","                 ):\n","        self.token_list = []\n","        self.label_list = []\n","\n","        print(' balance_data.csv 所在path:',path) # 地址不应该包含 ratings.csv\n","\n","        ratings_clean_filename = os.path.join(path, clean_file_name)\n","        ratings_filename = os.path.join(path, dataFile)\n","        if os.path.isfile(ratings_clean_filename):\n","            clean_pd = pd.read_csv(ratings_clean_filename)\n","        else:\n","            print('没有找到缓存的文件%s, 读取源文件%s'%(ratings_clean_filename, ratings_filename))\n","            ratings_pd = pd.read_csv(ratings_filename)\n","            print('开始生成缓存文件%s'%(ratings_clean_filename))\n","            clean_pd = pd.DataFrame({\n","                'labels':[],\n","                'data':[],\n","            })\n","            nonRatingCount = 0\n","            for i, row in ratings_pd.iterrows():\n","                if max_row != -1 and i > max_row:\n","                    break\n","                if not isinstance(row['data'], str) or row['data'] == '':\n","                    # print(i + 1, row['comment'])\n","                    nonRatingCount += 1\n","                    continue\n","                r0 = row['labels']\n","                if r0 == -1:\n","                    r0 = 2\n","                    \n","                if i % 10000 == 9999:\n","                    print(i + 1, r0)\n","\n","                words = list(wordCuter.cut(row['data']))\n","                # 0 在词向量集中是‘，’，换个词向量集可能表示其他\n","                token = [ word2index[words[i]] if i < len(words) and words[i] in word2index else 0 \n","                          for i in range(100)] \n","                \n","                newRow = DotDict()\n","\n","                newRow.labels = [r0]\n","                newRow.data = [json.dumps(token)]\n","\n","                clean_pd = clean_pd.append(pd.DataFrame(newRow), ignore_index=True)\n","            print('空的评论数量： %d'%(nonRatingCount))\n","            clean_pd.to_csv(ratings_clean_filename)\n","\n","        # 读取\n","        if isTrain:\n","            temp_pd = clean_pd.sample(frac=trainTestRate)\n","        else:\n","            temp_pd = clean_pd.sample(frac=trainTestRate)\n","            temp_pd = clean_pd.drop(temp_pd.index.tolist(), axis=0)\n","\n","        for i, row in temp_pd.iterrows():\n","            if max_row != -1 and i > max_row:\n","                break\n","\n","            self.label_list.append(torch.tensor(row['labels']).long())\n","            self.token_list.append(torch.from_numpy(numpy.array( json.loads(row['data']) ) ).long())\n","\n","    def __getitem__(self, index):\n","        return self.token_list[index], self.label_list[index]#, self.token_positions\n","\n","    def __len__(self):\n","        return len(self.label_list)\n","    \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bkLYEVtRpAP4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":89},"outputId":"e38d559a-c192-4789-8862-d3d3bb2157b3","executionInfo":{"status":"ok","timestamp":1588437440821,"user_tz":-480,"elapsed":244204,"user":{"displayName":"fuliu fuliu","photoUrl":"","userId":"17240005970423735187"}}},"source":["\n","ratingData = RatingData('../../DataSets/yf_dianping',\n","                             word2index = word2index,\n","                            #  max_row= 5,\n","                             isTrain=True,\n","                             trainTestRate = 0.8,\n","                             wordCuter = jieba,\n","                             dataFile = 'balance_data.csv',\n","                             clean_file_name='balance_data_4_HAN_TWV.csv',\n","                             )\n","trainLoader = torch.utils.data.DataLoader(dataset=ratingData,\n","                                          batch_size=512,\n","                                          shuffle = True,\n","                                          num_workers = 8,\n","                                          )\n","ratingData2 = RatingData('../../DataSets/yf_dianping',\n","                             word2index = word2index,\n","                             # max_row= 200000,\n","                             isTrain=False,\n","                             trainTestRate = 0.8,\n","                             wordCuter = jieba,\n","                             dataFile = 'balance_data.csv',\n","                             clean_file_name='balance_data_4_HAN_TWV.csv',\n","                             )\n","testLoader = torch.utils.data.DataLoader(dataset=ratingData2,\n","                                          batch_size=512,\n","                                          shuffle = True,\n","                                          num_workers = 8,\n","                                          )\n","print(len(ratingData.label_list))\n","print(len(ratingData2.label_list))"],"execution_count":12,"outputs":[{"output_type":"stream","text":[" balance_data.csv 所在path: ../../DataSets/yf_dianping\n"," balance_data.csv 所在path: ../../DataSets/yf_dianping\n","530311\n","132578\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uxwtXncXqESB","colab_type":"text"},"source":["## 创建模型"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"tFUNf2DRnaO_","colab_type":"code","colab":{}},"source":["isPrint = False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"pQYgCURnTocn","colab_type":"code","colab":{}},"source":["# class SelfAttention(nn.Module):\n","#     def __init__(self, num_input):\n","#         super(SelfAttention, self).__init__()\n","#         self.W = nn.Linear(num_input, num_input)\n","#         self.U = nn.Linear(num_input, 1)\n","\n","#     def forward(self, x):\n","#         if isPrint : print(\"SelfAttention 前 x.shape：\", x.shape)\n","#         u = F.tanh(self.W(x))\n","#         if isPrint : print(\"--> F.tanh(self.W(x)) 后 u.shape：\", u.shape, \"self.W：\", self.W)\n","#         a = F.softmax(self.U(u), dim=1)\n","#         if isPrint : print(\"--> F.softmax(self.U(u), dim=1) 后 a.shape：\", a.shape, \"self.U：\", self.U)\n","#         res = torch.mul(a, x).sum(dim=1)\n","#         if isPrint : print(\"--> torch.mul(a, x).sum(dim=1) 后 res.shape：\", res.shape)\n","#         return res\n","\n","\n","# class HAN(nn.Module):\n","#     def __init__(self, num_embeddings = 5845,\n","#                  num_classes = 10,\n","#                  num_words = 20,        # 每句话最多多少个词\n","#                  num_sentence = 10,     # 一篇文章多少个句子\n","#                  embedding_dim = 200,\n","#                  hidden_size_gru = 50,\n","#                  hidden_size_att = 100,\n","#                  ):\n","#         super(HAN, self).__init__()\n","\n","#         self.num_words = num_words\n","#         self.num_sentence = num_sentence\n","#         self.embed = nn.Embedding(num_embeddings, embedding_dim, 0)\n","\n","#         self.GRU1 = nn.GRU(embedding_dim,\n","#                            hidden_size_gru,\n","#                            bidirectional=True,  # 双向  Default: ``False``\n","#                            batch_first=True,    # : If ``True``, then the input and output tensors are provided as (batch, seq, feature). Default: ``False``\n","#                            )\n","#         self.self_attention1 = SelfAttention(hidden_size_gru * 2)\n","\n","\n","#         self.GRU2 = nn.GRU(hidden_size_gru * 2,\n","#                            hidden_size_gru * 2,\n","#                            bidirectional=True,  # 双向  Default: ``False``\n","#                            batch_first=True,    # : If ``True``, then the input and output tensors are provided as (batch, seq, feature). Default: ``False``\n","#                            )\n","#         self.self_attention2 = SelfAttention(hidden_size_gru * 4)\n","\n","#         self.fc = nn.Linear(hidden_size_gru * 4, num_classes)\n","\n","#     def forward(self, x:torch.Tensor, isSentenceSplit:bool=True):\n","#         if isPrint : print()\n","#         if isPrint : print()\n","#         if isPrint : print(\"x:\", x.shape)\n","#         if isSentenceSplit:\n","#             sentences = []\n","\n","#             for i in range(self.num_sentence):\n","#                 sentence = x[:, i * self.num_words: (i + 1) * self.num_words]\n","#                 if isPrint : print(\"-> sentence:\", sentence.shape)\n","#                 sentence = self.embed(sentence)\n","#                 if isPrint : print(\"-> embed 后 sentence:\", sentence.shape)\n","#                 sentence, _ = self.GRU1(sentence)\n","#                 if isPrint : print(\"-> GRU1 后 sentence:\", sentence.shape)\n","#                 sentence = self.self_attention1(sentence)\n","#                 if isPrint : print(\"-> self_attention1 后 sentence:\", sentence.shape)\n","#                 sentences.append(sentence)\n","#             sentences = torch.cat(sentences, dim=1)\n","#             if isPrint : print()\n","#             if isPrint : print(\"-> torch.cat(sentences, dim=1) 后 sentences:\", sentences.shape)\n","#             x = sentences.view(sentences.size(0), self.num_sentence, -1)\n","#             if isPrint : print(\"-> sentences.view 后 x:\", x.shape)\n","#         else:\n","#             sentences = self.embed(x)\n","#             if isPrint : print(\"-> embed 后 sentences:\", sentences.shape)\n","#             sentences, _ = self.GRU1(sentences)\n","#             if isPrint : print(\"-> GRU1 后 sentences:\", sentences.shape)\n","#             sentences = self.self_attention1(sentences)\n","#             if isPrint : print(\"-> self_attention1 后 sentences:\", sentences.shape)\n","#             x = sentences\n","#         if isPrint : print()\n","#         if isPrint : print(\"view2 后 x:\", x.shape)\n","#         x, _ = self.GRU2(x)\n","#         if isPrint : print(\"GRU2 后 x:\", x.shape)\n","#         x = self.self_attention2(x)\n","#         if isPrint : print(\"self_attention2 后 x:\", x.shape)\n","#         x = self.fc(x)\n","#         if isPrint : print(\"fc 后 x:\", x.shape)\n","#         return F.softmax(x, dim=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u_mTB-ILkP5w","colab_type":"code","colab":{}},"source":["class SelfAttention(nn.Module):\n","    def __init__(self, num_input, num_hidden):\n","        super(SelfAttention, self).__init__()\n","        self.W = nn.Linear(num_input, num_hidden)\n","        self.U = nn.Linear(num_hidden, 1)\n","\n","    def forward(self, x):\n","        u = F.tanh(self.W(x))\n","        a = F.softmax(self.U(u), dim=1)\n","        return torch.mul(a, x).sum(dim=1)\n","\n","\n","class HAN(nn.Module):\n","    def __init__(self, num_embeddings = 5845,\n","                 num_classes = 10,\n","                 num_words = 100,\n","                 embedding_dim = 200,\n","                 hidden_size_gru = 50,\n","                 hidden_size_att = 100,\n","                 ):\n","        super(HAN, self).__init__()\n","\n","        self.num_words = num_words\n","        self.embed = nn.Embedding(num_embeddings, embedding_dim, 0)\n","\n","        self.GRU1 = nn.GRU(embedding_dim,\n","                           hidden_size_gru,\n","                           bidirectional=True, # 双向  Default: ``False``\n","                           batch_first=True, # : If ``True``, then the input and output tensors are provided as (batch, seq, feature). Default: ``False``\n","                           )\n","        self.self_attention1 = SelfAttention(hidden_size_gru * 2, hidden_size_att)\n","\n","\n","        self.GRU2 = nn.GRU(hidden_size_gru * 2,\n","                           hidden_size_gru,\n","                           bidirectional=True,  # 双向  Default: ``False``\n","                           batch_first=True,    # : If ``True``, then the input and output tensors are provided as (batch, seq, feature). Default: ``False``\n","                           )\n","        self.self_attention2 = SelfAttention(hidden_size_gru * 2, hidden_size_att)\n","\n","        # self.fc = nn.Linear(hidden_size_att, num_classes)\n","        self.fc = nn.Linear(hidden_size_gru * 2, num_classes)\n","\n","    def forward(self, x:torch.Tensor):\n","        # view() 说明：Returns a new tensor with the same data as the self tensor but of a different shape.\n","        # size() 说明：Returns the size of the self tensor. The returned value is a subclass of tuple.\n","        # contiguous() 说明： Returns a contiguous tensor containing the same data as self tensor. If self tensor is contiguous, this function returns the self tensor.\n","\n","        '''\n","x: torch.Size([128, 100]) 100\n","view 后 x: torch.Size([12800, 1])\n","embed 后 x: torch.Size([12800, 1, 200])\n","GRU1 后 x: torch.Size([12800, 1, 400]) self.GRU1: GRU(200, 200, batch_first=True, bidirectional=True)\n","SelfAttention 前 x.shape： torch.Size([12800, 1, 400])\n","-> F.tanh(self.W(x)) 后 u.shape： torch.Size([12800, 1, 400]) self.W： Linear(in_features=400, out_features=400, bias=True)\n","-> F.softmax(self.U(u), dim=1) 后 a.shape： torch.Size([12800, 1, 1]) self.U： Linear(in_features=400, out_features=1, bias=True)\n","-> torch.mul(a, x).sum(dim=1) 后 res.shape： torch.Size([12800, 400])\n","self_attention1 后 x: torch.Size([12800, 400])\n","view2 后 x: torch.Size([128, 100, 400])\n","GRU2 后 x: torch.Size([128, 100, 400]) self.GRU2: GRU(400, 200, batch_first=True, bidirectional=True)\n","SelfAttention 前 x.shape： torch.Size([128, 100, 400])\n","-> F.tanh(self.W(x)) 后 u.shape： torch.Size([128, 100, 400]) self.W： Linear(in_features=400, out_features=400, bias=True)\n","-> F.softmax(self.U(u), dim=1) 后 a.shape： torch.Size([128, 100, 1]) self.U： Linear(in_features=400, out_features=1, bias=True)\n","-> torch.mul(a, x).sum(dim=1) 后 res.shape： torch.Size([128, 400])\n","self_attention2 后 x: torch.Size([128, 400])\n","fc 后 x: torch.Size([128, 3])\n","        '''\n","        # print(\"x:\", x.shape, self.num_words)\n","        x = x.view(x.size(0) * self.num_words, -1).contiguous()\n","        # print(\"view 后 x:\", x.shape)\n","        x = self.embed(x)\n","        # print(\"embed 后 x:\", x.shape)\n","        x, _ = self.GRU1(x)\n","        # print(\"GRU1 后 x:\", x.shape)\n","        # 根据打印结果显示，维度1的大小是1，self_attention1中计算后softmax最后得到了一堆1的值，\n","        # 然后此结果又和x相乘，最后得到的还是x，所以整个self_attention1相当于没有计算。所以这里不要这一行\n","        # x = self.self_attention1(x)  \n","        x = x.sum(1)\n","        # print(\"self_attention1 后 x:\", x.shape)\n","        x = x.view(x.size(0) // self.num_words, self.num_words, -1)\n","        # print(\"view2 后 x:\", x.shape)\n","        x, _ = self.GRU2(x)\n","        # print(\"GRU2 后 x:\", x.shape)\n","        x = self.self_attention2(x)\n","        # print(\"self_attention2 后 x:\", x.shape)\n","        x = self.fc(x)\n","        # print(\"fc 后 x:\", x.shape)\n","        return F.softmax(x, dim=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"2uMgphu4naPD","colab_type":"code","outputId":"16349b3a-1d3d-439c-f863-59e338955ebb","executionInfo":{"status":"ok","timestamp":1588449906917,"user_tz":-480,"elapsed":8291869,"user":{"displayName":"fuliu fuliu","photoUrl":"","userId":"17240005970423735187"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["\n","#创建模型\n","wordEmbedding = torch.FloatTensor(wordEmbedding)\n","num_embeddings = len(word2vec.wv.index2word)\n","# Classical\n","# model = HAN(num_embeddings,\n","#             num_classes = 3,\n","#             embedding_dim = word2vec.wv.vector_size,\n","#             num_words = 10,\n","#             num_sentence = 10,\n","#             hidden_size_gru = 512,\n","#             hidden_size_att = 768,\n","#             )\n","model = HAN(num_embeddings,\n","            num_classes = 3,\n","            embedding_dim = word2vec.wv.vector_size,\n","            num_words = 100,\n","            hidden_size_gru = 300,\n","            hidden_size_att = 500,\n","            )\n","print(model)\n","\n","modelParams = model.parameters()\n","for param in modelParams:\n","    if len(param.data.shape) > 1:\n","        # print('---', param.data.shape, param.data)\n","        torch.nn.init.kaiming_normal(param.data)\n","        # print('--->', param.data)\n","        \n","model.embed.from_pretrained(wordEmbedding)\n","\n","\n","##%% md\n","\n","## 开始训练过程\n","\n","##%%\n","\n","def trainOneEpoch(epoch, model:HAN, trainLoader, optimizer:Optimizer, lossFunc):\n","    if torch.cuda.is_available():\n","        model = model.cuda()\n","        lossFunc = lossFunc.cuda()\n","\n","    model.train()\n","\n","    startTime = time.time()\n","    for i, (x, y) in enumerate(trainLoader):\n","        if torch.cuda.is_available():\n","            x = x.cuda()\n","            y = y.cuda()\n","\n","        outputs = model(x)\n","        loss = lossFunc(outputs, y)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if i % 100 == 99:\n","            print('Epoch %d, %d/%d, loss:%f ' % (epoch, i, len(trainLoader), loss))\n","        # if i > 2000:\n","        #     break \n","    print('Epoch %d cost time: %.3fs' % (epoch, time.time() - startTime))\n","\n","\n","def testModel(epoch, model:HAN, testLoader):\n","    if torch.cuda.is_available():\n","        model = model.cuda()\n","\n","    model.eval()\n","\n","    total = 0\n","    correct = 0\n","\n","    startTime = time.time()\n","    for i, (x, y) in enumerate(testLoader):\n","        if torch.cuda.is_available():\n","            x = x.cuda()\n","            y = y.cuda()\n","\n","        outputs = model(x)\n","        _, predicted = torch.max(outputs.data, 1)\n","\n","        total += len(y)\n","        correct += predicted.data.eq(y.data).cpu().sum().numpy()\n","        \n","\n","        if i % 50 == 49:\n","            print('Epoch Test %d, %d/%d' % (epoch, i, len(testLoader)))\n","        # if i > 2000:\n","        #     break \n","    print('Epoch Test %d cost time: %.3fs' % (epoch, time.time() - startTime))\n","    print('准确率： %.3f' % (correct / total))\n","    return correct / total\n","\n","\n","def train(epoch, model, modelSavePath, isLoad, lr=0.0002):\n","    last_acc = 0\n","    if isLoad: \n","        model.load_state_dict(torch.load(modelSavePath))\n","        last_acc = testModel(epoch, model, testLoader)\n","    # optimizer=torch.optim.SGD(model.parameters(), momentum=0.9, lr=0.001)\n","    optimizer=torch.optim.Adam(model.parameters(), lr=0.0002)\n","    lossFunc =torch.nn.CrossEntropyLoss()\n","    for epoch in range(epoch):\n","        trainOneEpoch(epoch, model, trainLoader, optimizer, lossFunc)\n","        acc = testModel(epoch, model, testLoader)\n","        if last_acc < acc:\n","          torch.save(model.state_dict(), modelSavePath)\n","        last_acc = acc\n","\n","train(20, model, 'data/EmotionAnalyzeModel_balance_TWV.model', False, lr=0.001)\n","    "],"execution_count":23,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  \n"],"name":"stderr"},{"output_type":"stream","text":["HAN(\n","  (embed): Embedding(99999, 200, padding_idx=0)\n","  (GRU1): GRU(200, 300, batch_first=True, bidirectional=True)\n","  (self_attention1): SelfAttention(\n","    (W): Linear(in_features=600, out_features=500, bias=True)\n","    (U): Linear(in_features=500, out_features=1, bias=True)\n","  )\n","  (GRU2): GRU(600, 300, batch_first=True, bidirectional=True)\n","  (self_attention2): SelfAttention(\n","    (W): Linear(in_features=600, out_features=500, bias=True)\n","    (U): Linear(in_features=500, out_features=1, bias=True)\n","  )\n","  (fc): Linear(in_features=600, out_features=3, bias=True)\n",")\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 0, 99/1036, loss:0.939127 \n","Epoch 0, 199/1036, loss:0.887987 \n","Epoch 0, 299/1036, loss:0.885514 \n","Epoch 0, 399/1036, loss:0.870773 \n","Epoch 0, 499/1036, loss:0.865416 \n","Epoch 0, 599/1036, loss:0.890627 \n","Epoch 0, 699/1036, loss:0.851046 \n","Epoch 0, 799/1036, loss:0.849178 \n","Epoch 0, 899/1036, loss:0.841529 \n","Epoch 0, 999/1036, loss:0.858986 \n","Epoch 0 cost time: 385.529s\n","Epoch Test 0, 49/259\n","Epoch Test 0, 99/259\n","Epoch Test 0, 149/259\n","Epoch Test 0, 199/259\n","Epoch Test 0, 249/259\n","Epoch Test 0 cost time: 29.171s\n","准确率： 0.688\n","Epoch 1, 99/1036, loss:0.832145 \n","Epoch 1, 199/1036, loss:0.857766 \n","Epoch 1, 299/1036, loss:0.839933 \n","Epoch 1, 399/1036, loss:0.809588 \n","Epoch 1, 499/1036, loss:0.858444 \n","Epoch 1, 599/1036, loss:0.830124 \n","Epoch 1, 699/1036, loss:0.799018 \n","Epoch 1, 799/1036, loss:0.851675 \n","Epoch 1, 899/1036, loss:0.820567 \n","Epoch 1, 999/1036, loss:0.813953 \n","Epoch 1 cost time: 384.086s\n","Epoch Test 1, 49/259\n","Epoch Test 1, 99/259\n","Epoch Test 1, 149/259\n","Epoch Test 1, 199/259\n","Epoch Test 1, 249/259\n","Epoch Test 1 cost time: 29.197s\n","准确率： 0.720\n","Epoch 2, 99/1036, loss:0.815354 \n","Epoch 2, 199/1036, loss:0.834089 \n","Epoch 2, 299/1036, loss:0.837838 \n","Epoch 2, 399/1036, loss:0.809828 \n","Epoch 2, 499/1036, loss:0.845886 \n","Epoch 2, 599/1036, loss:0.836260 \n","Epoch 2, 699/1036, loss:0.817381 \n","Epoch 2, 799/1036, loss:0.819161 \n","Epoch 2, 899/1036, loss:0.818658 \n","Epoch 2, 999/1036, loss:0.813798 \n","Epoch 2 cost time: 384.148s\n","Epoch Test 2, 49/259\n","Epoch Test 2, 99/259\n","Epoch Test 2, 149/259\n","Epoch Test 2, 199/259\n","Epoch Test 2, 249/259\n","Epoch Test 2 cost time: 29.216s\n","准确率： 0.729\n","Epoch 3, 99/1036, loss:0.803071 \n","Epoch 3, 199/1036, loss:0.828041 \n","Epoch 3, 299/1036, loss:0.822507 \n","Epoch 3, 399/1036, loss:0.811669 \n","Epoch 3, 499/1036, loss:0.843604 \n","Epoch 3, 599/1036, loss:0.826365 \n","Epoch 3, 699/1036, loss:0.839747 \n","Epoch 3, 799/1036, loss:0.830135 \n","Epoch 3, 899/1036, loss:0.833823 \n","Epoch 3, 999/1036, loss:0.827629 \n","Epoch 3 cost time: 384.688s\n","Epoch Test 3, 49/259\n","Epoch Test 3, 99/259\n","Epoch Test 3, 149/259\n","Epoch Test 3, 199/259\n","Epoch Test 3, 249/259\n","Epoch Test 3 cost time: 29.209s\n","准确率： 0.738\n","Epoch 4, 99/1036, loss:0.789591 \n","Epoch 4, 199/1036, loss:0.813845 \n","Epoch 4, 299/1036, loss:0.784588 \n","Epoch 4, 399/1036, loss:0.805417 \n","Epoch 4, 499/1036, loss:0.794907 \n","Epoch 4, 599/1036, loss:0.813745 \n","Epoch 4, 699/1036, loss:0.785916 \n","Epoch 4, 799/1036, loss:0.793274 \n","Epoch 4, 899/1036, loss:0.785251 \n","Epoch 4, 999/1036, loss:0.808452 \n","Epoch 4 cost time: 383.851s\n","Epoch Test 4, 49/259\n","Epoch Test 4, 99/259\n","Epoch Test 4, 149/259\n","Epoch Test 4, 199/259\n","Epoch Test 4, 249/259\n","Epoch Test 4 cost time: 29.317s\n","准确率： 0.744\n","Epoch 5, 99/1036, loss:0.792812 \n","Epoch 5, 199/1036, loss:0.767308 \n","Epoch 5, 299/1036, loss:0.774032 \n","Epoch 5, 399/1036, loss:0.801540 \n","Epoch 5, 499/1036, loss:0.770668 \n","Epoch 5, 599/1036, loss:0.788897 \n","Epoch 5, 699/1036, loss:0.766040 \n","Epoch 5, 799/1036, loss:0.818780 \n","Epoch 5, 899/1036, loss:0.798207 \n","Epoch 5, 999/1036, loss:0.814740 \n","Epoch 5 cost time: 385.044s\n","Epoch Test 5, 49/259\n","Epoch Test 5, 99/259\n","Epoch Test 5, 149/259\n","Epoch Test 5, 199/259\n","Epoch Test 5, 249/259\n","Epoch Test 5 cost time: 29.312s\n","准确率： 0.755\n","Epoch 6, 99/1036, loss:0.750737 \n","Epoch 6, 199/1036, loss:0.772268 \n","Epoch 6, 299/1036, loss:0.776301 \n","Epoch 6, 399/1036, loss:0.759473 \n","Epoch 6, 499/1036, loss:0.762941 \n","Epoch 6, 599/1036, loss:0.803173 \n","Epoch 6, 699/1036, loss:0.805589 \n","Epoch 6, 799/1036, loss:0.800512 \n","Epoch 6, 899/1036, loss:0.780800 \n","Epoch 6, 999/1036, loss:0.791619 \n","Epoch 6 cost time: 386.229s\n","Epoch Test 6, 49/259\n","Epoch Test 6, 99/259\n","Epoch Test 6, 149/259\n","Epoch Test 6, 199/259\n","Epoch Test 6, 249/259\n","Epoch Test 6 cost time: 29.207s\n","准确率： 0.762\n","Epoch 7, 99/1036, loss:0.779047 \n","Epoch 7, 199/1036, loss:0.775671 \n","Epoch 7, 299/1036, loss:0.807542 \n","Epoch 7, 399/1036, loss:0.780422 \n","Epoch 7, 499/1036, loss:0.772576 \n","Epoch 7, 599/1036, loss:0.746071 \n","Epoch 7, 699/1036, loss:0.768609 \n","Epoch 7, 799/1036, loss:0.773170 \n","Epoch 7, 899/1036, loss:0.800522 \n","Epoch 7, 999/1036, loss:0.750788 \n","Epoch 7 cost time: 385.255s\n","Epoch Test 7, 49/259\n","Epoch Test 7, 99/259\n","Epoch Test 7, 149/259\n","Epoch Test 7, 199/259\n","Epoch Test 7, 249/259\n","Epoch Test 7 cost time: 29.085s\n","准确率： 0.766\n","Epoch 8, 99/1036, loss:0.779036 \n","Epoch 8, 199/1036, loss:0.758344 \n","Epoch 8, 299/1036, loss:0.745152 \n","Epoch 8, 399/1036, loss:0.735381 \n","Epoch 8, 499/1036, loss:0.793157 \n","Epoch 8, 599/1036, loss:0.775687 \n","Epoch 8, 699/1036, loss:0.775711 \n","Epoch 8, 799/1036, loss:0.794728 \n","Epoch 8, 899/1036, loss:0.775894 \n","Epoch 8, 999/1036, loss:0.780902 \n","Epoch 8 cost time: 384.029s\n","Epoch Test 8, 49/259\n","Epoch Test 8, 99/259\n","Epoch Test 8, 149/259\n","Epoch Test 8, 199/259\n","Epoch Test 8, 249/259\n","Epoch Test 8 cost time: 29.210s\n","准确率： 0.769\n","Epoch 9, 99/1036, loss:0.792108 \n","Epoch 9, 199/1036, loss:0.758160 \n","Epoch 9, 299/1036, loss:0.744144 \n","Epoch 9, 399/1036, loss:0.740713 \n","Epoch 9, 499/1036, loss:0.765881 \n","Epoch 9, 599/1036, loss:0.760345 \n","Epoch 9, 699/1036, loss:0.729910 \n","Epoch 9, 799/1036, loss:0.767180 \n","Epoch 9, 899/1036, loss:0.751740 \n","Epoch 9, 999/1036, loss:0.786328 \n","Epoch 9 cost time: 385.481s\n","Epoch Test 9, 49/259\n","Epoch Test 9, 99/259\n","Epoch Test 9, 149/259\n","Epoch Test 9, 199/259\n","Epoch Test 9, 249/259\n","Epoch Test 9 cost time: 29.208s\n","准确率： 0.771\n","Epoch 10, 99/1036, loss:0.724219 \n","Epoch 10, 199/1036, loss:0.757825 \n","Epoch 10, 299/1036, loss:0.754284 \n","Epoch 10, 399/1036, loss:0.742274 \n","Epoch 10, 499/1036, loss:0.751475 \n","Epoch 10, 599/1036, loss:0.755647 \n","Epoch 10, 699/1036, loss:0.750563 \n","Epoch 10, 799/1036, loss:0.756732 \n","Epoch 10, 899/1036, loss:0.755880 \n","Epoch 10, 999/1036, loss:0.780568 \n","Epoch 10 cost time: 385.365s\n","Epoch Test 10, 49/259\n","Epoch Test 10, 99/259\n","Epoch Test 10, 149/259\n","Epoch Test 10, 199/259\n","Epoch Test 10, 249/259\n","Epoch Test 10 cost time: 29.193s\n","准确率： 0.778\n","Epoch 11, 99/1036, loss:0.739118 \n","Epoch 11, 199/1036, loss:0.750570 \n","Epoch 11, 299/1036, loss:0.744773 \n","Epoch 11, 399/1036, loss:0.754055 \n","Epoch 11, 499/1036, loss:0.763874 \n","Epoch 11, 599/1036, loss:0.785975 \n","Epoch 11, 699/1036, loss:0.769343 \n","Epoch 11, 799/1036, loss:0.784927 \n","Epoch 11, 899/1036, loss:0.778263 \n","Epoch 11, 999/1036, loss:0.766932 \n","Epoch 11 cost time: 384.816s\n","Epoch Test 11, 49/259\n","Epoch Test 11, 99/259\n","Epoch Test 11, 149/259\n","Epoch Test 11, 199/259\n","Epoch Test 11, 249/259\n","Epoch Test 11 cost time: 29.118s\n","准确率： 0.776\n","Epoch 12, 99/1036, loss:0.744040 \n","Epoch 12, 199/1036, loss:0.759602 \n","Epoch 12, 299/1036, loss:0.761150 \n","Epoch 12, 399/1036, loss:0.747772 \n","Epoch 12, 499/1036, loss:0.745146 \n","Epoch 12, 599/1036, loss:0.771867 \n","Epoch 12, 699/1036, loss:0.748500 \n","Epoch 12, 799/1036, loss:0.768550 \n","Epoch 12, 899/1036, loss:0.774940 \n","Epoch 12, 999/1036, loss:0.752489 \n","Epoch 12 cost time: 385.819s\n","Epoch Test 12, 49/259\n","Epoch Test 12, 99/259\n","Epoch Test 12, 149/259\n","Epoch Test 12, 199/259\n","Epoch Test 12, 249/259\n","Epoch Test 12 cost time: 29.149s\n","准确率： 0.783\n","Epoch 13, 99/1036, loss:0.725653 \n","Epoch 13, 199/1036, loss:0.757583 \n","Epoch 13, 299/1036, loss:0.793927 \n","Epoch 13, 399/1036, loss:0.758148 \n","Epoch 13, 499/1036, loss:0.743980 \n","Epoch 13, 599/1036, loss:0.723396 \n","Epoch 13, 699/1036, loss:0.750201 \n","Epoch 13, 799/1036, loss:0.777475 \n","Epoch 13, 899/1036, loss:0.720624 \n","Epoch 13, 999/1036, loss:0.745322 \n","Epoch 13 cost time: 385.891s\n","Epoch Test 13, 49/259\n","Epoch Test 13, 99/259\n","Epoch Test 13, 149/259\n","Epoch Test 13, 199/259\n","Epoch Test 13, 249/259\n","Epoch Test 13 cost time: 29.180s\n","准确率： 0.784\n","Epoch 14, 99/1036, loss:0.741115 \n","Epoch 14, 199/1036, loss:0.769922 \n","Epoch 14, 299/1036, loss:0.715696 \n","Epoch 14, 399/1036, loss:0.736527 \n","Epoch 14, 499/1036, loss:0.753967 \n","Epoch 14, 599/1036, loss:0.758353 \n","Epoch 14, 699/1036, loss:0.764003 \n","Epoch 14, 799/1036, loss:0.735048 \n","Epoch 14, 899/1036, loss:0.718029 \n","Epoch 14, 999/1036, loss:0.741678 \n","Epoch 14 cost time: 385.950s\n","Epoch Test 14, 49/259\n","Epoch Test 14, 99/259\n","Epoch Test 14, 149/259\n","Epoch Test 14, 199/259\n","Epoch Test 14, 249/259\n","Epoch Test 14 cost time: 29.236s\n","准确率： 0.788\n","Epoch 15, 99/1036, loss:0.728617 \n","Epoch 15, 199/1036, loss:0.760469 \n","Epoch 15, 299/1036, loss:0.732215 \n","Epoch 15, 399/1036, loss:0.745189 \n","Epoch 15, 499/1036, loss:0.794041 \n","Epoch 15, 599/1036, loss:0.736671 \n","Epoch 15, 699/1036, loss:0.765786 \n","Epoch 15, 799/1036, loss:0.717512 \n","Epoch 15, 899/1036, loss:0.751273 \n","Epoch 15, 999/1036, loss:0.765527 \n","Epoch 15 cost time: 385.661s\n","Epoch Test 15, 49/259\n","Epoch Test 15, 99/259\n","Epoch Test 15, 149/259\n","Epoch Test 15, 199/259\n","Epoch Test 15, 249/259\n","Epoch Test 15 cost time: 29.176s\n","准确率： 0.791\n","Epoch 16, 99/1036, loss:0.740501 \n","Epoch 16, 199/1036, loss:0.771637 \n","Epoch 16, 299/1036, loss:0.749488 \n","Epoch 16, 399/1036, loss:0.745282 \n","Epoch 16, 499/1036, loss:0.736697 \n","Epoch 16, 599/1036, loss:0.744440 \n","Epoch 16, 699/1036, loss:0.755649 \n","Epoch 16, 799/1036, loss:0.728796 \n","Epoch 16, 899/1036, loss:0.749044 \n","Epoch 16, 999/1036, loss:0.735020 \n","Epoch 16 cost time: 384.713s\n","Epoch Test 16, 49/259\n","Epoch Test 16, 99/259\n","Epoch Test 16, 149/259\n","Epoch Test 16, 199/259\n","Epoch Test 16, 249/259\n","Epoch Test 16 cost time: 29.190s\n","准确率： 0.794\n","Epoch 17, 99/1036, loss:0.748153 \n","Epoch 17, 199/1036, loss:0.740478 \n","Epoch 17, 299/1036, loss:0.727243 \n","Epoch 17, 399/1036, loss:0.734643 \n","Epoch 17, 499/1036, loss:0.735954 \n","Epoch 17, 599/1036, loss:0.761949 \n","Epoch 17, 699/1036, loss:0.720917 \n","Epoch 17, 799/1036, loss:0.763618 \n","Epoch 17, 899/1036, loss:0.748902 \n","Epoch 17, 999/1036, loss:0.750448 \n","Epoch 17 cost time: 384.375s\n","Epoch Test 17, 49/259\n","Epoch Test 17, 99/259\n","Epoch Test 17, 149/259\n","Epoch Test 17, 199/259\n","Epoch Test 17, 249/259\n","Epoch Test 17 cost time: 29.150s\n","准确率： 0.795\n","Epoch 18, 99/1036, loss:0.714449 \n","Epoch 18, 199/1036, loss:0.699592 \n","Epoch 18, 299/1036, loss:0.719019 \n","Epoch 18, 399/1036, loss:0.727329 \n","Epoch 18, 499/1036, loss:0.741071 \n","Epoch 18, 599/1036, loss:0.751152 \n","Epoch 18, 699/1036, loss:0.741989 \n","Epoch 18, 799/1036, loss:0.724981 \n","Epoch 18, 899/1036, loss:0.760920 \n","Epoch 18, 999/1036, loss:0.745883 \n","Epoch 18 cost time: 384.665s\n","Epoch Test 18, 49/259\n","Epoch Test 18, 99/259\n","Epoch Test 18, 149/259\n","Epoch Test 18, 199/259\n","Epoch Test 18, 249/259\n","Epoch Test 18 cost time: 29.127s\n","准确率： 0.796\n","Epoch 19, 99/1036, loss:0.736364 \n","Epoch 19, 199/1036, loss:0.710417 \n","Epoch 19, 299/1036, loss:0.721730 \n","Epoch 19, 399/1036, loss:0.739445 \n","Epoch 19, 499/1036, loss:0.740644 \n","Epoch 19, 599/1036, loss:0.739626 \n","Epoch 19, 699/1036, loss:0.733363 \n","Epoch 19, 799/1036, loss:0.745436 \n","Epoch 19, 899/1036, loss:0.701313 \n","Epoch 19, 999/1036, loss:0.740134 \n","Epoch 19 cost time: 384.751s\n","Epoch Test 19, 49/259\n","Epoch Test 19, 99/259\n","Epoch Test 19, 149/259\n","Epoch Test 19, 199/259\n","Epoch Test 19, 249/259\n","Epoch Test 19 cost time: 29.149s\n","准确率： 0.799\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"IgjQbdtsnaPF","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"_N1cAYSInaPH","colab_type":"code","outputId":"63278cb0-2fee-463b-f183-5bc8cdcba76d","executionInfo":{"status":"ok","timestamp":1588367114120,"user_tz":-480,"elapsed":15906155,"user":{"displayName":"fuliu fuliu","photoUrl":"","userId":"17240005970423735187"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["train(20, model, 'data/EmotionAnalyzeModel_balance_TWV.model', True, lr=0.0003)\n","train(20, model, 'data/EmotionAnalyzeModel_balance_TWV.model', True, lr=0.0001)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch Test 20, 49/259\n","Epoch Test 20, 99/259\n","Epoch Test 20, 149/259\n","Epoch Test 20, 199/259\n","Epoch Test 20, 249/259\n","Epoch Test 20 cost time: 29.057s\n","准确率： 0.799\n","Epoch 0, 99/1036, loss:0.714727 \n","Epoch 0, 199/1036, loss:0.727061 \n","Epoch 0, 299/1036, loss:0.735342 \n","Epoch 0, 399/1036, loss:0.713944 \n","Epoch 0, 499/1036, loss:0.747210 \n","Epoch 0, 599/1036, loss:0.715627 \n","Epoch 0, 699/1036, loss:0.735560 \n","Epoch 0, 799/1036, loss:0.742908 \n","Epoch 0, 899/1036, loss:0.721923 \n","Epoch 0, 999/1036, loss:0.717881 \n","Epoch 0 cost time: 386.471s\n","Epoch Test 0, 49/259\n","Epoch Test 0, 99/259\n","Epoch Test 0, 149/259\n","Epoch Test 0, 199/259\n","Epoch Test 0, 249/259\n","Epoch Test 0 cost time: 29.250s\n","准确率： 0.800\n","Epoch 1, 99/1036, loss:0.714024 \n","Epoch 1, 199/1036, loss:0.742599 \n","Epoch 1, 299/1036, loss:0.707694 \n","Epoch 1, 399/1036, loss:0.715391 \n","Epoch 1, 499/1036, loss:0.707483 \n","Epoch 1, 599/1036, loss:0.757463 \n","Epoch 1, 699/1036, loss:0.722609 \n","Epoch 1, 799/1036, loss:0.747598 \n","Epoch 1, 899/1036, loss:0.741102 \n","Epoch 1, 999/1036, loss:0.709966 \n","Epoch 1 cost time: 385.281s\n","Epoch Test 1, 49/259\n","Epoch Test 1, 99/259\n","Epoch Test 1, 149/259\n","Epoch Test 1, 199/259\n","Epoch Test 1, 249/259\n","Epoch Test 1 cost time: 29.185s\n","准确率： 0.800\n","Epoch 2, 99/1036, loss:0.727897 \n","Epoch 2, 199/1036, loss:0.726096 \n","Epoch 2, 299/1036, loss:0.731328 \n","Epoch 2, 399/1036, loss:0.731069 \n","Epoch 2, 499/1036, loss:0.721781 \n","Epoch 2, 599/1036, loss:0.705656 \n","Epoch 2, 699/1036, loss:0.729057 \n","Epoch 2, 799/1036, loss:0.736802 \n","Epoch 2, 899/1036, loss:0.736054 \n","Epoch 2, 999/1036, loss:0.714527 \n","Epoch 2 cost time: 385.232s\n","Epoch Test 2, 49/259\n","Epoch Test 2, 99/259\n","Epoch Test 2, 149/259\n","Epoch Test 2, 199/259\n","Epoch Test 2, 249/259\n","Epoch Test 2 cost time: 28.953s\n","准确率： 0.802\n","Epoch 3, 99/1036, loss:0.757260 \n","Epoch 3, 199/1036, loss:0.702587 \n","Epoch 3, 299/1036, loss:0.704401 \n","Epoch 3, 399/1036, loss:0.721678 \n","Epoch 3, 499/1036, loss:0.705055 \n","Epoch 3, 599/1036, loss:0.724465 \n","Epoch 3, 699/1036, loss:0.709738 \n","Epoch 3, 799/1036, loss:0.704326 \n","Epoch 3, 899/1036, loss:0.696939 \n","Epoch 3, 999/1036, loss:0.734120 \n","Epoch 3 cost time: 384.382s\n","Epoch Test 3, 49/259\n","Epoch Test 3, 99/259\n","Epoch Test 3, 149/259\n","Epoch Test 3, 199/259\n","Epoch Test 3, 249/259\n","Epoch Test 3 cost time: 29.411s\n","准确率： 0.803\n","Epoch 4, 99/1036, loss:0.693293 \n","Epoch 4, 199/1036, loss:0.715221 \n","Epoch 4, 299/1036, loss:0.721061 \n","Epoch 4, 399/1036, loss:0.738988 \n","Epoch 4, 499/1036, loss:0.713972 \n","Epoch 4, 599/1036, loss:0.687407 \n","Epoch 4, 699/1036, loss:0.754746 \n","Epoch 4, 799/1036, loss:0.727929 \n","Epoch 4, 899/1036, loss:0.720733 \n","Epoch 4, 999/1036, loss:0.732492 \n","Epoch 4 cost time: 384.670s\n","Epoch Test 4, 49/259\n","Epoch Test 4, 99/259\n","Epoch Test 4, 149/259\n","Epoch Test 4, 199/259\n","Epoch Test 4, 249/259\n","Epoch Test 4 cost time: 29.159s\n","准确率： 0.803\n","Epoch 5, 99/1036, loss:0.687814 \n","Epoch 5, 199/1036, loss:0.712701 \n","Epoch 5, 299/1036, loss:0.736263 \n","Epoch 5, 399/1036, loss:0.736096 \n","Epoch 5, 499/1036, loss:0.721411 \n","Epoch 5, 599/1036, loss:0.727887 \n","Epoch 5, 699/1036, loss:0.710986 \n","Epoch 5, 799/1036, loss:0.729778 \n","Epoch 5, 899/1036, loss:0.704557 \n","Epoch 5, 999/1036, loss:0.690293 \n","Epoch 5 cost time: 385.506s\n","Epoch Test 5, 49/259\n","Epoch Test 5, 99/259\n","Epoch Test 5, 149/259\n","Epoch Test 5, 199/259\n","Epoch Test 5, 249/259\n","Epoch Test 5 cost time: 29.196s\n","准确率： 0.806\n","Epoch 6, 99/1036, loss:0.736845 \n","Epoch 6, 199/1036, loss:0.708888 \n","Epoch 6, 299/1036, loss:0.688432 \n","Epoch 6, 399/1036, loss:0.746909 \n","Epoch 6, 499/1036, loss:0.746214 \n","Epoch 6, 599/1036, loss:0.774239 \n","Epoch 6, 699/1036, loss:0.727041 \n","Epoch 6, 799/1036, loss:0.718660 \n","Epoch 6, 899/1036, loss:0.756067 \n","Epoch 6, 999/1036, loss:0.699547 \n","Epoch 6 cost time: 384.356s\n","Epoch Test 6, 49/259\n","Epoch Test 6, 99/259\n","Epoch Test 6, 149/259\n","Epoch Test 6, 199/259\n","Epoch Test 6, 249/259\n","Epoch Test 6 cost time: 29.232s\n","准确率： 0.805\n","Epoch 7, 99/1036, loss:0.734520 \n","Epoch 7, 199/1036, loss:0.714769 \n","Epoch 7, 299/1036, loss:0.718937 \n","Epoch 7, 399/1036, loss:0.721897 \n","Epoch 7, 499/1036, loss:0.706004 \n","Epoch 7, 599/1036, loss:0.716526 \n","Epoch 7, 699/1036, loss:0.710110 \n","Epoch 7, 799/1036, loss:0.704780 \n","Epoch 7, 899/1036, loss:0.709690 \n","Epoch 7, 999/1036, loss:0.733981 \n","Epoch 7 cost time: 384.953s\n","Epoch Test 7, 49/259\n","Epoch Test 7, 99/259\n","Epoch Test 7, 149/259\n","Epoch Test 7, 199/259\n","Epoch Test 7, 249/259\n","Epoch Test 7 cost time: 29.044s\n","准确率： 0.808\n","Epoch 8, 99/1036, loss:0.716230 \n","Epoch 8, 199/1036, loss:0.729273 \n","Epoch 8, 299/1036, loss:0.729190 \n","Epoch 8, 399/1036, loss:0.716977 \n","Epoch 8, 499/1036, loss:0.708403 \n","Epoch 8, 599/1036, loss:0.734783 \n","Epoch 8, 699/1036, loss:0.717291 \n","Epoch 8, 799/1036, loss:0.698654 \n","Epoch 8, 899/1036, loss:0.731689 \n","Epoch 8, 999/1036, loss:0.725871 \n","Epoch 8 cost time: 384.729s\n","Epoch Test 8, 49/259\n","Epoch Test 8, 99/259\n","Epoch Test 8, 149/259\n","Epoch Test 8, 199/259\n","Epoch Test 8, 249/259\n","Epoch Test 8 cost time: 29.172s\n","准确率： 0.808\n","Epoch 9, 99/1036, loss:0.718931 \n","Epoch 9, 199/1036, loss:0.688392 \n","Epoch 9, 299/1036, loss:0.735441 \n","Epoch 9, 399/1036, loss:0.712664 \n","Epoch 9, 499/1036, loss:0.704468 \n","Epoch 9, 599/1036, loss:0.718067 \n","Epoch 9, 699/1036, loss:0.712520 \n","Epoch 9, 799/1036, loss:0.706967 \n","Epoch 9, 899/1036, loss:0.728967 \n","Epoch 9, 999/1036, loss:0.718424 \n","Epoch 9 cost time: 384.758s\n","Epoch Test 9, 49/259\n","Epoch Test 9, 99/259\n","Epoch Test 9, 149/259\n","Epoch Test 9, 199/259\n","Epoch Test 9, 249/259\n","Epoch Test 9 cost time: 29.029s\n","准确率： 0.810\n","Epoch 10, 99/1036, loss:0.729312 \n","Epoch 10, 199/1036, loss:0.708777 \n","Epoch 10, 299/1036, loss:0.713014 \n","Epoch 10, 399/1036, loss:0.742180 \n","Epoch 10, 499/1036, loss:0.712082 \n","Epoch 10, 599/1036, loss:0.729652 \n","Epoch 10, 699/1036, loss:0.707399 \n","Epoch 10, 799/1036, loss:0.710819 \n","Epoch 10, 899/1036, loss:0.706638 \n","Epoch 10, 999/1036, loss:0.726916 \n","Epoch 10 cost time: 384.972s\n","Epoch Test 10, 49/259\n","Epoch Test 10, 99/259\n","Epoch Test 10, 149/259\n","Epoch Test 10, 199/259\n","Epoch Test 10, 249/259\n","Epoch Test 10 cost time: 29.281s\n","准确率： 0.809\n","Epoch 11, 99/1036, loss:0.714486 \n","Epoch 11, 199/1036, loss:0.719416 \n","Epoch 11, 299/1036, loss:0.728981 \n","Epoch 11, 399/1036, loss:0.728279 \n","Epoch 11, 499/1036, loss:0.726086 \n","Epoch 11, 599/1036, loss:0.716597 \n","Epoch 11, 699/1036, loss:0.721141 \n","Epoch 11, 799/1036, loss:0.731582 \n","Epoch 11, 899/1036, loss:0.706918 \n","Epoch 11, 999/1036, loss:0.680088 \n","Epoch 11 cost time: 385.222s\n","Epoch Test 11, 49/259\n","Epoch Test 11, 99/259\n","Epoch Test 11, 149/259\n","Epoch Test 11, 199/259\n","Epoch Test 11, 249/259\n","Epoch Test 11 cost time: 29.260s\n","准确率： 0.811\n","Epoch 12, 99/1036, loss:0.697716 \n","Epoch 12, 199/1036, loss:0.706517 \n","Epoch 12, 299/1036, loss:0.706146 \n","Epoch 12, 399/1036, loss:0.700704 \n","Epoch 12, 499/1036, loss:0.707205 \n","Epoch 12, 599/1036, loss:0.698092 \n","Epoch 12, 699/1036, loss:0.705719 \n","Epoch 12, 799/1036, loss:0.744870 \n","Epoch 12, 899/1036, loss:0.725353 \n","Epoch 12, 999/1036, loss:0.744558 \n","Epoch 12 cost time: 385.054s\n","Epoch Test 12, 49/259\n","Epoch Test 12, 99/259\n","Epoch Test 12, 149/259\n","Epoch Test 12, 199/259\n","Epoch Test 12, 249/259\n","Epoch Test 12 cost time: 28.902s\n","准确率： 0.812\n","Epoch 13, 99/1036, loss:0.700293 \n","Epoch 13, 199/1036, loss:0.692088 \n","Epoch 13, 299/1036, loss:0.708226 \n","Epoch 13, 399/1036, loss:0.717027 \n","Epoch 13, 499/1036, loss:0.722966 \n","Epoch 13, 599/1036, loss:0.737880 \n","Epoch 13, 699/1036, loss:0.676903 \n","Epoch 13, 799/1036, loss:0.706348 \n","Epoch 13, 899/1036, loss:0.736459 \n","Epoch 13, 999/1036, loss:0.698449 \n","Epoch 13 cost time: 384.629s\n","Epoch Test 13, 49/259\n","Epoch Test 13, 99/259\n","Epoch Test 13, 149/259\n","Epoch Test 13, 199/259\n","Epoch Test 13, 249/259\n","Epoch Test 13 cost time: 29.166s\n","准确率： 0.812\n","Epoch 14, 99/1036, loss:0.706990 \n","Epoch 14, 199/1036, loss:0.697928 \n","Epoch 14, 299/1036, loss:0.719267 \n","Epoch 14, 399/1036, loss:0.717146 \n","Epoch 14, 499/1036, loss:0.715928 \n","Epoch 14, 599/1036, loss:0.699849 \n","Epoch 14, 699/1036, loss:0.716120 \n","Epoch 14, 799/1036, loss:0.703567 \n","Epoch 14, 899/1036, loss:0.718560 \n","Epoch 14, 999/1036, loss:0.709137 \n","Epoch 14 cost time: 384.554s\n","Epoch Test 14, 49/259\n","Epoch Test 14, 99/259\n","Epoch Test 14, 149/259\n","Epoch Test 14, 199/259\n","Epoch Test 14, 249/259\n","Epoch Test 14 cost time: 29.119s\n","准确率： 0.814\n","Epoch 15, 99/1036, loss:0.702631 \n","Epoch 15, 199/1036, loss:0.711792 \n","Epoch 15, 299/1036, loss:0.730352 \n","Epoch 15, 399/1036, loss:0.706638 \n","Epoch 15, 499/1036, loss:0.707101 \n","Epoch 15, 599/1036, loss:0.746231 \n","Epoch 15, 699/1036, loss:0.718117 \n","Epoch 15, 799/1036, loss:0.703292 \n","Epoch 15, 899/1036, loss:0.690406 \n","Epoch 15, 999/1036, loss:0.677350 \n","Epoch 15 cost time: 385.952s\n","Epoch Test 15, 49/259\n","Epoch Test 15, 99/259\n","Epoch Test 15, 149/259\n","Epoch Test 15, 199/259\n","Epoch Test 15, 249/259\n","Epoch Test 15 cost time: 29.017s\n","准确率： 0.814\n","Epoch 16, 99/1036, loss:0.701610 \n","Epoch 16, 199/1036, loss:0.710941 \n","Epoch 16, 299/1036, loss:0.702292 \n","Epoch 16, 399/1036, loss:0.724066 \n","Epoch 16, 499/1036, loss:0.692911 \n","Epoch 16, 599/1036, loss:0.706386 \n","Epoch 16, 699/1036, loss:0.709410 \n","Epoch 16, 799/1036, loss:0.711792 \n","Epoch 16, 899/1036, loss:0.687822 \n","Epoch 16, 999/1036, loss:0.721178 \n","Epoch 16 cost time: 385.916s\n","Epoch Test 16, 49/259\n","Epoch Test 16, 99/259\n","Epoch Test 16, 149/259\n","Epoch Test 16, 199/259\n","Epoch Test 16, 249/259\n","Epoch Test 16 cost time: 29.395s\n","准确率： 0.815\n","Epoch 17, 99/1036, loss:0.676738 \n","Epoch 17, 199/1036, loss:0.707529 \n","Epoch 17, 299/1036, loss:0.693275 \n","Epoch 17, 399/1036, loss:0.706030 \n","Epoch 17, 499/1036, loss:0.710268 \n","Epoch 17, 599/1036, loss:0.705171 \n","Epoch 17, 699/1036, loss:0.676181 \n","Epoch 17, 799/1036, loss:0.708968 \n","Epoch 17, 899/1036, loss:0.703147 \n","Epoch 17, 999/1036, loss:0.712212 \n","Epoch 17 cost time: 385.320s\n","Epoch Test 17, 49/259\n","Epoch Test 17, 99/259\n","Epoch Test 17, 149/259\n","Epoch Test 17, 199/259\n","Epoch Test 17, 249/259\n","Epoch Test 17 cost time: 29.065s\n","准确率： 0.816\n","Epoch 18, 99/1036, loss:0.732287 \n","Epoch 18, 199/1036, loss:0.689004 \n","Epoch 18, 299/1036, loss:0.680612 \n","Epoch 18, 399/1036, loss:0.702008 \n","Epoch 18, 499/1036, loss:0.693605 \n","Epoch 18, 599/1036, loss:0.705937 \n","Epoch 18, 699/1036, loss:0.696078 \n","Epoch 18, 799/1036, loss:0.707517 \n","Epoch 18, 899/1036, loss:0.707774 \n","Epoch 18, 999/1036, loss:0.710160 \n","Epoch 18 cost time: 384.653s\n","Epoch Test 18, 49/259\n","Epoch Test 18, 99/259\n","Epoch Test 18, 149/259\n","Epoch Test 18, 199/259\n","Epoch Test 18, 249/259\n","Epoch Test 18 cost time: 29.230s\n","准确率： 0.815\n","Epoch 19, 99/1036, loss:0.694407 \n","Epoch 19, 199/1036, loss:0.699676 \n","Epoch 19, 299/1036, loss:0.709509 \n","Epoch 19, 399/1036, loss:0.710696 \n","Epoch 19, 499/1036, loss:0.702177 \n","Epoch 19, 599/1036, loss:0.684240 \n","Epoch 19, 699/1036, loss:0.698248 \n","Epoch 19, 799/1036, loss:0.706982 \n","Epoch 19, 899/1036, loss:0.711464 \n","Epoch 19, 999/1036, loss:0.702138 \n","Epoch 19 cost time: 384.856s\n","Epoch Test 19, 49/259\n","Epoch Test 19, 99/259\n","Epoch Test 19, 149/259\n","Epoch Test 19, 199/259\n","Epoch Test 19, 249/259\n","Epoch Test 19 cost time: 29.060s\n","准确率： 0.816\n","Epoch Test 20, 49/259\n","Epoch Test 20, 99/259\n","Epoch Test 20, 149/259\n","Epoch Test 20, 199/259\n","Epoch Test 20, 249/259\n","Epoch Test 20 cost time: 29.196s\n","准确率： 0.816\n","Epoch 0, 99/1036, loss:0.691797 \n","Epoch 0, 199/1036, loss:0.691972 \n","Epoch 0, 299/1036, loss:0.749735 \n","Epoch 0, 399/1036, loss:0.737148 \n","Epoch 0, 499/1036, loss:0.703527 \n","Epoch 0, 599/1036, loss:0.733549 \n","Epoch 0, 699/1036, loss:0.679365 \n","Epoch 0, 799/1036, loss:0.709475 \n","Epoch 0, 899/1036, loss:0.706501 \n","Epoch 0, 999/1036, loss:0.674374 \n","Epoch 0 cost time: 384.846s\n","Epoch Test 0, 49/259\n","Epoch Test 0, 99/259\n","Epoch Test 0, 149/259\n","Epoch Test 0, 199/259\n","Epoch Test 0, 249/259\n","Epoch Test 0 cost time: 29.258s\n","准确率： 0.816\n","Epoch 1, 99/1036, loss:0.742738 \n","Epoch 1, 199/1036, loss:0.712131 \n","Epoch 1, 299/1036, loss:0.727480 \n","Epoch 1, 399/1036, loss:0.728158 \n","Epoch 1, 499/1036, loss:0.720390 \n","Epoch 1, 599/1036, loss:0.691500 \n","Epoch 1, 699/1036, loss:0.716941 \n","Epoch 1, 799/1036, loss:0.703174 \n","Epoch 1, 899/1036, loss:0.703159 \n","Epoch 1, 999/1036, loss:0.732760 \n","Epoch 1 cost time: 385.001s\n","Epoch Test 1, 49/259\n","Epoch Test 1, 99/259\n","Epoch Test 1, 149/259\n","Epoch Test 1, 199/259\n","Epoch Test 1, 249/259\n","Epoch Test 1 cost time: 29.098s\n","准确率： 0.818\n","Epoch 2, 99/1036, loss:0.729412 \n","Epoch 2, 199/1036, loss:0.707358 \n","Epoch 2, 299/1036, loss:0.692203 \n","Epoch 2, 399/1036, loss:0.718763 \n","Epoch 2, 499/1036, loss:0.696388 \n","Epoch 2, 599/1036, loss:0.702190 \n","Epoch 2, 699/1036, loss:0.697793 \n","Epoch 2, 799/1036, loss:0.708260 \n","Epoch 2, 899/1036, loss:0.705824 \n","Epoch 2, 999/1036, loss:0.715383 \n","Epoch 2 cost time: 383.968s\n","Epoch Test 2, 49/259\n","Epoch Test 2, 99/259\n","Epoch Test 2, 149/259\n","Epoch Test 2, 199/259\n","Epoch Test 2, 249/259\n","Epoch Test 2 cost time: 29.142s\n","准确率： 0.818\n","Epoch 3, 99/1036, loss:0.701494 \n","Epoch 3, 199/1036, loss:0.703628 \n","Epoch 3, 299/1036, loss:0.683493 \n","Epoch 3, 399/1036, loss:0.707626 \n","Epoch 3, 499/1036, loss:0.691038 \n","Epoch 3, 599/1036, loss:0.688839 \n","Epoch 3, 699/1036, loss:0.699356 \n","Epoch 3, 799/1036, loss:0.707525 \n","Epoch 3, 899/1036, loss:0.737090 \n","Epoch 3, 999/1036, loss:0.735729 \n","Epoch 3 cost time: 384.911s\n","Epoch Test 3, 49/259\n","Epoch Test 3, 99/259\n","Epoch Test 3, 149/259\n","Epoch Test 3, 199/259\n","Epoch Test 3, 249/259\n","Epoch Test 3 cost time: 29.270s\n","准确率： 0.819\n","Epoch 4, 99/1036, loss:0.705378 \n","Epoch 4, 199/1036, loss:0.697533 \n","Epoch 4, 299/1036, loss:0.698058 \n","Epoch 4, 399/1036, loss:0.695931 \n","Epoch 4, 499/1036, loss:0.697746 \n","Epoch 4, 599/1036, loss:0.686954 \n","Epoch 4, 699/1036, loss:0.714837 \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gzvump0AVdJY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":793},"outputId":"640fb6ee-7f1d-4dd3-e29c-f6581d632826","executionInfo":{"status":"error","timestamp":1588459946880,"user_tz":-480,"elapsed":1868,"user":{"displayName":"fuliu fuliu","photoUrl":"","userId":"17240005970423735187"}}},"source":["\n","def eval(model, modelSavePath, isLoad = True):\n","    if isLoad: model.load_state_dict(torch.load(modelSavePath))\n","    testModel(0, model, testLoader)\n","\n","eval(model, 'EmotionAnalyzeModelData_ClassicalHAN_OB_plus.model', isLoad = True)"],"execution_count":25,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-6efc7f591ee6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtestModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'EmotionAnalyzeModelData_ClassicalHAN_OB_plus.model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misLoad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-25-6efc7f591ee6>\u001b[0m in \u001b[0;36meval\u001b[0;34m(model, modelSavePath, isLoad)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelSavePath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misLoad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0misLoad\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelSavePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtestModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 847\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    848\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for HAN:\n\tsize mismatch for GRU1.weight_ih_l0: copying a param with shape torch.Size([1536, 200]) from checkpoint, the shape in current model is torch.Size([900, 200]).\n\tsize mismatch for GRU1.weight_hh_l0: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([900, 300]).\n\tsize mismatch for GRU1.bias_ih_l0: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([900]).\n\tsize mismatch for GRU1.bias_hh_l0: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([900]).\n\tsize mismatch for GRU1.weight_ih_l0_reverse: copying a param with shape torch.Size([1536, 200]) from checkpoint, the shape in current model is torch.Size([900, 200]).\n\tsize mismatch for GRU1.weight_hh_l0_reverse: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([900, 300]).\n\tsize mismatch for GRU1.bias_ih_l0_reverse: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([900]).\n\tsize mismatch for GRU1.bias_hh_l0_reverse: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([900]).\n\tsize mismatch for self_attention1.W.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([500, 600]).\n\tsize mismatch for self_attention1.W.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([500]).\n\tsize mismatch for self_attention1.U.weight: copying a param with shape torch.Size([1, 1024]) from checkpoint, the shape in current model is torch.Size([1, 500]).\n\tsize mismatch for GRU2.weight_ih_l0: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([900, 600]).\n\tsize mismatch for GRU2.weight_hh_l0: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([900, 300]).\n\tsize mismatch for GRU2.bias_ih_l0: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([900]).\n\tsize mismatch for GRU2.bias_hh_l0: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([900]).\n\tsize mismatch for GRU2.weight_ih_l0_reverse: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([900, 600]).\n\tsize mismatch for GRU2.weight_hh_l0_reverse: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([900, 300]).\n\tsize mismatch for GRU2.bias_ih_l0_reverse: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([900]).\n\tsize mismatch for GRU2.bias_hh_l0_reverse: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([900]).\n\tsize mismatch for self_attention2.W.weight: copying a param with shape torch.Size([2048, 2048]) from checkpoint, the shape in current model is torch.Size([500, 600]).\n\tsize mismatch for self_attention2.W.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([500]).\n\tsize mismatch for self_attention2.U.weight: copying a param with shape torch.Size([1, 2048]) from checkpoint, the shape in current model is torch.Size([1, 500]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([3, 2048]) from checkpoint, the shape in current model is torch.Size([3, 600])."]}]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"wKXjE4UBTodM","colab_type":"code","colab":{}},"source":["lossFunc =torch.nn.CrossEntropyLoss()\n","input = torch.randn(3, 5, requires_grad=True)\n","target = torch.empty(3, dtype=torch.long).random_(5)\n","_, predicted = torch.max(input.data, 1)\n","output = lossFunc(input, target)\n","print('input',input, '\\n target',target, '\\n output', output)\n","output = lossFunc(input, predicted)\n","print('predicted',predicted, '\\n target',target, '\\n output', output)\n","\n","print(predicted.data.eq(target).cpu().sum())\n","print(target.data.eq(predicted).cpu().sum())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"Z1zQWh6DTodQ","colab_type":"code","colab":{}},"source":["# 何凯明初始化\n","\n","w = torch.Tensor(3, 5, 2)\n","print(w)\n","print(nn.init.kaiming_uniform(w))\n","print(w)\n","w = torch.Tensor(3, 5, 2)\n","print(w)\n","print(torch.nn.init.kaiming_normal(w))\n","print(w)"],"execution_count":0,"outputs":[]}]}