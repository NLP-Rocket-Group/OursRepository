{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n"
     ]
    }
   ],
   "source": [
    "from HANEmotionAnalyze import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import numpy\n",
    "import time, math\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import thulac\n",
    "thulac = thulac.thulac()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.0075, -0.1934,  0.4951],\n",
      "         [ 0.8171,  0.8547,  0.8109],\n",
      "         [-1.4215, -0.8628,  0.0806],\n",
      "         [-1.2153, -0.6327,  0.6027],\n",
      "         [-0.4956,  0.1243, -0.1238]],\n",
      "\n",
      "        [[ 0.8171,  0.8547,  0.8109],\n",
      "         [-1.2153, -0.6327,  0.6027],\n",
      "         [ 0.2287,  0.2536,  0.7272],\n",
      "         [-1.4215, -0.8628,  0.0806],\n",
      "         [ 0.1302,  1.5466,  2.5407]]], grad_fn=<EmbeddingBackward>)\n",
      "tensor([[[ 0.2614, -1.2762, -1.6266],\n",
      "         [ 2.0536, -0.1082, -1.0891],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [-1.2285,  1.0499, -0.2950],\n",
      "         [-1.7455,  0.3302, -1.1412]],\n",
      "\n",
      "        [[ 2.0536, -0.1082, -1.0891],\n",
      "         [-1.2285,  1.0499, -0.2950],\n",
      "         [-2.4718, -0.9203,  0.8703],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [-0.4835,  2.0943, -1.7310]]], grad_fn=<EmbeddingBackward>)\n",
      "tensor([[4.0000, 5.1000, 6.3000]])\n",
      "tensor([[1.0000, 2.3000, 3.0000]])\n"
     ]
    }
   ],
   "source": [
    "# import torch.functional as F\n",
    "# \n",
    "embedding = nn.Embedding(10, 3)\n",
    "input = torch.LongTensor([[0, 1,2,4,5],[1, 4,3,2,9]])\n",
    "print(embedding(input))\n",
    "embedding2 = nn.Embedding(10, 3, padding_idx=2)\n",
    "# input = torch.LongTensor([[1,2,0,5, 6,7,8,9]])\n",
    "print(embedding2(input))\n",
    "weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n",
    "embedding3 = nn.Embedding.from_pretrained(weight)\n",
    "input = torch.LongTensor([1])\n",
    "print(embedding3(input))\n",
    "input = torch.LongTensor([0])\n",
    "print(embedding3(input))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 读取词向量\n",
    "建立词语列表"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'Word2VecKeyedVectors' on <module 'gensim.models.deprecated.keyedvectors' from 'f:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\gensim\\\\models\\\\deprecated\\\\keyedvectors.py'>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32mf:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1329\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1330\u001B[1;33m             \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mWord2Vec\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcls\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1331\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mf:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1243\u001B[0m         \"\"\"\n\u001B[1;32m-> 1244\u001B[1;33m         \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mBaseWordEmbeddingsModel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcls\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1245\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'ns_exponent'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mf:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(cls, fname_or_handle, **kwargs)\u001B[0m\n\u001B[0;32m    602\u001B[0m         \"\"\"\n\u001B[1;32m--> 603\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mBaseAny2VecModel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcls\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfname_or_handle\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    604\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mf:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(cls, fname, mmap)\u001B[0m\n\u001B[0;32m    425\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 426\u001B[1;33m         \u001B[0mobj\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0munpickle\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    427\u001B[0m         \u001B[0mobj\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_load_specials\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmmap\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcompress\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msubname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mf:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py\u001B[0m in \u001B[0;36munpickle\u001B[1;34m(fname)\u001B[0m\n\u001B[0;32m   1383\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mversion_info\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1384\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0m_pickle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'latin1'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1385\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mf:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\random\\_pickle.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mmtrand\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mRandomState\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0m_philox\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mPhilox\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0m_pcg64\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mPCG64\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m_philox.pyx\u001B[0m in \u001B[0;36minit numpy.random._philox\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_bit_generator.pyx\u001B[0m in \u001B[0;36minit numpy.random._bit_generator\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: type object 'numpy.random._bit_generator.SeedSequence' has no attribute '__reduce_cython__'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-5-352cffc8a1b0>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;31m# file = '../../PretrainedData/Tencent_AILab_ChineseEmbedding/Tencent_AILab_ChineseEmbedding.txt'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mfile\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'../../DataSets/Word2Vect/xingrong_50_thulac/word2vect_50_w5.model'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m \u001B[0mword2vec\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mWord2Vec\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfile\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m \u001B[1;31m# word2vec = KeyedVectors.load_word2vec_format(file, binary=False)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[0mword2vec\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minit_sims\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mreplace\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# 神奇，很省内存，可以运算most_similar\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mf:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1339\u001B[0m             \u001B[0mlogger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Model saved using code from earlier Gensim Version. Re-loading old model in a compatible way.'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1340\u001B[0m             \u001B[1;32mfrom\u001B[0m \u001B[0mgensim\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodels\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdeprecated\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mword2vec\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mload_old_word2vec\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1341\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mload_old_word2vec\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1342\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1343\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mf:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\deprecated\\word2vec.py\u001B[0m in \u001B[0;36mload_old_word2vec\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    170\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    171\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mload_old_word2vec\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 172\u001B[1;33m     \u001B[0mold_model\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mWord2Vec\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    173\u001B[0m     \u001B[0mvector_size\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mold_model\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'vector_size'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mold_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayer1_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    174\u001B[0m     params = {\n",
      "\u001B[1;32mf:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\deprecated\\word2vec.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1639\u001B[0m     \u001B[1;33m@\u001B[0m\u001B[0mclassmethod\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1640\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcls\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1641\u001B[1;33m         \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mWord2Vec\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcls\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1642\u001B[0m         \u001B[1;31m# update older models\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1643\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'table'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mf:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\deprecated\\old_saveload.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(cls, fname, mmap)\u001B[0m\n\u001B[0;32m     85\u001B[0m         \u001B[0mcompress\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msubname\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSaveLoad\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_adapt_by_suffix\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     86\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 87\u001B[1;33m         \u001B[0mobj\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0munpickle\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     88\u001B[0m         \u001B[0mobj\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_load_specials\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmmap\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcompress\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msubname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     89\u001B[0m         \u001B[0mlogger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"loaded %s\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mf:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\deprecated\\old_saveload.py\u001B[0m in \u001B[0;36munpickle\u001B[1;34m(fname)\u001B[0m\n\u001B[0;32m    377\u001B[0m             b'gensim.models.wrappers.fasttext', b'gensim.models.deprecated.fasttext_wrapper')\n\u001B[0;32m    378\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mversion_info\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 379\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0m_pickle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloads\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfile_bytes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'latin1'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    380\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    381\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0m_pickle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloads\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfile_bytes\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: Can't get attribute 'Word2VecKeyedVectors' on <module 'gensim.models.deprecated.keyedvectors' from 'f:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\gensim\\\\models\\\\deprecated\\\\keyedvectors.py'>"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "\n",
    "# file = '../../PretrainedData/Tencent_AILab_ChineseEmbedding/Tencent_AILab_ChineseEmbedding.txt'\n",
    "file = '../../DataSets/Word2Vect/xingrong_50_thulac/word2vect_50_w5.model'\n",
    "word2vec = Word2Vec.load(file)\n",
    "# word2vec = KeyedVectors.load_word2vec_format(file, binary=False)\n",
    "word2vec.init_sims(replace=True)  # 神奇，很省内存，可以运算most_similar\n",
    "word2vec.vector_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(word2vec)\n",
    "# print(word2vec.wv.vocab)\n",
    "# print(len(word2vec.index2word))\n",
    "print(len(word2vec.wv.index2word))\n",
    "print(word2vec.wv.index2word[0])\n",
    "print(word2vec.wv.index2word[1])\n",
    "print(word2vec.wv.index2word[2])\n",
    "print(word2vec.wv.index2word[1522])\n",
    "print(word2vec.wv.index2entity[1522])\n",
    "print(word2vec.similar_by_word('中国'))\n",
    "print(word2vec.similar_by_word('天才'))\n",
    "print(word2vec.wv)\n",
    "print('word2vec.wv.vocab ---- >', word2vec.wv.vocab)\n",
    "print(word2vec.wv.index2word)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wordEmbedding = [word2vec.wv[word]  for word in word2vec.wv.index2word]\n",
    "word2index = { word:i for i, word in enumerate(word2vec.wv.index2word)}\n",
    "print(wordEmbedding[:10])\n",
    "print(word2index['中国'])\n",
    "print(word2index['天才'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#创建模型\n",
    "wordEmbedding = torch.FloatTensor(wordEmbedding)\n",
    "num_embeddings = len(word2vec.wv.index2word)\n",
    "model = HAN(num_embeddings, embedding_dim=word2vec.wv.vector_size)\n",
    "print(model)\n",
    "model.embed.from_pretrained(wordEmbedding)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 读取训练数据"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_ngrams(word, num_min = 1, num_max = 3):\n",
    "    ngrams =[]\n",
    "    for ngram_length in range(num_min, min(len(word), num_max) + 1):\n",
    "        for i in range(len(word) - ngram_length + 1):\n",
    "            # print(i, i + ngram_length)\n",
    "            ngrams.append(word[i : i + ngram_length])\n",
    "    # print(ngrams)\n",
    "    return list(set(ngrams))\n",
    "\n",
    "print(compute_ngrams('you'))\n",
    "print(compute_ngrams('I Think'))\n",
    "print(compute_ngrams('中华人民共和国万岁'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 从词向量文本文件 word2vec 中获取词向量，如果获取到直接返回，若没有获取到，那么把这个词拆开\n",
    "# 成为 ngrams 的新词组，并在 word2vec 中找新词组中的词向量并相加取平均，最后得到平均词向量输出\n",
    "def wordVec(word, word2vec, min_n = 1, max_n = 3):\n",
    "    # 确认词向量维度\n",
    "    word_size = word2vec.wv.syn0[0].shape[0]\n",
    "\n",
    "    # 如果在词典之中，直接返回词向量\n",
    "    if word in word2vec.wv.vocab.keys():\n",
    "        return word2vec[word]\n",
    "    else:\n",
    "        # 计算word的ngrams词组\n",
    "        ngrams = compute_ngrams(word, min_n, max_n)\n",
    "        # 不在词典的情况下\n",
    "        word_vec = numpy.zeros(word_size, dtype=numpy.float32)\n",
    "        ngrams_found = 0\n",
    "        ngrams_single = [ng for ng in ngrams if len(ng) == 1]\n",
    "        ngrams_more = [ng for ng in ngrams if len(ng) > 1]\n",
    "        # 先只接受2个单词长度以上的词向量\n",
    "        for ngram in ngrams_more:\n",
    "            if ngram in word2vec.wv.vocab.keys():\n",
    "                word_vec += word2vec[ngram]\n",
    "                ngrams_found += 1\n",
    "                #print(ngram)\n",
    "        # 如果，没有匹配到，那么最后是考虑单个词向量\n",
    "        if ngrams_found == 0:\n",
    "            for ngram in ngrams_single:\n",
    "                word_vec += word2vec[ngram]\n",
    "                ngrams_found += 1\n",
    "        if word_vec.any():\n",
    "            return word_vec / max(1, ngrams_found)\n",
    "        else:\n",
    "            # 不抛出异常，而是打印提示，并返回0向量。\n",
    "            print(KeyError('all ngrams for word %s absent from model' % word))\n",
    "            return word_vec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DotDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        dict.__init__(self, *args, **kwargs)\n",
    "        self.__dict__ = self"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def isNan(a):\n",
    "    return a != a\n",
    "\n",
    "class RatingData(data.Dataset):\n",
    "    def __init__(self, path, word2index, max_row = -1, trainTestRate = 0.85, isTrain = True):\n",
    "        self.token_list = []\n",
    "        self.label_list = []\n",
    "        # self.token_positions = torch.tensor([i for i in range(100)])\n",
    "\n",
    "        print('地址不应该包含 ratings.csv   path:',path) # 地址不应该包含 ratings.csv\n",
    "\n",
    "        ratings_clean_filename = os.path.join(path, 'ratings_clean_4_HAN.csv')\n",
    "        ratings_filename = os.path.join(path, 'ratings.csv')\n",
    "        if os.path.isfile(ratings_clean_filename):\n",
    "            clean_pd = pd.read_csv(ratings_clean_filename)\n",
    "        else:\n",
    "            print('没有找到缓存的文件%s, 读取源文件%s'%(ratings_clean_filename, ratings_filename))\n",
    "            ratings_pd = pd.read_csv(ratings_filename)\n",
    "            print('开始生成缓存文件%s'%(ratings_clean_filename))\n",
    "            clean_pd = pd.DataFrame({\n",
    "                'userId':[],\n",
    "                'restId':[],\n",
    "                'rating':[],\n",
    "                'comment':[],\n",
    "            })\n",
    "            nonRatingCount = 0\n",
    "            for i, row in ratings_pd.iterrows():\n",
    "                if max_row != -1 and i > max_row:\n",
    "                    break\n",
    "                if not isinstance(row['comment'], str) or row['comment'] == '':\n",
    "                    # print(i + 1, row['comment'])\n",
    "                    nonRatingCount += 1\n",
    "                    continue\n",
    "                r0 = row['rating']\n",
    "                r1 = row['rating_env']\n",
    "                r2 = row['rating_flavor']\n",
    "                r3 = row['rating_service']\n",
    "                if r0 == '' or isNan(r0): r0 = 0 # 假设总评分为 0 表示未评分\n",
    "                if r1 == '' or isNan(r1): r1 = 3\n",
    "                if r2 == '' or isNan(r2): r2 = 3\n",
    "                if r3 == '' or isNan(r3): r3 = 3\n",
    "                r0 = round(r0 * 0.5 + (r1 + r2 + r3) * 0.1666666)\n",
    "                if i % 10000 == 9999:\n",
    "                    print(i + 1, r0)\n",
    "\n",
    "                # token = tokenizer.encode(text=str(row['comment']), max_length=100, pad_to_max_length = True)\n",
    "                words = thulac.cut(row['comment'], text=True)\n",
    "                # 0 在词向量集中是‘，’，换个词向量集可能表示其他\n",
    "                token = [ word2index[words[i]] if i < len(words) and words[i] in word2index else 0 for i in range(100)] \n",
    "                \n",
    "                newRow = DotDict()\n",
    "                newRow.userId = [row['userId']]\n",
    "                newRow.restId = [row['restId']]\n",
    "                newRow.rating = [r0]\n",
    "                newRow.comment = [json.dumps(token)]\n",
    "\n",
    "                clean_pd = clean_pd.append(pd.DataFrame(newRow), ignore_index=True)\n",
    "            print('空的评论数量： %d'%(nonRatingCount))\n",
    "            clean_pd.to_csv(ratings_clean_filename)\n",
    "\n",
    "        # 读取\n",
    "        if isTrain:\n",
    "            temp_pd = clean_pd[ : int(len(clean_pd) * trainTestRate)]\n",
    "        else:\n",
    "            temp_pd = clean_pd[int(len(clean_pd) * trainTestRate) : ]\n",
    "\n",
    "        for i, row in temp_pd.iterrows():\n",
    "            if max_row != -1 and i > max_row:\n",
    "                break\n",
    "\n",
    "            self.label_list.append(torch.tensor(row['rating']).long())\n",
    "            self.token_list.append(torch.from_numpy(numpy.array( json.loads(row['comment']) ) ).long())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.token_list[index], self.label_list[index]#, self.token_positions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_list)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ratingData = RatingData('../../DataSets/yf_dianping',\n",
    "                             word2index = word2index,\n",
    "                             max_row= 100000,\n",
    "                             isTrain=True,\n",
    "                             )\n",
    "trainLoader = torch.utils.data.DataLoader(dataset=ratingData,\n",
    "                                          batch_size=10,\n",
    "                                          shuffle = True,\n",
    "                                          # num_workers = 0,\n",
    "                                          )\n",
    "ratingData2 = RatingData('../../DataSets/yf_dianping',\n",
    "                             word2index = word2index,\n",
    "                             max_row= 100000,\n",
    "                             isTrain=False,\n",
    "                             )\n",
    "testLoader = torch.utils.data.DataLoader(dataset=ratingData2,\n",
    "                                          batch_size=10,\n",
    "                                          shuffle = True,\n",
    "                                          # num_workers = 0,\n",
    "                                          )\n",
    "print(len(ratingData.label_list))\n",
    "print(len(ratingData2.label_list))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 开始训练过程"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def trainOneEpoch(epoch, model:HAN, trainLoader, optimizer:Optimizer, lossFunc):\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        lossFunc = lossFunc.cuda()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    startTime = time.time()\n",
    "    for i, (x, y) in enumerate(trainLoader):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        outputs = model(x)\n",
    "        loss = lossFunc(outputs, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 1000 == 999:\n",
    "            print('Epoch %d, %d/%d, loss:%f ' % (epoch, i, len(trainLoader), loss.means()))\n",
    "    print('Epoch %d cost time: %.3fs' % (epoch, time.time() - startTime))\n",
    "\n",
    "\n",
    "def testModel(epoch, model:HAN, testLoader):\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    startTime = time.time()\n",
    "    for i, (x, y, p) in enumerate(testLoader):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        outputs = model(x)\n",
    "        # loss = outputs[0]\n",
    "        # logits = outputs[1]\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += x.size(0)\n",
    "        correct += predicted.data.eq(y.data).cpu().sum()\n",
    "\n",
    "        if i % 1000 == 999:\n",
    "            print('Epoch Test %d, %d/%d' % (epoch, i, len(testLoader)))\n",
    "    print('Epoch Test %d cost time: %.3fs' % (epoch, time.time() - startTime))\n",
    "    print('准确率： %.3f' % (correct / total))\n",
    "\n",
    "\n",
    "def train(nepoch, model, modelSavePath):\n",
    "    optimizer=torch.optim.SGD(model.parameters(), momentum=0.9, lr=0.001)\n",
    "    lossFunc =torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(nepoch):\n",
    "        trainOneEpoch(epoch, model, trainLoader, optimizer, lossFunc)\n",
    "        testModel(epoch, model, testLoader)\n",
    "    torch.save(model.state_dict(), modelSavePath)\n",
    "\n",
    "def eval(modelSavePath, isLoad = True):\n",
    "    if isLoad: model.load_state_dict(torch.load(modelSavePath))\n",
    "    testModel(0, model, testLoader)\n",
    "\n",
    "train(10, model, 'EmotionAnalyzeModelData.model')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}