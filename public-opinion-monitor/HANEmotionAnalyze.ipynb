{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Model loaded succeed\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from HANEmotionAnalyze import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy\n",
    "import time, math\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import thulac\n",
    "thulac = thulac.thulac()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([[[ 0.5330,  0.1214,  0.5117],\n",
      "         [ 0.6444,  0.2468,  1.7833],\n",
      "         [-1.9501, -1.5159,  2.1440],\n",
      "         [-0.1257,  0.6628, -1.1044],\n",
      "         [-1.1152,  0.3954,  0.6023]],\n",
      "\n",
      "        [[ 0.6444,  0.2468,  1.7833],\n",
      "         [-0.1257,  0.6628, -1.1044],\n",
      "         [-0.1768,  0.3839,  0.3991],\n",
      "         [-1.9501, -1.5159,  2.1440],\n",
      "         [ 1.1567, -0.8254,  0.0972]]], grad_fn=<EmbeddingBackward>)\n",
      "tensor([[[ 1.2689,  0.9966,  0.1185],\n",
      "         [ 0.8692,  0.6171, -0.2417],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [-0.4652, -0.9552, -1.0436],\n",
      "         [ 2.2206,  0.8936,  0.4039]],\n",
      "\n",
      "        [[ 0.8692,  0.6171, -0.2417],\n",
      "         [-0.4652, -0.9552, -1.0436],\n",
      "         [-1.0174, -0.1631, -0.3047],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.2563,  0.5658, -0.5279]]], grad_fn=<EmbeddingBackward>)\n",
      "tensor([[4.0000, 5.1000, 6.3000]])\n",
      "tensor([[1.0000, 2.3000, 3.0000]])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# import torch.functional as F\n",
    "# \n",
    "embedding = nn.Embedding(10, 3)\n",
    "input = torch.LongTensor([[0, 1,2,4,5],[1, 4,3,2,9]])\n",
    "print(embedding(input))\n",
    "embedding2 = nn.Embedding(10, 3, padding_idx=2)\n",
    "# input = torch.LongTensor([[1,2,0,5, 6,7,8,9]])\n",
    "print(embedding2(input))\n",
    "weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n",
    "embedding3 = nn.Embedding.from_pretrained(weight)\n",
    "input = torch.LongTensor([1])\n",
    "print(embedding3(input))\n",
    "input = torch.LongTensor([0])\n",
    "print(embedding3(input))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 读取词向量\n",
    "建立词语列表"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "\n",
    "# file = '../../PretrainedData/Tencent_AILab_ChineseEmbedding/Tencent_AILab_ChineseEmbedding.txt'\n",
    "file = '../../DataSets/Word2Vect/xingrong_50_thulac/word2vect_50_w5.model'\n",
    "word2vec = Word2Vec.load(file)\n",
    "# word2vec = KeyedVectors.load_word2vec_format(file, binary=False)\n",
    "word2vec.init_sims(replace=True)  # 神奇，很省内存，可以运算most_similar\n",
    "word2vec.vector_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(word2vec)\n",
    "# print(word2vec.wv.vocab)\n",
    "# print(len(word2vec.index2word))\n",
    "print(len(word2vec.wv.index2word))\n",
    "print(word2vec.wv.index2word[0])\n",
    "print(word2vec.wv.index2word[1])\n",
    "print(word2vec.wv.index2word[2])\n",
    "print(word2vec.wv.index2word[1522])\n",
    "print(word2vec.wv.index2entity[1522])\n",
    "print(word2vec.similar_by_word('中国'))\n",
    "print(word2vec.similar_by_word('天才'))\n",
    "print(word2vec.wv)\n",
    "print('word2vec.wv.vocab ---- >', word2vec.wv.vocab)\n",
    "print(word2vec.wv.index2word)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wordEmbedding = [word2vec.wv[word]  for word in word2vec.wv.index2word]\n",
    "word2index = { word:i for i, word in enumerate(word2vec.wv.index2word)}\n",
    "print(wordEmbedding[:10])\n",
    "print(word2index['中国'])\n",
    "print(word2index['天才'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#创建模型\n",
    "wordEmbedding = torch.FloatTensor(wordEmbedding)\n",
    "num_embeddings = len(word2vec.wv.index2word)\n",
    "model = HAN(num_embeddings, embedding_dim=word2vec.wv.vector_size)\n",
    "print(model)\n",
    "model.embed.from_pretrained(wordEmbedding)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 读取训练数据"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_ngrams(word, num_min = 1, num_max = 3):\n",
    "    ngrams =[]\n",
    "    for ngram_length in range(num_min, min(len(word), num_max) + 1):\n",
    "        for i in range(len(word) - ngram_length + 1):\n",
    "            # print(i, i + ngram_length)\n",
    "            ngrams.append(word[i : i + ngram_length])\n",
    "    # print(ngrams)\n",
    "    return list(set(ngrams))\n",
    "\n",
    "print(compute_ngrams('you'))\n",
    "print(compute_ngrams('I Think'))\n",
    "print(compute_ngrams('中华人民共和国万岁'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 从词向量文本文件 word2vec 中获取词向量，如果获取到直接返回，若没有获取到，那么把这个词拆开\n",
    "# 成为 ngrams 的新词组，并在 word2vec 中找新词组中的词向量并相加取平均，最后得到平均词向量输出\n",
    "def wordVec(word, word2vec, min_n = 1, max_n = 3):\n",
    "    # 确认词向量维度\n",
    "    word_size = word2vec.wv.syn0[0].shape[0]\n",
    "\n",
    "    # 如果在词典之中，直接返回词向量\n",
    "    if word in word2vec.wv.vocab.keys():\n",
    "        return word2vec[word]\n",
    "    else:\n",
    "        # 计算word的ngrams词组\n",
    "        ngrams = compute_ngrams(word, min_n, max_n)\n",
    "        # 不在词典的情况下\n",
    "        word_vec = numpy.zeros(word_size, dtype=numpy.float32)\n",
    "        ngrams_found = 0\n",
    "        ngrams_single = [ng for ng in ngrams if len(ng) == 1]\n",
    "        ngrams_more = [ng for ng in ngrams if len(ng) > 1]\n",
    "        # 先只接受2个单词长度以上的词向量\n",
    "        for ngram in ngrams_more:\n",
    "            if ngram in word2vec.wv.vocab.keys():\n",
    "                word_vec += word2vec[ngram]\n",
    "                ngrams_found += 1\n",
    "                #print(ngram)\n",
    "        # 如果，没有匹配到，那么最后是考虑单个词向量\n",
    "        if ngrams_found == 0:\n",
    "            for ngram in ngrams_single:\n",
    "                word_vec += word2vec[ngram]\n",
    "                ngrams_found += 1\n",
    "        if word_vec.any():\n",
    "            return word_vec / max(1, ngrams_found)\n",
    "        else:\n",
    "            # 不抛出异常，而是打印提示，并返回0向量。\n",
    "            print(KeyError('all ngrams for word %s absent from model' % word))\n",
    "            return word_vec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DotDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        dict.__init__(self, *args, **kwargs)\n",
    "        self.__dict__ = self"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def isNan(a):\n",
    "    return a != a\n",
    "\n",
    "class RatingData(data.Dataset):\n",
    "    def __init__(self, path, word2index, max_row = -1, trainTestRate = 0.85, isTrain = True):\n",
    "        self.token_list = []\n",
    "        self.label_list = []\n",
    "        # self.token_positions = torch.tensor([i for i in range(100)])\n",
    "\n",
    "        print('地址不应该包含 ratings.csv   path:',path) # 地址不应该包含 ratings.csv\n",
    "\n",
    "        ratings_clean_filename = os.path.join(path, 'ratings_clean_4_HAN.csv')\n",
    "        ratings_filename = os.path.join(path, 'ratings.csv')\n",
    "        if os.path.isfile(ratings_clean_filename):\n",
    "            clean_pd = pd.read_csv(ratings_clean_filename)\n",
    "        else:\n",
    "            print('没有找到缓存的文件%s, 读取源文件%s'%(ratings_clean_filename, ratings_filename))\n",
    "            ratings_pd = pd.read_csv(ratings_filename)\n",
    "            print('开始生成缓存文件%s'%(ratings_clean_filename))\n",
    "            clean_pd = pd.DataFrame({\n",
    "                'userId':[],\n",
    "                'restId':[],\n",
    "                'rating':[],\n",
    "                'comment':[],\n",
    "            })\n",
    "            nonRatingCount = 0\n",
    "            for i, row in ratings_pd.iterrows():\n",
    "                if max_row != -1 and i > max_row:\n",
    "                    break\n",
    "                if not isinstance(row['comment'], str) or row['comment'] == '':\n",
    "                    # print(i + 1, row['comment'])\n",
    "                    nonRatingCount += 1\n",
    "                    continue\n",
    "                r0 = row['rating']\n",
    "                r1 = row['rating_env']\n",
    "                r2 = row['rating_flavor']\n",
    "                r3 = row['rating_service']\n",
    "                if r0 == '' or isNan(r0): r0 = 0 # 假设总评分为 0 表示未评分\n",
    "                if r1 == '' or isNan(r1): r1 = 3\n",
    "                if r2 == '' or isNan(r2): r2 = 3\n",
    "                if r3 == '' or isNan(r3): r3 = 3\n",
    "                r0 = round(r0 * 0.5 + (r1 + r2 + r3) * 0.1666666)\n",
    "                if i % 10000 == 9999:\n",
    "                    print(i + 1, r0)\n",
    "\n",
    "                # token = tokenizer.encode(text=str(row['comment']), max_length=100, pad_to_max_length = True)\n",
    "                words = thulac.cut(row['comment'], text=True)\n",
    "                # 0 在词向量集中是‘，’，换个词向量集可能表示其他\n",
    "                token = [ word2index[words[i]] if i < len(words) and words[i] in word2index else 0 for i in range(100)] \n",
    "                \n",
    "                newRow = DotDict()\n",
    "                newRow.userId = [row['userId']]\n",
    "                newRow.restId = [row['restId']]\n",
    "                newRow.rating = [r0]\n",
    "                newRow.comment = [json.dumps(token)]\n",
    "\n",
    "                clean_pd = clean_pd.append(pd.DataFrame(newRow), ignore_index=True)\n",
    "            print('空的评论数量： %d'%(nonRatingCount))\n",
    "            clean_pd.to_csv(ratings_clean_filename)\n",
    "\n",
    "        # 读取\n",
    "        if isTrain:\n",
    "            temp_pd = clean_pd[ : int(len(clean_pd) * trainTestRate)]\n",
    "        else:\n",
    "            temp_pd = clean_pd[int(len(clean_pd) * trainTestRate) : ]\n",
    "\n",
    "        for i, row in temp_pd.iterrows():\n",
    "            if max_row != -1 and i > max_row:\n",
    "                break\n",
    "\n",
    "            self.label_list.append(torch.tensor(row['rating']).long())\n",
    "            self.token_list.append(torch.from_numpy(numpy.array( json.loads(row['comment']) ) ).long())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.token_list[index], self.label_list[index]#, self.token_positions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_list)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ratingData = RatingData('../../DataSets/yf_dianping',\n",
    "                             word2index = word2index,\n",
    "                             max_row= 100000,\n",
    "                             isTrain=True,\n",
    "                             )\n",
    "trainLoader = torch.utils.data.DataLoader(dataset=ratingData,\n",
    "                                          batch_size=10,\n",
    "                                          shuffle = True,\n",
    "                                          # num_workers = 0,\n",
    "                                          )\n",
    "ratingData2 = RatingData('../../DataSets/yf_dianping',\n",
    "                             word2index = word2index,\n",
    "                             max_row= 100000,\n",
    "                             isTrain=False,\n",
    "                             )\n",
    "testLoader = torch.utils.data.DataLoader(dataset=ratingData2,\n",
    "                                          batch_size=10,\n",
    "                                          shuffle = True,\n",
    "                                          # num_workers = 0,\n",
    "                                          )\n",
    "print(len(ratingData.label_list))\n",
    "print(len(ratingData2.label_list))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}