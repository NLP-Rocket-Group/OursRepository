{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Model loaded succeed\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from HANEmotionAnalyze import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import numpy\n",
    "import time, math\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import thulac\n",
    "thulac = thulac.thulac()\n",
    "import jieba"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([[[-1.0199, -0.6915, -1.6168],\n",
      "         [ 0.7839, -0.0831,  0.2477],\n",
      "         [ 1.2109,  0.8160,  2.6015],\n",
      "         [ 0.6143,  1.8320, -0.2086],\n",
      "         [-1.4623,  1.1073, -0.5561]],\n",
      "\n",
      "        [[ 0.7839, -0.0831,  0.2477],\n",
      "         [ 0.6143,  1.8320, -0.2086],\n",
      "         [-0.2993,  0.0033,  1.5736],\n",
      "         [ 1.2109,  0.8160,  2.6015],\n",
      "         [-0.0288,  1.0980, -1.2566]]], grad_fn=<EmbeddingBackward>)\n",
      "tensor([[[-0.2129,  0.4830,  0.2750],\n",
      "         [-0.7534, -0.2165,  0.4123],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6356, -0.4989, -0.3891],\n",
      "         [ 0.0896, -1.6047, -0.6569]],\n",
      "\n",
      "        [[-0.7534, -0.2165,  0.4123],\n",
      "         [ 0.6356, -0.4989, -0.3891],\n",
      "         [-1.1882, -0.4552, -0.3379],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [-1.1927,  1.1059,  0.1896]]], grad_fn=<EmbeddingBackward>)\n",
      "tensor([[4.0000, 5.1000, 6.3000]])\n",
      "tensor([[1.0000, 2.3000, 3.0000]])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# import torch.functional as F\n",
    "# \n",
    "embedding = nn.Embedding(10, 3)\n",
    "input = torch.LongTensor([[0, 1,2,4,5],[1, 4,3,2,9]])\n",
    "print(embedding(input))\n",
    "embedding2 = nn.Embedding(10, 3, padding_idx=2)\n",
    "# input = torch.LongTensor([[1,2,0,5, 6,7,8,9]])\n",
    "print(embedding2(input))\n",
    "weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n",
    "embedding3 = nn.Embedding.from_pretrained(weight)\n",
    "input = torch.LongTensor([1])\n",
    "print(embedding3(input))\n",
    "input = torch.LongTensor([0])\n",
    "print(embedding3(input))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 读取词向量\n",
    "建立词语列表"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "300"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 3
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "\n",
    "# file = '../../PretrainedData/Tencent_AILab_ChineseEmbedding/Tencent_AILab_ChineseEmbedding.txt'\n",
    "# file = '../../DataSets/Word2Vect/xingrong_50_thulac/word2vect_50_w5.model'\n",
    "file = '../../DataSets/Word2Vect/xiejunjie_300_jieba/wiki_han_word2vec_300维度.model'\n",
    "word2vec = Word2Vec.load(file)\n",
    "# word2vec = KeyedVectors.load_word2vec_format(file, binary=False)\n",
    "word2vec.init_sims(replace=True)  # 神奇，很省内存，可以运算most_similar\n",
    "word2vec.vector_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Word2Vec(vocab=109683, size=300, alpha=0.025)\n",
      "109683\n",
      "\n",
      "的\n",
      "在\n",
      "厅\n",
      "厅\n",
      "[('我国', 0.4018472731113434), ('中方', 0.39661142230033875), ('各国', 0.3935665488243103), ('欧美', 0.39242100715637207), ('强国', 0.3893248438835144), ('亚洲', 0.3873169422149658), ('国内', 0.38274306058883667), ('非洲', 0.36376285552978516), ('哈萨克斯坦', 0.36366286873817444), ('外国', 0.36008766293525696)]\n",
      "[('死神', 0.6855337619781494), ('泰勒', 0.6751863956451416), ('天生', 0.6720190644264221), ('青梅竹马', 0.6718809008598328), ('英俊', 0.6717637181282043), ('杰克', 0.6692911386489868), ('女人', 0.6632643938064575), ('私生子', 0.6598134636878967), ('王子', 0.6573523283004761), ('安娜', 0.6552286744117737)]\n",
      "<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x00000232BED31148>\n",
      "word2vec.wv.vocab ---- > "
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(word2vec)\n",
    "# print(word2vec.wv.vocab)\n",
    "# print(len(word2vec.index2word))\n",
    "print(len(word2vec.wv.index2word))\n",
    "print(word2vec.wv.index2word[0])\n",
    "print(word2vec.wv.index2word[1])\n",
    "print(word2vec.wv.index2word[2])\n",
    "print(word2vec.wv.index2word[1522])\n",
    "print(word2vec.wv.index2entity[1522])\n",
    "print(word2vec.similar_by_word('中国'))\n",
    "print(word2vec.similar_by_word('天才'))\n",
    "print(word2vec.wv)\n",
    "print('word2vec.wv.vocab ---- >', word2vec.wv.vocab)\n",
    "print(word2vec.wv.index2word)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[array([ 6.30656257e-02, -1.12314887e-01,  4.85262945e-02, -8.55197757e-02,\n",
      "        4.55908328e-02,  6.46558106e-02, -4.27534170e-02,  5.36949895e-02,\n",
      "       -2.26631403e-01,  6.25260845e-02,  1.83971524e-02, -3.61814126e-02,\n",
      "       -7.80684203e-02, -9.78453308e-02,  3.07323225e-02,  4.43015546e-02,\n",
      "       -1.03984829e-02, -3.16940509e-02, -4.82366160e-02, -2.55376995e-02,\n",
      "       -1.94135922e-04, -2.42569018e-02, -5.79886399e-02,  1.26771117e-02,\n",
      "        7.19933212e-03, -5.69830649e-02, -1.26355989e-02,  2.16077268e-02,\n",
      "       -8.15668255e-02,  9.75341350e-03,  4.03154232e-02, -2.12772768e-02,\n",
      "        4.27430905e-02, -2.53562219e-02,  9.61699188e-02,  4.80885766e-02,\n",
      "       -5.42805195e-02, -3.23644653e-02, -1.98951829e-02,  3.14243254e-03,\n",
      "       -4.23267344e-03,  8.15014020e-02,  5.07650971e-02, -6.43669665e-02,\n",
      "        5.04725389e-02,  2.95574106e-02, -1.39722880e-02, -9.62358862e-02,\n",
      "       -4.40400690e-02, -2.26076338e-02, -4.84022610e-02,  6.48457836e-03,\n",
      "       -5.65880351e-02, -2.68249288e-02, -9.42117721e-03, -5.28412238e-02,\n",
      "        1.59993432e-02, -5.98132756e-05,  2.67991647e-02,  8.58393386e-02,\n",
      "       -6.14708215e-02, -1.31292026e-02,  9.25972536e-02, -1.43445958e-03,\n",
      "        7.76087940e-02, -8.47513080e-02, -6.86354062e-04,  4.90272306e-02,\n",
      "        4.52526994e-02,  3.10066957e-02, -8.88350939e-07, -2.16583558e-03,\n",
      "        2.12598816e-02, -6.56151325e-02,  1.27818763e-01, -4.38730493e-02,\n",
      "       -5.34825074e-03,  4.39950265e-02,  8.09856728e-02,  7.09735006e-02,\n",
      "       -7.31261298e-02,  3.40790004e-02,  4.49708104e-02,  5.00067770e-02,\n",
      "        2.73315981e-02, -7.53549486e-02,  1.78500488e-02, -3.48950252e-02,\n",
      "       -1.32188827e-01,  6.42795339e-02,  1.65490992e-02, -4.45144027e-02,\n",
      "        5.84028549e-02, -8.89065713e-02, -8.25003758e-02,  6.15820512e-02,\n",
      "       -8.85820165e-02, -6.39488548e-02, -6.73423856e-02, -5.77983782e-02,\n",
      "        5.27221262e-02,  4.84629795e-02,  5.70684411e-02,  2.56182048e-02,\n",
      "       -3.42088714e-02, -2.53666565e-03, -3.39778289e-02, -4.57062125e-02,\n",
      "       -3.82729806e-02, -2.95173340e-02,  4.70805801e-02, -6.08570594e-03,\n",
      "        1.64899435e-02,  6.94641247e-02, -4.60656360e-03, -3.71345729e-02,\n",
      "        7.28948340e-02,  1.37535445e-02, -1.14699885e-01, -5.77401817e-02,\n",
      "       -9.34074167e-03,  7.46211316e-03, -5.42243086e-02, -8.79147742e-03,\n",
      "        2.63150558e-02, -1.69871338e-02,  5.41037098e-02,  6.59247935e-02,\n",
      "        1.67181361e-02,  4.41165529e-02,  3.86219025e-02, -1.58149321e-02,\n",
      "       -3.33839320e-02, -2.86918972e-02, -2.84854881e-03,  4.81493492e-03,\n",
      "        3.35541964e-02, -7.10851327e-02, -3.55387367e-02, -6.68534562e-02,\n",
      "        6.75072372e-02,  2.81740408e-02,  6.61929557e-03,  2.68618902e-03,\n",
      "       -2.00795215e-02, -1.88215468e-02,  1.50708541e-01, -2.18910743e-02,\n",
      "        1.31342802e-02, -5.80100119e-02, -3.91371250e-02,  6.13707714e-02,\n",
      "        3.14369723e-02,  1.14150383e-02,  3.81162576e-02,  2.19377056e-02,\n",
      "       -2.88434848e-02, -3.10160760e-02,  3.07617262e-02, -4.83107492e-02,\n",
      "        3.51032498e-03, -7.94669148e-03,  4.07654122e-02, -3.98778766e-02,\n",
      "        1.02130271e-01,  3.84604447e-02, -5.77925220e-02, -1.03571162e-01,\n",
      "        4.88629751e-02,  1.20413192e-01,  2.72024469e-03,  8.06539580e-02,\n",
      "       -3.32767032e-02,  9.14076809e-03,  7.51832053e-02, -3.36114019e-02,\n",
      "        2.57226769e-02,  6.10233434e-02,  2.69902591e-02, -2.56587546e-02,\n",
      "        9.70835239e-02, -2.53332611e-02, -7.78485686e-02,  2.00512074e-02,\n",
      "       -2.33499277e-02, -6.93145767e-02,  3.09368446e-02, -2.11694278e-02,\n",
      "        4.14785184e-02, -3.52637023e-02, -7.20726848e-02,  2.65667569e-02,\n",
      "       -3.44735272e-02,  2.81915013e-02,  5.33172302e-02, -9.41388085e-02,\n",
      "       -6.96730837e-02, -2.56213564e-02,  5.17414249e-02, -2.95046251e-02,\n",
      "       -4.68526594e-02, -1.23722903e-01, -1.10665567e-01, -2.40235049e-02,\n",
      "        2.24292651e-02, -2.53645647e-02, -3.13879512e-02,  3.15920226e-02,\n",
      "       -7.91629478e-02, -1.41667984e-02,  3.82459685e-02,  8.04619864e-02,\n",
      "       -2.18224921e-03, -9.78122130e-02, -5.10852225e-02, -4.17598113e-02,\n",
      "       -2.76642926e-02, -1.54269919e-01, -1.63619414e-01,  5.58192022e-02,\n",
      "        1.08085766e-01, -4.33016047e-02,  5.85271493e-02, -3.32978480e-02,\n",
      "        6.34150114e-03,  1.66612659e-02, -6.55340999e-02,  1.21571058e-02,\n",
      "       -4.34048520e-03,  1.00306403e-02, -1.51651055e-02,  5.94954342e-02,\n",
      "        1.00235559e-01,  5.70678338e-02, -9.29579977e-03,  2.74547637e-02,\n",
      "       -2.65176455e-03,  2.56057340e-03, -6.01494312e-02, -5.14229387e-02,\n",
      "        9.31526721e-02, -7.33152106e-02, -2.76716035e-02, -6.18751571e-02,\n",
      "        6.26666322e-02,  9.24999174e-03,  9.48182717e-02, -7.11040571e-03,\n",
      "        4.70663495e-02, -2.28074915e-03,  4.00182232e-03,  8.35489109e-02,\n",
      "       -2.57432945e-02,  1.30927622e-01, -9.29127634e-03, -5.01400828e-02,\n",
      "        7.28835613e-02,  5.56571633e-02,  2.44132672e-02,  8.40459689e-02,\n",
      "       -7.06698447e-02, -4.49935906e-03,  1.32746264e-01,  1.23881839e-01,\n",
      "        6.00073375e-02,  2.85008959e-02, -4.52274457e-02, -5.60277654e-03,\n",
      "       -6.18442222e-02,  6.75204322e-02,  2.13755425e-02, -5.67645356e-02,\n",
      "       -1.05406150e-01, -4.78417054e-02, -8.86159465e-02, -9.50396359e-02,\n",
      "       -3.89411976e-03,  5.87190362e-03,  4.32894155e-02, -4.09058779e-02,\n",
      "       -9.36807394e-02,  2.73718573e-02,  7.21197352e-02, -1.00012966e-01,\n",
      "       -6.78094104e-03,  1.08832791e-01, -2.83442587e-02, -9.29948762e-02,\n",
      "       -5.44204786e-02, -2.07094438e-02, -2.65138759e-03,  2.73685344e-02,\n",
      "       -3.07401903e-02,  5.39640598e-02, -5.80019243e-02, -4.28977981e-02,\n",
      "        1.10815592e-01, -7.68478587e-03, -5.33225015e-03,  5.49635254e-02],\n",
      "      dtype=float32), array([-4.09566015e-02, -1.03954844e-01,  8.48398730e-02,  7.05011114e-02,\n",
      "        3.25393789e-02,  9.00207926e-03,  2.45929435e-02,  8.87814239e-02,\n",
      "        6.97642341e-02, -4.33057472e-02, -5.06850006e-03,  3.46736014e-02,\n",
      "        6.89995736e-02,  1.52824707e-02,  1.11383103e-01, -3.97901423e-02,\n",
      "       -1.05561921e-02, -2.60800086e-02,  5.61919659e-02,  5.80820721e-03,\n",
      "       -8.37504566e-02, -6.90403357e-02,  6.85841963e-02,  6.54803682e-03,\n",
      "        5.02882376e-02, -8.99724662e-03, -1.38774263e-02, -9.37270373e-02,\n",
      "       -9.89373028e-02,  3.45833064e-03, -6.91147298e-02, -1.10565610e-01,\n",
      "        2.52028294e-02,  1.99191105e-02, -1.68981627e-02, -8.11823923e-03,\n",
      "        7.01338649e-02,  1.30814351e-02, -1.13046326e-01, -3.20556238e-02,\n",
      "       -2.17231866e-02,  2.81595849e-02, -9.13382918e-02, -6.58184215e-02,\n",
      "        1.01955775e-02,  2.26646364e-02, -1.29390303e-02, -2.98579689e-02,\n",
      "       -3.64858508e-02, -9.25524533e-02,  1.29169980e-02,  2.93325763e-02,\n",
      "        9.11961868e-02, -5.38576990e-02, -2.64377886e-04,  6.95226295e-03,\n",
      "        3.76407765e-02,  7.66539723e-02,  8.08360279e-02,  2.33010352e-02,\n",
      "       -6.11078031e-02,  8.49181693e-03,  4.49239053e-02,  2.00847555e-02,\n",
      "        7.32278079e-02,  6.32423535e-02,  1.87278800e-02,  9.54781100e-02,\n",
      "       -1.13251269e-01,  9.09209717e-03,  3.76076587e-02,  2.61082705e-02,\n",
      "        6.11683987e-02,  1.94371846e-02, -6.04110323e-02,  4.22401913e-02,\n",
      "        2.48894934e-02,  6.62686080e-02,  1.68645475e-02,  1.75567358e-04,\n",
      "       -4.82641309e-02, -4.15097512e-02,  5.23989461e-02,  7.72108287e-02,\n",
      "        4.30595092e-02, -1.77639965e-02, -5.83501011e-02,  2.87591498e-02,\n",
      "       -6.67819241e-03, -7.48152807e-02, -5.17281219e-02,  3.97930630e-02,\n",
      "       -5.95096871e-02, -5.80362603e-02,  6.27484024e-02,  1.37804653e-02,\n",
      "       -4.52923477e-02,  1.01065591e-01, -1.97737496e-02, -3.96040939e-02,\n",
      "        5.10045439e-02, -8.07934627e-03,  2.77125929e-03, -4.97696316e-03,\n",
      "        1.33737540e-02, -3.45988646e-02,  3.87510695e-02, -4.17496264e-02,\n",
      "       -2.95338109e-02, -1.34666357e-02, -1.77181792e-02,  3.73465903e-02,\n",
      "       -6.73296005e-02, -9.47240144e-02, -8.72898027e-02, -8.28576386e-02,\n",
      "        5.16039208e-02,  1.16171902e-02,  6.64910907e-03,  4.18908894e-02,\n",
      "       -6.47701323e-02, -1.19838700e-01, -7.49276532e-03, -1.22003276e-02,\n",
      "        4.64267880e-02,  7.63605833e-02,  2.05760542e-02, -5.79339862e-02,\n",
      "        1.57554876e-02,  4.09134217e-02,  5.17859347e-02,  8.06773826e-03,\n",
      "        4.73407507e-02, -4.83775064e-02,  1.01666164e-03, -1.10500395e-01,\n",
      "        1.04829418e-02, -2.49949694e-02,  8.91393572e-02,  1.00068770e-01,\n",
      "        9.35235620e-02,  1.13673113e-01,  2.43098605e-02, -5.73680922e-02,\n",
      "       -9.84455571e-02,  1.14932612e-01, -3.48393880e-02, -2.17371117e-02,\n",
      "       -5.51348589e-02, -2.01665964e-02,  3.50428186e-02,  4.65764292e-02,\n",
      "        9.88148153e-02, -5.85787669e-02, -2.80280970e-02, -6.62159594e-03,\n",
      "       -5.87906875e-02, -4.98267896e-02, -1.56902537e-01, -6.61077024e-03,\n",
      "       -5.31833339e-03, -6.57078251e-02, -1.27530331e-02, -5.47813289e-02,\n",
      "       -2.01040097e-02,  1.81850605e-02,  8.58416483e-02, -7.24910870e-02,\n",
      "       -1.24463268e-01,  1.74956974e-02, -1.84624810e-02, -1.33693025e-01,\n",
      "       -3.21263932e-02, -6.65101185e-02,  1.34534771e-02,  2.29557455e-02,\n",
      "        1.52774407e-02,  4.52676043e-02, -3.43597122e-02,  2.25557741e-02,\n",
      "       -1.27398357e-01,  4.94421124e-02,  7.51757342e-03,  3.08125131e-02,\n",
      "        1.50519475e-01,  1.79039799e-02,  1.21117551e-02,  3.29410583e-02,\n",
      "        3.80107090e-02,  9.81533825e-02,  6.62494972e-02,  1.05218105e-01,\n",
      "       -1.17855994e-02, -1.71074979e-02,  3.95084582e-02,  3.29986922e-02,\n",
      "        1.21901907e-01, -6.61918744e-02, -6.78690895e-02,  2.86720321e-02,\n",
      "        9.07728523e-02,  7.29446486e-02,  1.01470044e-02, -5.47661707e-02,\n",
      "       -3.09721231e-02,  3.19702104e-02, -2.23585814e-02,  2.05501970e-02,\n",
      "       -9.36517641e-02,  3.89571977e-03,  6.43647015e-02, -1.50609622e-02,\n",
      "        5.25735766e-02,  1.05561130e-02,  1.34185059e-02,  3.51918396e-03,\n",
      "        3.95499878e-02, -1.47899603e-02,  1.01339266e-01,  1.93441436e-02,\n",
      "       -2.50978749e-02, -8.35912898e-02, -5.83535731e-02, -8.27981967e-06,\n",
      "       -2.99644135e-02,  7.45135546e-02, -1.98270474e-02, -2.72093974e-02,\n",
      "       -6.80835545e-02, -7.30001703e-02, -2.51696184e-02,  1.18566537e-02,\n",
      "        1.96338221e-02, -2.82781990e-03, -6.98758755e-03, -1.32800993e-02,\n",
      "       -6.92682283e-04, -8.60854611e-03, -3.50115485e-02,  7.16126710e-02,\n",
      "        2.99772192e-02, -5.42191304e-02, -4.05526422e-02,  1.18639961e-01,\n",
      "        3.73045802e-02, -4.00153026e-02, -1.45505413e-01, -2.91601960e-02,\n",
      "       -6.96045067e-03, -9.38693210e-02, -1.80133041e-02,  2.82320417e-02,\n",
      "       -9.63249877e-02, -1.20695986e-01,  8.75889603e-03, -1.58471763e-02,\n",
      "        3.67860682e-02, -1.64702777e-02, -3.01165450e-02, -7.68222585e-02,\n",
      "        2.93491483e-02, -3.88257802e-02, -1.08090984e-02,  2.74176672e-02,\n",
      "        1.43370435e-01, -4.86788601e-02, -8.58846121e-03,  1.99550577e-02,\n",
      "        3.37875746e-02,  2.62338426e-02, -3.15784737e-02, -2.01917859e-03,\n",
      "        5.13615906e-02, -9.80357900e-02, -1.50423180e-02,  8.68443251e-02,\n",
      "        1.43296272e-01, -1.70700941e-02,  7.26225674e-02, -4.18596789e-02,\n",
      "        4.85574193e-02, -4.34794389e-02,  2.89643612e-02,  7.31185004e-02,\n",
      "       -3.46393064e-02,  4.54308763e-02,  5.72230257e-02, -2.04850659e-02,\n",
      "       -8.54291692e-02,  1.79453229e-03,  1.21788621e-01,  5.05151004e-02,\n",
      "       -2.22229082e-02, -6.51464090e-02,  5.57091534e-02,  7.97976181e-02,\n",
      "       -7.21668303e-02, -7.74733946e-02,  2.63534170e-02,  5.22009060e-02],\n",
      "      dtype=float32), array([ 3.41440961e-02,  9.11432058e-02,  1.03889786e-01,  6.04129806e-02,\n",
      "        6.86829956e-03,  7.51583874e-02, -1.09777264e-02, -3.97986211e-02,\n",
      "       -7.68079236e-02, -9.17774905e-03,  2.52796076e-02,  3.42965033e-03,\n",
      "       -1.32629694e-02, -4.90812361e-02, -1.72873680e-02, -7.05451742e-02,\n",
      "       -4.08572145e-03, -2.46222522e-02,  5.69400657e-03, -5.92105985e-02,\n",
      "        6.47573024e-02, -8.04266557e-02,  1.31526561e-02,  9.19842944e-02,\n",
      "       -2.19667070e-02,  9.69863161e-02,  6.95497394e-02,  4.10561860e-02,\n",
      "       -8.55769515e-02,  5.45839965e-02, -1.13117680e-01, -7.27724880e-02,\n",
      "        8.25665984e-03,  2.02559046e-02,  6.26112968e-02,  2.03581844e-02,\n",
      "        2.84988601e-02, -1.00668178e-04,  8.67734104e-02,  1.05384506e-01,\n",
      "       -5.12692779e-02,  6.63746446e-02, -3.16910371e-02,  1.05833123e-02,\n",
      "       -4.40338552e-02, -3.36884409e-02, -6.60418198e-02,  1.22975046e-02,\n",
      "       -3.78579311e-02, -3.32630314e-02, -1.55709730e-02,  7.17058182e-02,\n",
      "        9.58771557e-02, -8.35905299e-02, -6.83614463e-02, -1.96716674e-02,\n",
      "       -1.89818423e-02,  2.79790908e-02,  1.56759769e-02, -1.86641067e-02,\n",
      "        8.89384970e-02, -6.22787979e-03,  1.02059633e-01, -2.93545029e-03,\n",
      "        3.64601216e-03,  7.44009390e-02, -1.81084760e-02,  1.61969792e-02,\n",
      "        3.71940657e-02, -3.95072140e-02,  6.43435568e-02, -3.32121924e-02,\n",
      "       -2.35990509e-02, -1.46234864e-02, -1.79730728e-01,  5.30038439e-02,\n",
      "        2.17609741e-02,  1.92372054e-02, -1.32612027e-02,  1.21781859e-03,\n",
      "       -1.73477400e-02,  9.61617753e-03,  1.63081974e-01, -7.71874338e-02,\n",
      "        3.10220476e-02, -4.98476950e-03, -1.07870609e-01, -3.79946455e-02,\n",
      "       -5.17786629e-02, -5.65197058e-02,  1.12288389e-02, -2.04011053e-02,\n",
      "       -7.19863772e-02, -5.53089827e-02, -3.20194364e-02,  8.39318633e-02,\n",
      "       -7.31792375e-02,  2.94635841e-03,  2.56906170e-03, -9.27672386e-02,\n",
      "       -5.72874723e-03,  1.82715114e-02, -4.72070947e-02,  8.21913630e-02,\n",
      "       -1.02814265e-01, -4.43561524e-02, -2.44329292e-02, -3.35546434e-02,\n",
      "        4.79368940e-02,  9.78830233e-02, -6.07527345e-02,  8.75292346e-03,\n",
      "       -5.07000610e-02, -4.87314537e-02, -8.20851997e-02, -1.92592740e-02,\n",
      "       -8.30780938e-02,  3.31603922e-02,  1.32437218e-02,  7.78395161e-02,\n",
      "       -3.75225991e-02,  1.95154045e-02, -1.17143989e-02,  6.54604435e-02,\n",
      "        5.85523853e-03,  6.70419447e-03,  3.71208340e-02, -8.04897770e-02,\n",
      "        4.66886349e-02,  5.21179885e-02, -3.34847867e-02,  7.70473853e-02,\n",
      "       -1.12423459e-02,  6.20818436e-02,  1.26565741e-02,  5.19584976e-02,\n",
      "        1.04127839e-01,  7.33028874e-02, -1.15530826e-02,  7.59178028e-02,\n",
      "        4.46996614e-02,  1.70777049e-02,  2.98029464e-03, -5.58345057e-02,\n",
      "        4.48338203e-02, -3.31256762e-02, -2.18107440e-02, -4.63174731e-02,\n",
      "        5.45546748e-02, -8.73947591e-02, -7.87835568e-03,  1.40311616e-02,\n",
      "        7.17637986e-02, -6.19381554e-02, -2.73539331e-02,  1.55983150e-01,\n",
      "        2.94890385e-02,  1.02514829e-02, -9.02635697e-03,  5.59816249e-02,\n",
      "       -4.94266562e-02,  5.98260667e-03, -9.91912037e-02, -8.80282894e-02,\n",
      "        4.99709062e-02,  9.91194509e-04,  2.33460367e-02, -1.28273055e-01,\n",
      "       -1.10325873e-01,  7.37767480e-03, -5.15611321e-02,  2.09742300e-02,\n",
      "       -5.96475489e-02, -2.60180552e-02, -2.69719884e-02, -1.16446547e-01,\n",
      "       -3.48573700e-02,  3.93054634e-02, -6.04626089e-02, -6.56518945e-03,\n",
      "        1.00817587e-02, -4.20311242e-02,  5.86569160e-02, -2.61078756e-02,\n",
      "        9.84955803e-02,  2.34263260e-02, -2.09059134e-01,  6.94629326e-02,\n",
      "        9.73916873e-02,  1.06154487e-01, -5.14981635e-02,  1.06430799e-01,\n",
      "        1.99732184e-02, -2.63310485e-02, -1.43689826e-01,  7.33209401e-02,\n",
      "        1.28133252e-01, -1.26823727e-02, -8.35534930e-02,  3.29333693e-02,\n",
      "       -2.12144759e-02,  5.70167713e-02,  5.94585277e-02, -1.18710235e-01,\n",
      "        2.07379051e-02,  2.21985183e-03,  4.65023592e-02, -1.43545633e-02,\n",
      "       -2.03530528e-02,  4.20694910e-02,  6.77046627e-02, -6.30193427e-02,\n",
      "        3.38931344e-02,  1.48454592e-01,  4.54724506e-02, -1.38688963e-02,\n",
      "       -8.60197470e-02, -3.58255841e-02,  9.92415696e-02,  7.13335052e-02,\n",
      "       -2.78122332e-02,  3.84750701e-02, -7.79822245e-02,  7.62688145e-02,\n",
      "       -4.57303450e-02,  1.26213208e-01, -9.42809135e-02, -1.82810836e-02,\n",
      "        1.95507761e-02, -2.27963440e-02,  1.56290885e-02, -5.89652210e-02,\n",
      "       -3.76445279e-02,  9.56877787e-03,  3.10470797e-02, -4.72355150e-02,\n",
      "        7.80364871e-02, -3.91012728e-02,  1.70564242e-02, -4.43054959e-02,\n",
      "       -3.82844284e-02, -7.64268870e-03,  1.16888627e-01, -2.20918451e-02,\n",
      "        2.61649191e-02, -1.05683953e-01, -1.17928488e-02, -1.82441175e-02,\n",
      "       -1.13752754e-02,  3.96285877e-02,  2.24364512e-02,  6.14953302e-02,\n",
      "       -7.95856770e-03, -2.23060623e-02, -1.87916104e-02,  4.84005846e-02,\n",
      "       -1.32617261e-02,  1.84537396e-02, -1.19349398e-02, -4.63391915e-02,\n",
      "        1.52585004e-03,  2.71940026e-02, -5.81737757e-02,  1.99022405e-02,\n",
      "        4.87585068e-02, -8.24615359e-02, -5.44381142e-02,  4.27451357e-02,\n",
      "        5.22967204e-02,  6.64693341e-02, -8.70356243e-03,  2.99199410e-02,\n",
      "        4.93141450e-02, -2.33798753e-02,  8.15781113e-03,  3.84675302e-02,\n",
      "        1.95232872e-02, -1.66055653e-02, -5.01793809e-02,  1.28656896e-02,\n",
      "        1.50619065e-02, -1.15191683e-01, -3.67809869e-02, -3.46973166e-02,\n",
      "        8.39576498e-02, -3.03770173e-02, -1.61749292e-02,  1.00691272e-02,\n",
      "       -1.44524768e-03,  9.13834106e-03, -2.88064349e-02,  1.17618237e-02,\n",
      "        4.94254492e-02,  1.84462536e-02, -1.68723129e-02, -6.20201556e-03,\n",
      "        1.77062619e-02,  1.54611422e-02,  3.50605734e-02, -1.73949916e-02],\n",
      "      dtype=float32), array([ 3.42593230e-02, -2.93631945e-02, -5.10789528e-02, -4.73260023e-02,\n",
      "        6.98169544e-02,  5.81612960e-02,  8.76889899e-02,  6.50281236e-02,\n",
      "        6.95900023e-02,  1.18660508e-02,  6.64459690e-02, -5.31817712e-02,\n",
      "        4.68295161e-03,  2.50001587e-02,  1.05003677e-01,  3.41471285e-02,\n",
      "        2.26988178e-02,  4.48725969e-02,  2.50262246e-02, -1.19923437e-02,\n",
      "        8.57637525e-02, -8.87589157e-02,  4.14559692e-02,  8.96080509e-02,\n",
      "        2.38687787e-02,  2.35073431e-03, -6.50453344e-02, -1.17648197e-02,\n",
      "       -2.65708473e-02, -6.70698211e-02, -1.52269313e-02, -2.20288541e-02,\n",
      "        5.97235970e-02,  3.41302082e-02, -4.43417765e-02, -1.53114833e-02,\n",
      "        3.50280665e-02,  1.79822762e-02, -7.92423934e-02,  8.24345462e-03,\n",
      "        3.99656184e-02, -2.08538398e-02,  1.43830821e-01,  1.75829809e-02,\n",
      "       -3.16951759e-02, -1.22853154e-02,  1.50457406e-02, -2.30444651e-02,\n",
      "        8.33742023e-02,  1.32548407e-01,  5.87033182e-02,  3.30462344e-02,\n",
      "        6.07456267e-03, -6.37267083e-02, -6.89107105e-02,  3.95448543e-02,\n",
      "        1.40497480e-02,  2.31114570e-02, -6.77159280e-02, -5.15952297e-02,\n",
      "        1.77200686e-03,  2.79338891e-03, -8.20141882e-02,  1.10494979e-02,\n",
      "        3.24926004e-02, -6.47833422e-02, -5.94024211e-02, -1.48213552e-02,\n",
      "       -4.34022546e-02, -2.70564537e-02, -6.26021177e-02, -2.78271530e-02,\n",
      "       -1.42145222e-02,  5.24594076e-03,  9.61992145e-03,  1.34413794e-03,\n",
      "       -7.46932775e-02, -3.05843609e-03,  7.22167417e-02,  4.75861039e-03,\n",
      "       -2.64264829e-03, -1.03408225e-01,  5.63558154e-02, -6.89985454e-02,\n",
      "       -1.61634177e-01, -9.14997384e-02,  5.39514758e-02,  1.49986260e-02,\n",
      "       -2.27846820e-02, -6.17832430e-02,  1.19676605e-01, -3.19745131e-02,\n",
      "       -7.40122050e-02, -7.40905479e-02, -1.08910464e-01,  5.82321212e-02,\n",
      "        7.70188794e-02,  1.04094506e-03, -5.40528297e-02,  6.41140016e-03,\n",
      "        9.07598361e-02,  1.10558875e-01, -4.96332273e-02,  8.76576733e-03,\n",
      "       -2.00743061e-02, -2.79978290e-02,  3.58577669e-02,  5.54076582e-03,\n",
      "       -4.71057696e-03, -6.33883872e-04, -3.06371395e-02, -3.64430659e-02,\n",
      "        3.02107558e-02,  5.10410704e-02, -1.78267527e-02, -1.02073379e-01,\n",
      "        5.16425446e-02, -3.59203219e-02,  1.27095610e-01, -7.96642154e-02,\n",
      "        1.76596716e-02, -6.60200976e-03, -7.50152841e-02, -4.71420996e-02,\n",
      "        2.18691994e-02, -7.70732900e-03,  6.63639978e-02,  6.62455708e-02,\n",
      "        5.90310059e-03,  5.76215610e-02,  2.31583565e-02,  9.30299889e-03,\n",
      "       -4.29331930e-03,  7.62927979e-02,  2.12543597e-03,  2.45003346e-02,\n",
      "       -5.95472232e-02,  1.97712635e-03,  4.88081984e-02,  4.55130823e-02,\n",
      "        5.05962782e-02, -2.99180243e-02, -3.44885439e-02, -1.87884066e-02,\n",
      "       -1.19125657e-01, -9.88712609e-02,  3.68325934e-02, -3.98295559e-02,\n",
      "        8.84285942e-02, -2.25788392e-02,  5.16732410e-02, -5.16711660e-02,\n",
      "       -1.98141783e-01, -5.33103310e-02,  7.11297169e-02, -3.91214490e-02,\n",
      "        1.49968276e-02, -1.53056592e-01,  2.41828747e-02, -3.66337374e-02,\n",
      "        8.68745521e-02,  2.14542635e-02, -5.41464463e-02, -5.55945153e-05,\n",
      "        6.53980598e-02, -1.88694727e-02, -5.47514856e-02, -6.72851205e-02,\n",
      "        8.53332579e-02, -1.85251478e-02, -8.62060860e-02, -1.61366668e-02,\n",
      "        1.78231373e-02, -2.63312273e-02, -1.18875362e-01,  1.78031344e-02,\n",
      "       -1.05598442e-01,  5.68195991e-02, -5.47693782e-02, -2.86252573e-02,\n",
      "        8.20308998e-02, -8.28419253e-02,  5.00114784e-02, -8.05142894e-03,\n",
      "        6.80573955e-02,  4.93930234e-03,  1.76065806e-02, -1.28657103e-01,\n",
      "       -1.46164410e-02, -9.04742256e-03, -5.44268079e-03, -1.04408145e-01,\n",
      "       -4.78535816e-02, -1.44179419e-01, -5.95491044e-02, -4.53170389e-02,\n",
      "        2.20061168e-02, -5.22465585e-03, -2.31367182e-02,  8.22718348e-03,\n",
      "        2.68373229e-02,  1.20887614e-03,  9.09648985e-02, -6.80260286e-02,\n",
      "       -2.64251977e-02, -1.62414685e-02, -1.22968275e-02,  1.50311098e-01,\n",
      "        1.06685169e-01, -7.02651823e-03,  1.40483230e-02, -5.06085828e-02,\n",
      "        3.73183824e-02,  3.99921425e-02, -2.03427747e-02,  7.93080661e-04,\n",
      "        7.73732141e-02, -7.31794015e-02, -3.50165963e-02,  1.77293673e-01,\n",
      "        4.99768108e-02, -2.41592340e-02,  7.60735497e-02,  3.16412412e-02,\n",
      "        5.14595620e-02,  3.40176933e-02,  1.78565271e-02,  4.81096618e-02,\n",
      "        1.43423853e-02,  2.53046025e-02, -2.00159959e-02, -4.15524803e-02,\n",
      "        1.14060380e-01,  1.71368290e-02,  1.07220508e-01, -2.54249312e-02,\n",
      "        5.35225235e-02,  2.00247904e-03, -2.64020059e-02, -4.45593558e-02,\n",
      "        4.36188206e-02,  3.40430513e-02, -8.33816268e-03,  3.66802067e-02,\n",
      "        6.69604819e-03,  2.54448056e-02,  4.68039587e-02,  6.69371290e-03,\n",
      "        8.58456921e-03,  6.68732524e-02,  1.22930035e-02, -4.90160771e-02,\n",
      "        3.31319645e-02,  5.90534285e-02,  9.51285567e-03, -6.44514412e-02,\n",
      "       -6.04087114e-02, -1.90500375e-02, -5.31615950e-02,  1.37195989e-01,\n",
      "        1.05098560e-01, -2.49722879e-02,  4.05603461e-03,  3.00134420e-02,\n",
      "       -9.12922397e-02, -5.50414771e-02, -6.26635328e-02, -4.56452668e-02,\n",
      "       -1.71932373e-02, -3.58915403e-02,  5.69956824e-02, -6.00368083e-02,\n",
      "       -4.16327454e-02,  4.20684032e-02,  3.69511582e-02, -4.84347865e-02,\n",
      "        1.73815843e-02,  4.95536439e-02, -3.77978683e-02,  3.68757509e-02,\n",
      "        2.89575886e-02, -3.28239389e-02,  1.82109866e-02, -4.11491841e-02,\n",
      "        1.11660130e-01, -2.48940717e-02, -1.36059225e-02,  1.04992781e-02,\n",
      "        4.89790291e-02,  9.37776465e-04, -1.32994801e-01, -1.47139654e-03,\n",
      "        1.75102632e-02, -2.08141822e-02, -6.50993511e-02,  3.37708704e-02,\n",
      "        3.29985358e-02,  4.43482436e-02,  7.07995966e-02, -9.11519676e-03],\n",
      "      dtype=float32), array([-0.00884925,  0.01826358,  0.07391366,  0.11156622, -0.01105812,\n",
      "        0.04181591, -0.08267279, -0.05144818,  0.00605555,  0.02502412,\n",
      "       -0.02404184, -0.00623045,  0.00718525, -0.03924875,  0.06500801,\n",
      "        0.08121299,  0.03320098, -0.12662889,  0.03659607, -0.11269601,\n",
      "       -0.01173142, -0.00447365,  0.04847997,  0.04772   ,  0.05104848,\n",
      "       -0.03675204, -0.01373998, -0.1504966 ,  0.01535953, -0.05015828,\n",
      "       -0.09421209, -0.01072974,  0.05035555,  0.01200517, -0.02093322,\n",
      "       -0.02282234,  0.02985631,  0.07122941, -0.06023526,  0.06732041,\n",
      "       -0.01971386,  0.00438996, -0.0144063 , -0.01170166,  0.07602021,\n",
      "        0.09972502,  0.04202057, -0.08520595, -0.03169004, -0.01171203,\n",
      "       -0.06108218, -0.02013254,  0.10921788, -0.06581493, -0.08006314,\n",
      "       -0.03982757, -0.08596758,  0.01239194, -0.08016212, -0.01874462,\n",
      "       -0.03941502,  0.01004256,  0.07282098, -0.01961978,  0.08076449,\n",
      "        0.02221882, -0.06509624, -0.03502942, -0.14100927,  0.03599355,\n",
      "        0.01637018, -0.00301442, -0.08895178,  0.00483404, -0.03873879,\n",
      "       -0.00829374, -0.03410005, -0.01368506, -0.03748164,  0.06022029,\n",
      "        0.03498948, -0.04716588, -0.07686356,  0.05755838, -0.0182454 ,\n",
      "       -0.04053152, -0.07787794, -0.13202605,  0.07466162, -0.10475334,\n",
      "       -0.04486985, -0.0254293 ,  0.0015261 , -0.07694685,  0.06451064,\n",
      "        0.08211369,  0.03169856,  0.08589811,  0.07833682, -0.00293701,\n",
      "        0.03695002, -0.01399293,  0.0629502 , -0.04552465, -0.02326274,\n",
      "        0.04251668, -0.03866804,  0.00561339,  0.04723575, -0.06907593,\n",
      "       -0.04894812,  0.02226126, -0.03247652, -0.07092369, -0.00761689,\n",
      "        0.09511366, -0.0681882 ,  0.00037365, -0.02507125, -0.00794155,\n",
      "       -0.03723539,  0.01513545,  0.00223499, -0.01455473, -0.04487351,\n",
      "        0.03899472, -0.01898397, -0.0082406 , -0.00756356,  0.01552779,\n",
      "        0.07373746,  0.0308365 , -0.0512881 ,  0.03441687,  0.02024227,\n",
      "       -0.03477423,  0.00043655, -0.05307219,  0.01929811, -0.07681216,\n",
      "       -0.02993597,  0.02399787,  0.06718529, -0.05267536,  0.1081531 ,\n",
      "        0.01724147, -0.05926202, -0.05226028,  0.07310385, -0.06649844,\n",
      "        0.06294949, -0.10703577,  0.02504389, -0.09995712, -0.07174196,\n",
      "        0.11455704, -0.03796821, -0.04683346,  0.02487133, -0.01208542,\n",
      "        0.02925506,  0.07558446, -0.07200317, -0.0836124 ,  0.09466425,\n",
      "       -0.03403961,  0.0161133 , -0.03554942, -0.06109013,  0.00470495,\n",
      "        0.04303269, -0.04592181, -0.02278531, -0.07572841,  0.00060601,\n",
      "       -0.00673985, -0.05195507, -0.02239293, -0.00425976, -0.05592455,\n",
      "       -0.09124995,  0.10112596, -0.02906555, -0.04240173,  0.00317884,\n",
      "        0.03096148, -0.09085635,  0.02549519,  0.05683354,  0.00452943,\n",
      "        0.01938207,  0.1201589 ,  0.03786395, -0.02623683, -0.08043256,\n",
      "        0.06741793, -0.01621961, -0.03191611, -0.05453568,  0.07613498,\n",
      "        0.07687286,  0.01639879, -0.00959005,  0.05816635,  0.05439517,\n",
      "       -0.09369289, -0.07168939, -0.09599791, -0.00360497,  0.05905376,\n",
      "        0.04586736, -0.11678743,  0.04342688,  0.04627948, -0.08070116,\n",
      "       -0.00722126,  0.10672012, -0.00941736,  0.03478171,  0.10289555,\n",
      "        0.06842097, -0.07300042, -0.05183674, -0.00993474,  0.00207684,\n",
      "       -0.01242656, -0.08583651, -0.04337779, -0.12003588, -0.00206381,\n",
      "        0.01711177,  0.04175824, -0.07438362,  0.02604884,  0.03846895,\n",
      "       -0.0248899 ,  0.00274387, -0.04176622,  0.01819862, -0.02000717,\n",
      "       -0.03791962,  0.02675457,  0.01227469, -0.03040954,  0.0507558 ,\n",
      "       -0.09190986, -0.08106308, -0.00273476, -0.03391359, -0.06688888,\n",
      "       -0.08127305,  0.05957929, -0.04458148,  0.04527041,  0.11023275,\n",
      "       -0.1001117 , -0.02322447, -0.07088468,  0.06766829, -0.13606918,\n",
      "        0.01492588,  0.08339287, -0.12239044,  0.01319051,  0.01208873,\n",
      "        0.08337095,  0.0325647 , -0.04531398,  0.00518819, -0.02901939,\n",
      "        0.00725875,  0.03657642,  0.00339321, -0.08976656,  0.02002457,\n",
      "        0.0244428 , -0.00420531, -0.04585719,  0.04575922,  0.00157641,\n",
      "        0.08494589, -0.05312832,  0.07240282, -0.03101583,  0.03203312,\n",
      "        0.01688873,  0.10047866,  0.00254734, -0.10153009, -0.02782102,\n",
      "        0.03345088, -0.02125914, -0.0334749 , -0.14420548, -0.08016314,\n",
      "        0.10142508, -0.06924404,  0.07708407,  0.02687018, -0.05853768],\n",
      "      dtype=float32), array([ 0.00153911, -0.08118005,  0.08195975, -0.0328974 , -0.06097488,\n",
      "        0.09011725, -0.0586205 , -0.05719988,  0.04003064,  0.02981257,\n",
      "        0.03899329, -0.0555842 ,  0.0496329 ,  0.03672478,  0.02917311,\n",
      "        0.00120147,  0.00079645, -0.04002262,  0.11548071, -0.02860364,\n",
      "        0.03072534, -0.03453031,  0.0450512 ,  0.03079896, -0.05671341,\n",
      "        0.04451223, -0.02662441, -0.09347015, -0.04794478, -0.08516262,\n",
      "        0.02963366,  0.04113468,  0.0170688 ,  0.05398661, -0.06977857,\n",
      "       -0.06433402,  0.05692073,  0.06397142,  0.03908594, -0.00883533,\n",
      "        0.08392233, -0.14729486, -0.01262104, -0.06093379,  0.00499895,\n",
      "        0.09123971,  0.06722654, -0.07340781,  0.01001314, -0.10409105,\n",
      "        0.00680238,  0.01129157,  0.04710882,  0.00552564, -0.06645013,\n",
      "       -0.04705549,  0.03530018,  0.06096371,  0.0275649 ,  0.06567026,\n",
      "       -0.04126956, -0.02469224,  0.05733392,  0.00460359,  0.00133736,\n",
      "       -0.03596366, -0.01481187, -0.00927324,  0.04522981,  0.04894612,\n",
      "        0.05536579, -0.10716119, -0.1328733 ,  0.06697946, -0.04127567,\n",
      "        0.11722215, -0.12210927, -0.01854997, -0.0650549 ,  0.08125662,\n",
      "       -0.04811638, -0.02320324, -0.02480916,  0.07238759,  0.01651094,\n",
      "       -0.03823366,  0.0667957 , -0.03981263, -0.03368654, -0.03595344,\n",
      "        0.0713377 , -0.02392942,  0.04419864,  0.0313903 , -0.03604148,\n",
      "        0.0615808 , -0.02440276,  0.06180584, -0.09068851,  0.02684898,\n",
      "        0.05448942,  0.01031064, -0.10274263, -0.07095912,  0.0724668 ,\n",
      "        0.046052  ,  0.06540902, -0.0371016 ,  0.0501033 ,  0.02598718,\n",
      "       -0.08787504, -0.0654044 ,  0.00415596,  0.01772584, -0.0702199 ,\n",
      "        0.05078619, -0.03194202,  0.08257783, -0.06041501,  0.04431994,\n",
      "        0.01843182, -0.11552165, -0.07219025, -0.03453178,  0.03298873,\n",
      "        0.0030413 ,  0.02279275,  0.04470708, -0.03052962, -0.03388751,\n",
      "        0.00743859, -0.02318465,  0.02187029,  0.0270138 , -0.01140746,\n",
      "       -0.00846714,  0.00716035, -0.0526246 , -0.03482816,  0.04813049,\n",
      "        0.09170622,  0.00710698, -0.08663366, -0.01281988, -0.02820173,\n",
      "        0.10187989, -0.04415073,  0.04445146, -0.02937344, -0.0199482 ,\n",
      "       -0.01184588, -0.02519026, -0.00718402, -0.05614402, -0.08313542,\n",
      "        0.06789767,  0.08444183, -0.00318545, -0.10705163, -0.04479191,\n",
      "       -0.06383423,  0.0281061 , -0.04718296, -0.04233422,  0.0702297 ,\n",
      "       -0.02895365,  0.08766624, -0.1801032 , -0.03138664,  0.14746976,\n",
      "       -0.05207776, -0.12216201, -0.00686356,  0.00060567,  0.10326044,\n",
      "        0.02616256,  0.04551321, -0.00654071, -0.01459925, -0.03667397,\n",
      "       -0.00908485,  0.02699557,  0.02789519,  0.04248323, -0.04199328,\n",
      "       -0.09406161, -0.1429203 , -0.04694108,  0.03360061, -0.06920313,\n",
      "       -0.04895161,  0.16388018,  0.05490297, -0.00534564,  0.10626984,\n",
      "        0.02473987,  0.04476643,  0.05342951, -0.19763443,  0.02021129,\n",
      "       -0.04697086,  0.00495273,  0.00118724, -0.0137554 ,  0.0608693 ,\n",
      "        0.04446543,  0.04631687, -0.08276563,  0.04028971,  0.05503328,\n",
      "        0.06107379, -0.04550124,  0.01483073, -0.02989339,  0.09624581,\n",
      "        0.02311517,  0.04973911,  0.00715414, -0.00889304,  0.03675613,\n",
      "        0.08052849,  0.07066618, -0.00155362, -0.03915097,  0.03612505,\n",
      "       -0.03127353, -0.04651407, -0.02291128, -0.00909352, -0.09990509,\n",
      "       -0.0346033 ,  0.0223465 ,  0.05004253, -0.120993  ,  0.13099672,\n",
      "        0.05458462, -0.02283363, -0.08291608, -0.01844645, -0.01758665,\n",
      "        0.01988097, -0.03541671, -0.02192286,  0.00293759, -0.01519931,\n",
      "       -0.02118257, -0.07940301, -0.00609701, -0.01181239,  0.04025567,\n",
      "       -0.05935382,  0.12500994, -0.08666337,  0.00356516,  0.07405923,\n",
      "       -0.02177314, -0.00198702, -0.03008729, -0.02687824, -0.01834774,\n",
      "       -0.05271063,  0.0194512 , -0.02285047, -0.04480811,  0.02921859,\n",
      "        0.03584522, -0.00207934, -0.00437741,  0.02971244, -0.0216024 ,\n",
      "        0.05891917, -0.00298187,  0.00453831,  0.00521979, -0.02136308,\n",
      "        0.03685342,  0.00358917,  0.11390141,  0.04740651, -0.06805062,\n",
      "        0.05730706, -0.01389055,  0.08886611,  0.10150432, -0.03987204,\n",
      "       -0.02809366, -0.07596828, -0.00586303, -0.03548037, -0.03715304,\n",
      "        0.01280806,  0.04569451,  0.01605914, -0.04437071, -0.07460143,\n",
      "        0.07122403, -0.01642791, -0.01348469,  0.04896307,  0.09832016],\n",
      "      dtype=float32), array([-0.02626166, -0.05800952,  0.01779714,  0.09139745,  0.00131704,\n",
      "       -0.02830234,  0.00406731,  0.01554729, -0.02606265, -0.01960678,\n",
      "        0.0363156 , -0.02850153,  0.0294503 , -0.08276918,  0.0972117 ,\n",
      "       -0.03745214, -0.02882263, -0.04439246,  0.00304974,  0.05570782,\n",
      "        0.00362789, -0.0234314 , -0.01146992,  0.03893267,  0.04890573,\n",
      "        0.09959892,  0.07989369, -0.06366017, -0.01429027, -0.04312964,\n",
      "       -0.02780519, -0.08741183, -0.00787887,  0.07816159, -0.01474545,\n",
      "        0.06865329,  0.10306777, -0.1102848 , -0.07749379,  0.04603681,\n",
      "       -0.10218524, -0.0100805 , -0.00829487,  0.02219211,  0.02568235,\n",
      "        0.0638861 , -0.03127345, -0.06426333, -0.06158599, -0.08088476,\n",
      "        0.08379436,  0.03765758,  0.07783605, -0.07931625, -0.01482888,\n",
      "       -0.05575119,  0.0243263 ,  0.0825707 ,  0.0697734 ,  0.03008126,\n",
      "        0.00965852, -0.00341621, -0.02858108,  0.0009345 ,  0.0571728 ,\n",
      "        0.09926471,  0.02532328, -0.03790317, -0.01994047, -0.0932777 ,\n",
      "       -0.03395311, -0.00555506, -0.0341621 ,  0.06286987, -0.06666986,\n",
      "        0.00284927, -0.01483306, -0.00634093,  0.00246485,  0.00840527,\n",
      "       -0.05456093, -0.05088836,  0.08463506,  0.02773152,  0.00927974,\n",
      "        0.07164233,  0.01658819,  0.1004961 ,  0.01752567,  0.06969062,\n",
      "       -0.0280961 ,  0.04937143, -0.1102905 ,  0.17765631,  0.10806576,\n",
      "        0.04090298, -0.03244111,  0.0710505 , -0.0236085 , -0.01213399,\n",
      "       -0.0704485 ,  0.02298536,  0.03032258, -0.06795198,  0.08375499,\n",
      "       -0.01828313,  0.06120717,  0.00830996,  0.03013169,  0.11862989,\n",
      "        0.04382801, -0.01856416,  0.05387056, -0.03458365,  0.05810287,\n",
      "       -0.02490479, -0.03380043,  0.07532366,  0.0329912 , -0.02798785,\n",
      "       -0.06160675,  0.06655892, -0.02262891,  0.03725707,  0.04741853,\n",
      "        0.08246027,  0.01303187, -0.00594833, -0.01682309, -0.10288028,\n",
      "        0.01488283, -0.02518665, -0.02243871,  0.02644645,  0.04920357,\n",
      "        0.05089033, -0.01895311,  0.03093179,  0.06539523,  0.15155894,\n",
      "        0.06500748, -0.03946964,  0.03506508, -0.05409551,  0.03893089,\n",
      "        0.11346559, -0.01790141, -0.01789358,  0.02011276,  0.02265164,\n",
      "        0.02133714, -0.02804362, -0.01839216, -0.03159145,  0.07035215,\n",
      "        0.02975568, -0.00256018,  0.05975217, -0.10486189,  0.0144302 ,\n",
      "       -0.00597729,  0.05695296, -0.05647493, -0.04239968, -0.02842454,\n",
      "        0.03513355,  0.09316868, -0.06563331, -0.04137258,  0.06920502,\n",
      "       -0.03799676, -0.01320651, -0.12348821, -0.01320617,  0.02438612,\n",
      "       -0.03724146, -0.03393256, -0.0546996 , -0.11458977, -0.01999717,\n",
      "       -0.06246351,  0.0205835 ,  0.04505618,  0.00766123, -0.01513597,\n",
      "       -0.05552949,  0.0292985 , -0.01753835,  0.02686644, -0.03005744,\n",
      "        0.03846409,  0.06050583,  0.05664948, -0.08370744, -0.0227057 ,\n",
      "        0.06017225,  0.1154329 ,  0.01620861, -0.08606713,  0.09092657,\n",
      "        0.02552759,  0.05565372, -0.06158271,  0.06438935, -0.12277174,\n",
      "        0.04981929, -0.03220665, -0.07850539, -0.08722664,  0.08468933,\n",
      "        0.03045592,  0.05508399,  0.03253943,  0.02638353, -0.03644025,\n",
      "       -0.0302318 ,  0.05070377,  0.05944631,  0.0303771 , -0.02993393,\n",
      "        0.11052501, -0.03789414, -0.06878031, -0.03710979,  0.04624723,\n",
      "       -0.00582242, -0.08693857,  0.10982386, -0.0846481 , -0.05502711,\n",
      "        0.02721819, -0.00624085,  0.05052273,  0.09670968,  0.02329821,\n",
      "       -0.0561017 , -0.06130004, -0.13202406,  0.02490694, -0.06522878,\n",
      "        0.08917357, -0.0781582 , -0.03343432, -0.02498073, -0.03678183,\n",
      "       -0.04409772, -0.05073732,  0.0209435 , -0.04053426, -0.02642584,\n",
      "       -0.03413355, -0.01524978, -0.16898586,  0.01621213,  0.04988238,\n",
      "        0.07233596,  0.02032084, -0.02172309, -0.01555825, -0.11605502,\n",
      "       -0.07075445, -0.01200854, -0.02280848,  0.03491147,  0.09916748,\n",
      "        0.01501488, -0.04078403, -0.01905564, -0.00580558, -0.06778434,\n",
      "        0.04891409, -0.06724516,  0.00452171, -0.03716186,  0.04452775,\n",
      "        0.07279693, -0.07514754,  0.06232101, -0.05615058, -0.06818636,\n",
      "        0.07552236, -0.02067724,  0.03541669, -0.0129585 ,  0.0501262 ,\n",
      "        0.05909918, -0.07769722, -0.05310088, -0.04295729,  0.03954041,\n",
      "        0.14040354,  0.06160668, -0.00759882, -0.10669726, -0.03181272,\n",
      "       -0.03692181, -0.0359194 , -0.09805516,  0.08155809, -0.01866148],\n",
      "      dtype=float32), array([ 5.42256758e-02, -1.36869997e-02,  5.63742667e-02, -1.76552590e-02,\n",
      "        5.40666655e-02,  7.96939358e-02, -1.96707621e-02, -9.54882707e-03,\n",
      "       -1.91104114e-02, -6.61647692e-02,  7.35003352e-02, -8.46525803e-02,\n",
      "       -8.29943176e-03,  1.33396899e-02, -7.49479458e-02,  3.52781378e-02,\n",
      "       -8.78517479e-02, -3.09684277e-02, -1.57321289e-01, -4.70764004e-03,\n",
      "        1.12925731e-02, -2.71322038e-02, -2.23630983e-02,  2.10506544e-02,\n",
      "       -9.40269753e-02,  1.03738956e-01, -8.21368620e-02,  1.45952022e-02,\n",
      "        2.69556325e-02, -2.30907146e-02,  8.26626923e-03, -8.72244034e-03,\n",
      "       -3.68995294e-02, -3.02599128e-02,  8.77981260e-02, -2.80291494e-02,\n",
      "       -2.08007265e-02,  2.58015655e-02,  6.73869103e-02,  2.95455381e-02,\n",
      "       -3.78655568e-02, -1.31337956e-01,  4.12085429e-02,  4.33000475e-02,\n",
      "       -5.84155414e-03,  1.31329820e-02, -1.02277495e-01, -1.41784310e-01,\n",
      "        2.86723804e-02,  1.40386205e-02, -4.86790761e-02,  9.98812169e-02,\n",
      "        6.82267100e-02, -6.11447282e-02, -3.89899462e-02, -7.59582743e-02,\n",
      "       -6.61000088e-02,  2.66286060e-02, -4.59683724e-02,  6.96510822e-02,\n",
      "       -1.17376000e-01, -5.00990041e-02,  3.44110839e-02, -8.37123320e-02,\n",
      "        6.11901283e-02, -7.18345121e-02,  1.02576492e-02, -5.79058565e-02,\n",
      "        1.37567863e-01,  4.46778610e-02, -1.77958794e-02,  7.26146176e-02,\n",
      "       -7.50663737e-03, -5.57356812e-02,  2.70808432e-02,  7.68765714e-03,\n",
      "        1.20100595e-01, -5.70038855e-02,  1.89711899e-03,  5.47647811e-02,\n",
      "       -9.48002283e-03,  4.19440567e-02, -3.09091862e-02, -3.37593141e-04,\n",
      "       -1.23105785e-02, -8.10220689e-02, -4.74324562e-02, -3.10703609e-02,\n",
      "        1.82177816e-02, -5.05647138e-02, -3.32498103e-02, -9.53970030e-02,\n",
      "       -3.97177786e-02, -1.79807208e-02, -5.73488660e-02, -8.07934850e-02,\n",
      "        1.04243550e-02, -1.93416346e-02, -4.70086299e-02,  1.57188810e-02,\n",
      "        7.70054609e-02, -1.01050317e-01, -5.41705042e-02,  4.66480106e-02,\n",
      "        3.71965133e-02, -6.72277063e-02, -3.52804624e-02,  4.36166488e-02,\n",
      "        3.96762714e-02, -6.84452206e-02, -2.52466388e-02,  1.99905243e-02,\n",
      "        9.50613841e-02,  2.21515317e-02, -4.58033606e-02, -3.04895900e-02,\n",
      "        4.86703366e-02,  8.93631503e-02,  1.01525582e-01,  2.58779693e-02,\n",
      "        4.61096764e-02, -7.04334024e-03, -3.87932062e-02,  1.01578059e-02,\n",
      "       -6.39008284e-02, -7.56021962e-02,  4.59319800e-02,  6.08191974e-02,\n",
      "        2.58126849e-04,  1.19505301e-01,  7.37848803e-02, -9.66017097e-02,\n",
      "       -4.02511805e-02,  8.96941721e-02, -3.61862555e-02,  1.06583275e-02,\n",
      "        4.17978466e-02,  2.62376796e-02,  2.17569843e-02,  1.27427606e-03,\n",
      "       -8.18062276e-02, -5.31127416e-02, -2.24818345e-02, -1.62771381e-02,\n",
      "        1.36987595e-02,  6.69485107e-02,  4.22983468e-02, -1.96285788e-02,\n",
      "        6.97519407e-02,  1.14943437e-01, -6.25589043e-02,  1.11462940e-02,\n",
      "       -6.54402822e-02, -1.96099021e-02,  1.61412135e-01, -5.04862331e-02,\n",
      "        1.59636550e-02, -8.61527920e-02,  1.03329450e-01,  1.80233996e-02,\n",
      "        4.62461915e-03, -1.70330986e-05, -8.97633936e-03, -6.70269430e-02,\n",
      "        8.26751143e-02, -2.88733058e-02,  2.49600766e-04,  8.49718170e-04,\n",
      "        3.41180712e-02, -1.74641591e-02, -6.01848438e-02,  8.36702213e-02,\n",
      "        2.92923246e-02,  7.06471205e-02, -9.64424610e-02,  3.24731204e-03,\n",
      "        5.65605126e-02, -1.72025077e-02,  2.61797048e-02,  1.80665907e-02,\n",
      "        5.46807721e-02, -6.34798929e-02, -4.80018966e-02,  6.16231468e-03,\n",
      "       -7.26477653e-02,  9.46828499e-02,  5.06913066e-02,  3.52406129e-02,\n",
      "       -7.49494359e-02,  1.50526285e-01,  1.64158568e-02,  7.23559186e-02,\n",
      "       -1.32612102e-02, -6.31308034e-02,  3.39797363e-02, -5.63211218e-02,\n",
      "       -8.46248027e-03,  7.47287646e-02, -2.65780520e-02, -2.49148645e-02,\n",
      "        4.72235903e-02, -5.05804494e-02,  4.50150073e-02,  1.70226246e-02,\n",
      "        4.27082293e-02,  4.14038217e-03,  5.11103347e-02,  1.34431005e-01,\n",
      "        2.16728784e-02,  1.49792656e-02,  9.75788236e-02, -1.98430389e-01,\n",
      "       -1.05661177e-03, -2.33359206e-02, -3.07523645e-02,  2.47724764e-02,\n",
      "        5.29869944e-02, -1.26478717e-01, -2.25093346e-02, -5.71949966e-02,\n",
      "        1.96154527e-02, -1.92339495e-02,  6.33432046e-02,  6.76917210e-02,\n",
      "        5.63780265e-03,  1.03870168e-01, -1.98346023e-02,  7.58304819e-02,\n",
      "       -2.10129526e-02, -4.07175310e-02, -3.99028249e-02, -2.28024479e-02,\n",
      "        4.28999178e-02, -2.30115373e-03, -5.23289219e-02, -4.37812544e-02,\n",
      "        1.50688708e-01,  3.35450284e-02,  8.12493917e-03, -5.54420501e-02,\n",
      "        5.85794188e-02, -3.53865512e-02, -2.10394878e-02,  3.74412425e-02,\n",
      "       -5.28143570e-02, -3.99932712e-02, -3.76041196e-02,  5.15926965e-02,\n",
      "       -1.36370389e-02,  4.18827236e-02,  2.41612121e-02, -9.40286461e-03,\n",
      "        4.01662104e-02,  4.11091223e-02,  3.72301489e-02,  1.36914179e-02,\n",
      "       -7.93700386e-03,  6.79388046e-02, -3.01285889e-02,  3.06104515e-02,\n",
      "        8.33430663e-02,  1.86199080e-02, -8.31174999e-02,  1.12914396e-02,\n",
      "        8.67015347e-02,  6.15490600e-02, -3.98699716e-02, -4.89760563e-02,\n",
      "        7.17920065e-03, -4.52285483e-02, -2.00414658e-02, -8.72785319e-03,\n",
      "       -3.61304767e-02,  6.52077124e-02, -4.30541635e-02, -4.98100929e-02,\n",
      "        4.06269990e-02,  8.70403051e-02,  5.98546788e-02, -1.28739299e-02,\n",
      "        9.60717872e-02,  1.16299041e-01,  1.49185183e-02, -6.98787868e-02,\n",
      "        1.09095853e-02, -5.24506345e-02, -2.93999584e-03,  5.84539324e-02,\n",
      "        7.17783049e-02, -6.63395673e-02,  5.59593886e-02, -2.72477847e-02,\n",
      "        1.05388630e-02,  3.82054560e-02, -4.28681001e-02,  2.90700607e-03,\n",
      "        4.16241661e-02, -7.44762197e-02,  6.42956886e-03, -1.58074230e-03],\n",
      "      dtype=float32), array([-6.67608576e-03,  4.20960188e-02,  9.60608870e-02,  1.13141790e-01,\n",
      "        2.77161468e-02,  7.11222878e-03, -5.90742193e-02,  1.43359145e-02,\n",
      "        6.95958436e-02,  1.69963986e-02,  1.15732895e-02,  3.82941067e-02,\n",
      "        5.93030527e-02, -1.46069936e-02,  5.14336266e-02,  1.22035043e-02,\n",
      "        6.37539327e-02, -6.74434304e-02,  1.81749538e-02, -1.07241198e-02,\n",
      "        7.17116892e-02,  3.86187509e-02, -6.37387708e-02,  5.44505306e-02,\n",
      "        4.46781628e-02,  2.07956489e-02, -2.43138857e-02, -9.38648731e-02,\n",
      "       -7.26831704e-02,  2.54469346e-02, -1.41486032e-02, -3.44273336e-02,\n",
      "       -3.74753475e-02, -5.37005113e-03,  6.47821426e-02, -7.38203675e-02,\n",
      "        2.05113273e-02,  6.38843253e-02, -1.11701898e-01,  2.03072093e-02,\n",
      "       -3.01898345e-02,  3.08331177e-02,  2.21780990e-03,  5.93953282e-02,\n",
      "        4.58040508e-03, -6.70386180e-02, -7.44177625e-02, -5.05766682e-02,\n",
      "       -1.02212876e-01, -2.92819235e-02, -6.53487816e-02, -1.88074950e-02,\n",
      "        4.58293147e-02, -6.55226260e-02,  2.95669716e-02, -4.70558356e-04,\n",
      "        2.03534607e-02,  1.20370440e-01, -3.47961448e-02,  7.52678886e-02,\n",
      "        1.10748950e-02,  2.15320438e-02, -2.60052774e-02, -1.04206458e-01,\n",
      "       -1.48268417e-02,  9.27197337e-02,  3.55896950e-02,  7.81276077e-02,\n",
      "       -4.25690524e-02, -3.34178843e-02,  2.87525728e-02, -1.13022448e-02,\n",
      "       -5.77043518e-02,  1.49905561e-02, -8.38017687e-02,  9.64619778e-03,\n",
      "       -5.81222326e-02, -4.00414877e-02,  3.89769375e-02, -8.62307698e-02,\n",
      "       -9.10735279e-02, -1.09281950e-01, -3.07802111e-02,  1.37642205e-01,\n",
      "       -1.33578116e-02, -1.01142339e-02, -1.06220171e-02,  4.69941720e-02,\n",
      "       -7.11644292e-02, -8.41387361e-02,  5.39080203e-02,  1.06755652e-01,\n",
      "        1.09718606e-01, -6.60784766e-02,  1.28825858e-01,  4.89673726e-02,\n",
      "       -1.05619781e-01,  1.49995601e-02,  3.18805687e-02, -3.70058231e-02,\n",
      "       -1.37870600e-02, -8.59416351e-02, -8.16328973e-02, -1.48090973e-01,\n",
      "       -6.93901330e-02, -3.27253267e-02,  1.16696186e-01, -1.01722419e-01,\n",
      "       -1.09093618e-02,  6.97728842e-02, -2.03408133e-02,  1.67422723e-02,\n",
      "       -6.25697896e-03, -1.45932242e-01, -7.26756454e-02,  4.17404994e-02,\n",
      "       -7.06189349e-02, -1.35533111e-02, -3.14074755e-02, -1.70224123e-02,\n",
      "       -6.61849305e-02, -3.57939638e-02,  1.64608154e-02,  6.36370260e-06,\n",
      "       -8.90492834e-03,  4.23041806e-02,  5.67115992e-02, -9.78130847e-02,\n",
      "        8.87235347e-03, -1.39853302e-02, -2.54198238e-02,  4.59368490e-02,\n",
      "       -7.62088001e-02,  2.37618294e-03,  4.31046635e-02, -7.89137650e-03,\n",
      "        7.79762026e-03, -2.87845191e-02,  4.01353948e-02,  6.57752901e-02,\n",
      "        6.06224425e-02,  7.85977766e-02, -1.03424182e-02, -1.39524683e-01,\n",
      "       -2.11756453e-02,  7.79722035e-02,  1.93430893e-02,  3.17712165e-02,\n",
      "       -4.69447635e-02, -1.06713669e-02,  9.19901729e-02, -6.68926090e-02,\n",
      "       -3.18204910e-02, -9.97200683e-02, -1.53100928e-02,  1.10324718e-01,\n",
      "       -7.25328103e-02, -7.49215111e-02, -1.34887807e-02, -6.84003066e-03,\n",
      "        5.28215729e-02,  2.09171255e-03, -4.69385460e-02, -9.32471082e-02,\n",
      "       -2.19972320e-02,  6.91769819e-04, -2.46161520e-02,  2.10998431e-02,\n",
      "        2.20768899e-02,  1.35922479e-02, -9.22339261e-02,  1.03861995e-01,\n",
      "       -3.93773131e-02, -4.04366292e-03,  1.19259423e-02,  2.16789637e-02,\n",
      "       -2.32014209e-02, -7.57401884e-02, -9.10323858e-02, -3.03507596e-02,\n",
      "        3.01652960e-02,  9.83121395e-02,  6.73260242e-02,  9.05492753e-02,\n",
      "       -6.61677048e-02,  5.60680814e-02, -1.93926226e-02,  3.26784439e-02,\n",
      "        9.81687382e-02, -1.08353226e-02, -3.14136669e-02,  3.61986868e-02,\n",
      "       -3.53253703e-03,  8.03976282e-02, -1.59629516e-03,  8.53129923e-02,\n",
      "        1.47532985e-01,  1.50557514e-02, -1.04106963e-01,  4.67730546e-03,\n",
      "        6.17335066e-02,  8.00365675e-03,  1.10967932e-02, -1.96630135e-04,\n",
      "       -6.43109679e-02,  2.18257960e-02, -3.09833456e-02, -2.93694586e-02,\n",
      "       -3.89165245e-02,  1.34430200e-01, -4.90708388e-02, -2.33621784e-02,\n",
      "        8.54035467e-02,  1.90113503e-02,  4.24305685e-02,  6.03052825e-02,\n",
      "        3.07332389e-02,  5.55892438e-02,  2.71397382e-02,  1.34421557e-01,\n",
      "        9.35546309e-02,  5.62342815e-03, -2.47264169e-02, -1.55092767e-02,\n",
      "        7.79418275e-03,  4.04033586e-02, -5.99392615e-02, -9.88617241e-02,\n",
      "        3.52965221e-02, -8.28663781e-02, -6.29376899e-03,  5.20060062e-02,\n",
      "        5.22440970e-02, -1.71683617e-02,  1.25563880e-02, -3.18403244e-02,\n",
      "       -8.92642047e-03, -5.59049752e-03, -3.28286132e-03, -4.74541634e-03,\n",
      "        3.92040126e-02, -4.01070453e-02,  4.98601682e-02,  5.62110543e-02,\n",
      "       -8.41163248e-02, -7.24691525e-02, -1.39311980e-02, -2.28554029e-02,\n",
      "       -2.82670017e-02, -4.30921875e-02, -4.37409356e-02,  4.99694645e-02,\n",
      "       -1.73653346e-02, -7.12479055e-02, -5.50835440e-03,  2.80579776e-02,\n",
      "        1.62092096e-03,  1.73110180e-02, -3.22605781e-02, -5.24404831e-02,\n",
      "       -1.21100973e-02, -8.91627520e-02,  5.31928912e-02,  6.43784031e-02,\n",
      "        1.01595163e-01, -1.66818779e-02,  3.24779190e-02, -6.59184232e-02,\n",
      "       -8.66750032e-02,  7.93045387e-02, -4.62688878e-03, -3.65967900e-02,\n",
      "       -3.11445389e-02,  1.23227211e-02, -1.59321725e-02,  2.42430605e-02,\n",
      "        6.84354007e-02, -4.05474119e-02, -7.10612535e-02, -5.58273792e-02,\n",
      "       -4.87914830e-02, -3.15299854e-02, -3.23499963e-02,  1.74909458e-02,\n",
      "       -5.76867647e-02, -1.27186617e-02,  5.84377386e-02,  2.79783159e-02,\n",
      "       -8.39985907e-02,  2.57142191e-03, -7.21863750e-03,  9.01038274e-02,\n",
      "        8.71852934e-02, -1.23450957e-01, -9.42277610e-02, -4.04062346e-02,\n",
      "        2.32471172e-02,  2.29692031e-02, -1.81030780e-02,  3.62128876e-02],\n",
      "      dtype=float32), array([-0.05599715,  0.09232661,  0.02856717,  0.03922853,  0.06373315,\n",
      "        0.05783059,  0.03760409,  0.00098036, -0.03888972, -0.04223406,\n",
      "        0.01406798,  0.02909052, -0.10325421, -0.078818  , -0.01789882,\n",
      "        0.03083967, -0.0533117 , -0.15745927,  0.05249369,  0.02429527,\n",
      "        0.1170584 ,  0.00836351, -0.00447374,  0.02368038,  0.02133869,\n",
      "       -0.00128505,  0.02236533,  0.07900905, -0.00986045,  0.01651729,\n",
      "       -0.01299433,  0.02728896,  0.16974437,  0.01726028,  0.01403358,\n",
      "        0.03543566,  0.11797738, -0.01662329,  0.03176951, -0.16380346,\n",
      "       -0.10169875,  0.00634092, -0.04287021,  0.02057382, -0.11889798,\n",
      "        0.06035064,  0.05398304,  0.00360173,  0.13362014, -0.00698664,\n",
      "        0.02584037,  0.03258907, -0.00989548, -0.01709799,  0.00313073,\n",
      "        0.08540785, -0.03482765,  0.11453092, -0.0508584 , -0.02971342,\n",
      "        0.03950407, -0.09639423,  0.01975607,  0.08806883, -0.02442437,\n",
      "       -0.01344692,  0.01668806,  0.05729014, -0.0068989 , -0.00703591,\n",
      "        0.04024338, -0.02414514, -0.01526307,  0.07270049, -0.08164021,\n",
      "        0.03801102, -0.00218925,  0.01502065,  0.05621436, -0.07051809,\n",
      "       -0.1013    ,  0.04468388, -0.01240677, -0.08144838,  0.0638073 ,\n",
      "       -0.09911297, -0.04080105, -0.06056977, -0.05278076, -0.02380339,\n",
      "        0.12909645,  0.02334313,  0.06579876,  0.02685268,  0.01058498,\n",
      "        0.05574195, -0.00899358, -0.03663979, -0.01781378, -0.02971212,\n",
      "       -0.05692352, -0.03799686,  0.00079359,  0.06041475,  0.08312461,\n",
      "        0.04152164,  0.02971056,  0.09533779,  0.03248798,  0.03962236,\n",
      "       -0.14783883, -0.0601651 ,  0.02976126,  0.01544569, -0.05390104,\n",
      "       -0.09420343, -0.01739686, -0.01061626,  0.0354763 ,  0.03005002,\n",
      "       -0.03457673,  0.05978407, -0.02373197, -0.0329708 , -0.04461712,\n",
      "       -0.02256355, -0.04629264,  0.03626825,  0.00630154,  0.02305371,\n",
      "        0.00383984, -0.0020638 , -0.0385057 ,  0.01662974, -0.01375665,\n",
      "        0.07368471,  0.00548809,  0.02194284,  0.03586926, -0.02306614,\n",
      "        0.0507779 ,  0.07149391, -0.00106475, -0.10043865, -0.05695022,\n",
      "       -0.05333145, -0.02878528,  0.03363839, -0.03550364, -0.01845666,\n",
      "        0.03259679, -0.02037543, -0.12765692, -0.02286388, -0.0219848 ,\n",
      "       -0.01625417,  0.01477798, -0.0141531 , -0.09873779,  0.03444181,\n",
      "       -0.00223939,  0.09156957, -0.02689664,  0.02381828, -0.0326025 ,\n",
      "        0.03581037,  0.01768789, -0.03025499,  0.05499133, -0.02918974,\n",
      "        0.07264591, -0.03750066, -0.0621322 , -0.06892397,  0.01818351,\n",
      "       -0.06490478,  0.03076357,  0.04916348,  0.00301074, -0.03400999,\n",
      "        0.05533252, -0.04404905, -0.00032624,  0.05110829,  0.01797878,\n",
      "       -0.11222752,  0.0186209 , -0.04528959, -0.00651214, -0.04658692,\n",
      "        0.13348836,  0.12286056, -0.02151169,  0.02902607, -0.06420109,\n",
      "       -0.05273475,  0.02043397,  0.0783575 , -0.01352001,  0.04512704,\n",
      "        0.01299245, -0.01241807,  0.03869912,  0.00448549, -0.05661736,\n",
      "       -0.02328174, -0.04117502,  0.03451412,  0.10398204, -0.09009282,\n",
      "       -0.13812134,  0.08313733,  0.01824371,  0.03184203,  0.06708957,\n",
      "       -0.093258  ,  0.06714518, -0.08906437,  0.05899345,  0.06292058,\n",
      "       -0.11076413,  0.09346875,  0.03481811,  0.05195146, -0.08030419,\n",
      "        0.06679022, -0.02031983, -0.12102187, -0.02562026,  0.09252436,\n",
      "        0.10253279,  0.01621711,  0.03709516,  0.01411409,  0.01820215,\n",
      "       -0.01009863,  0.14144823, -0.02626609, -0.02574839, -0.0222329 ,\n",
      "       -0.0488877 ,  0.04619402, -0.00861717, -0.07333302,  0.05662749,\n",
      "        0.0619521 , -0.05213543, -0.01825627, -0.03053899, -0.03008693,\n",
      "        0.00113   , -0.02424833, -0.00631918, -0.06469762,  0.01695165,\n",
      "       -0.09514729,  0.01870375,  0.00708949, -0.12503019, -0.04848435,\n",
      "        0.01633182,  0.07051851, -0.12458934, -0.07156598, -0.04493077,\n",
      "        0.02754774, -0.01193439,  0.01290104,  0.0220279 , -0.02740869,\n",
      "       -0.06456057, -0.09141124,  0.02519646, -0.00763861, -0.04246636,\n",
      "       -0.00577329, -0.05083378,  0.09665469, -0.00702819,  0.04636599,\n",
      "       -0.0280288 , -0.05451186, -0.01430172,  0.05747627,  0.03644517,\n",
      "       -0.11608602,  0.06496498,  0.08693293,  0.03068505, -0.07615519,\n",
      "       -0.06959306, -0.02402887,  0.05848878,  0.03353997,  0.02286242,\n",
      "        0.04026948, -0.12194502,  0.02661985,  0.01178623,  0.00694943],\n",
      "      dtype=float32)]\n",
      "17\n",
      "8741\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "wordEmbedding = [word2vec.wv[word]  for word in word2vec.wv.index2word]\n",
    "word2index = { word:i for i, word in enumerate(word2vec.wv.index2word)}\n",
    "print(wordEmbedding[:10])\n",
    "print(word2index['中国'])\n",
    "print(word2index['天才'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 读取训练数据"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['ou', 'y', 'yo', 'you', 'u', 'o']\n",
      "['k', ' ', 'Th', 'I ', ' T', 'T', 'Thi', 'hin', 'ink', ' Th', 'h', 'n', 'I T', 'in', 'hi', 'I', 'i', 'nk']\n",
      "['人', '和国万', '和国', '中', '华人民', '共', '民', '民共', '民共和', '华人', '人民', '中华人', '华', '国', '人民共', '国万', '国万岁', '共和国', '万', '共和', '中华', '万岁', '岁', '和']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def compute_ngrams(word, num_min = 1, num_max = 3):\n",
    "    ngrams =[]\n",
    "    for ngram_length in range(num_min, min(len(word), num_max) + 1):\n",
    "        for i in range(len(word) - ngram_length + 1):\n",
    "            # print(i, i + ngram_length)\n",
    "            ngrams.append(word[i : i + ngram_length])\n",
    "    # print(ngrams)\n",
    "    return list(set(ngrams))\n",
    "\n",
    "print(compute_ngrams('you'))\n",
    "print(compute_ngrams('I Think'))\n",
    "print(compute_ngrams('中华人民共和国万岁'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# 从词向量文本文件 word2vec 中获取词向量，如果获取到直接返回，若没有获取到，那么把这个词拆开\n",
    "# 成为 ngrams 的新词组，并在 word2vec 中找新词组中的词向量并相加取平均，最后得到平均词向量输出\n",
    "def wordVec(word, word2vec, min_n = 1, max_n = 3):\n",
    "    # 确认词向量维度\n",
    "    word_size = word2vec.wv.syn0[0].shape[0]\n",
    "\n",
    "    # 如果在词典之中，直接返回词向量\n",
    "    if word in word2vec.wv.vocab.keys():\n",
    "        return word2vec[word]\n",
    "    else:\n",
    "        # 计算word的ngrams词组\n",
    "        ngrams = compute_ngrams(word, min_n, max_n)\n",
    "        # 不在词典的情况下\n",
    "        word_vec = numpy.zeros(word_size, dtype=numpy.float32)\n",
    "        ngrams_found = 0\n",
    "        ngrams_single = [ng for ng in ngrams if len(ng) == 1]\n",
    "        ngrams_more = [ng for ng in ngrams if len(ng) > 1]\n",
    "        # 先只接受2个单词长度以上的词向量\n",
    "        for ngram in ngrams_more:\n",
    "            if ngram in word2vec.wv.vocab.keys():\n",
    "                word_vec += word2vec[ngram]\n",
    "                ngrams_found += 1\n",
    "                #print(ngram)\n",
    "        # 如果，没有匹配到，那么最后是考虑单个词向量\n",
    "        if ngrams_found == 0:\n",
    "            for ngram in ngrams_single:\n",
    "                word_vec += word2vec[ngram]\n",
    "                ngrams_found += 1\n",
    "        if word_vec.any():\n",
    "            return word_vec / max(1, ngrams_found)\n",
    "        else:\n",
    "            # 不抛出异常，而是打印提示，并返回0向量。\n",
    "            print(KeyError('all ngrams for word %s absent from model' % word))\n",
    "            return word_vec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class DotDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        dict.__init__(self, *args, **kwargs)\n",
    "        self.__dict__ = self"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def isNan(a):\n",
    "    return a != a\n",
    "\n",
    "class RatingData(data.Dataset):\n",
    "    def __init__(self, path, \n",
    "                 word2index, \n",
    "                 max_row = -1, \n",
    "                 trainTestRate = 0.85, \n",
    "                 isTrain = True, \n",
    "                 wordCuter = thulac,\n",
    "                 clean_file_name = 'ratings_clean_4_HAN.csv',\n",
    "                 ):\n",
    "        self.token_list = []\n",
    "        self.label_list = []\n",
    "        # self.token_positions = torch.tensor([i for i in range(100)])\n",
    "\n",
    "        print(' ratings.csv 所在path:',path) # 地址不应该包含 ratings.csv\n",
    "\n",
    "        ratings_clean_filename = os.path.join(path, clean_file_name)\n",
    "        ratings_filename = os.path.join(path, 'ratings.csv')\n",
    "        if os.path.isfile(ratings_clean_filename):\n",
    "            clean_pd = pd.read_csv(ratings_clean_filename)\n",
    "        else:\n",
    "            print('没有找到缓存的文件%s, 读取源文件%s'%(ratings_clean_filename, ratings_filename))\n",
    "            ratings_pd = pd.read_csv(ratings_filename)\n",
    "            print('开始生成缓存文件%s'%(ratings_clean_filename))\n",
    "            clean_pd = pd.DataFrame({\n",
    "                'userId':[],\n",
    "                'restId':[],\n",
    "                'rating':[],\n",
    "                'comment':[],\n",
    "            })\n",
    "            nonRatingCount = 0\n",
    "            for i, row in ratings_pd.iterrows():\n",
    "                if max_row != -1 and i > max_row:\n",
    "                    break\n",
    "                if not isinstance(row['comment'], str) or row['comment'] == '':\n",
    "                    # print(i + 1, row['comment'])\n",
    "                    nonRatingCount += 1\n",
    "                    continue\n",
    "                r0 = row['rating']\n",
    "                r1 = row['rating_env']\n",
    "                r2 = row['rating_flavor']\n",
    "                r3 = row['rating_service']\n",
    "                if r0 == '' or isNan(r0): r0 = 0 # 假设总评分为 0 表示未评分\n",
    "                if r1 == '' or isNan(r1): r1 = 3\n",
    "                if r2 == '' or isNan(r2): r2 = 3\n",
    "                if r3 == '' or isNan(r3): r3 = 3\n",
    "                r0 = round(r0 * 0.5 + (r1 + r2 + r3) * 0.1666666)\n",
    "                if i % 10000 == 9999:\n",
    "                    print(i + 1, r0)\n",
    "\n",
    "                # token = tokenizer.encode(text=str(row['comment']), max_length=100, pad_to_max_length = True)\n",
    "                # words = thulac.cut(row['comment'], text=True)\n",
    "                words = list(wordCuter.cut(row['comment']))\n",
    "                # 0 在词向量集中是‘，’，换个词向量集可能表示其他\n",
    "                token = [ word2index[words[i]] if i < len(words) and words[i] in word2index else 0 \n",
    "                          for i in range(100)] \n",
    "                \n",
    "                newRow = DotDict()\n",
    "                newRow.userId = [row['userId']]\n",
    "                newRow.restId = [row['restId']]\n",
    "                newRow.rating = [r0]\n",
    "                newRow.comment = [json.dumps(token)]\n",
    "\n",
    "                clean_pd = clean_pd.append(pd.DataFrame(newRow), ignore_index=True)\n",
    "            print('空的评论数量： %d'%(nonRatingCount))\n",
    "            clean_pd.to_csv(ratings_clean_filename)\n",
    "\n",
    "        # 读取\n",
    "        if isTrain:\n",
    "            temp_pd = clean_pd[ : int(len(clean_pd) * trainTestRate)]\n",
    "        else:\n",
    "            temp_pd = clean_pd[int(len(clean_pd) * trainTestRate) : ]\n",
    "\n",
    "        for i, row in temp_pd.iterrows():\n",
    "            if max_row != -1 and i > max_row:\n",
    "                break\n",
    "\n",
    "            self.label_list.append(torch.tensor(row['rating']).long())\n",
    "            self.token_list.append(torch.from_numpy(numpy.array( json.loads(row['comment']) ) ).long())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.token_list[index], self.label_list[index]#, self.token_positions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_list)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      " ratings.csv 所在path: ../../DataSets/yf_dianping\n",
      "没有找到缓存的文件../../DataSets/yf_dianping\\ratings_clean_4_HAN_jieba.csv, 读取源文件../../DataSets/yf_dianping\\ratings.csv\n",
      "开始生成缓存文件../../DataSets/yf_dianping\\ratings_clean_4_HAN_jieba.csv\n",
      "20000 3\n",
      "30000 4\n",
      "40000 1\n",
      "50000 3\n",
      "60000 2\n",
      "70000 1\n",
      "80000 2\n",
      "90000 1\n",
      "100000 1\n",
      "110000 2\n",
      "120000 3\n",
      "130000 1\n",
      "140000 4\n",
      "150000 1\n",
      "160000 1\n",
      "170000 3\n",
      "180000 1\n",
      "190000 3\n",
      "200000 2\n",
      "空的评论数量： 9072\n",
      " ratings.csv 所在path: ../../DataSets/yf_dianping\n",
      "162289\n",
      "28640\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "ratingData = RatingData('../../DataSets/yf_dianping',\n",
    "                             word2index = word2index,\n",
    "                             max_row= 200000,\n",
    "                             isTrain=True,\n",
    "                             wordCuter= jieba,\n",
    "                             clean_file_name='ratings_clean_4_HAN_jieba.csv',\n",
    "                             )\n",
    "trainLoader = torch.utils.data.DataLoader(dataset=ratingData,\n",
    "                                          batch_size=256,\n",
    "                                          shuffle = True,\n",
    "                                          # num_workers = 0,\n",
    "                                          )\n",
    "ratingData2 = RatingData('../../DataSets/yf_dianping',\n",
    "                             word2index = word2index,\n",
    "                             max_row= 200000,\n",
    "                             isTrain=False,\n",
    "                             wordCuter= jieba,\n",
    "                             clean_file_name='ratings_clean_4_HAN_jieba.csv',\n",
    "                             )\n",
    "testLoader = torch.utils.data.DataLoader(dataset=ratingData2,\n",
    "                                          batch_size=256,\n",
    "                                          shuffle = True,\n",
    "                                          # num_workers = 0,\n",
    "                                          )\n",
    "print(len(ratingData.label_list))\n",
    "print(len(ratingData2.label_list))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "HAN(\n",
      "  (embed): Embedding(109683, 300, padding_idx=0)\n",
      "  (GRU1): GRU(300, 128, batch_first=True, bidirectional=True)\n",
      "  (self_attention1): SelfAttention(\n",
      "    (W): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (U): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      "  (GRU2): GRU(256, 128, batch_first=True, bidirectional=True)\n",
      "  (self_attention2): SelfAttention(\n",
      "    (W): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (U): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "--- torch.Size([109683, 300]) tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.4536,  0.7981, -0.1686,  ...,  0.5869,  1.4749,  0.1103],\n",
      "        [-0.9167, -1.4935, -0.1726,  ...,  1.0392, -0.1247, -0.9844],\n",
      "        ...,\n",
      "        [ 0.3840, -1.4033, -0.1209,  ...,  1.8694,  0.0456,  1.3015],\n",
      "        [-0.3421, -1.0351, -0.0067,  ...,  0.5667, -2.9257, -1.7058],\n",
      "        [-0.8390, -0.4597,  1.0564,  ...,  1.8105,  1.1417, -1.5699]])\n",
      "---> tensor([[-0.1445,  0.0364, -0.1456,  ...,  0.0005,  0.0213,  0.0401],\n",
      "        [ 0.0247,  0.0858, -0.0866,  ..., -0.0793, -0.0435, -0.1684],\n",
      "        [ 0.0310, -0.0072,  0.0038,  ...,  0.0185,  0.0105,  0.0297],\n",
      "        ...,\n",
      "        [ 0.0199, -0.0477,  0.0798,  ...,  0.0934, -0.1652,  0.0425],\n",
      "        [-0.1087,  0.0599,  0.0447,  ...,  0.0547,  0.0014,  0.0135],\n",
      "        [-0.1006,  0.0771,  0.0747,  ..., -0.0824,  0.0039, -0.0339]])\n",
      "--- torch.Size([384, 300]) tensor([[ 0.0104, -0.0458, -0.0668,  ..., -0.0523,  0.0404, -0.0136],\n",
      "        [ 0.0494, -0.0415,  0.0192,  ..., -0.0139,  0.0014,  0.0291],\n",
      "        [ 0.0214, -0.0215,  0.0187,  ...,  0.0164,  0.0375,  0.0689],\n",
      "        ...,\n",
      "        [ 0.0150,  0.0280, -0.0659,  ..., -0.0463, -0.0384,  0.0867],\n",
      "        [ 0.0759, -0.0166,  0.0470,  ...,  0.0264, -0.0218, -0.0643],\n",
      "        [ 0.0108,  0.0319,  0.0440,  ...,  0.0524,  0.0634,  0.0380]])\n",
      "---> tensor([[-0.0277,  0.0385,  0.0074,  ..., -0.0945, -0.1031,  0.1350],\n",
      "        [ 0.0369,  0.0722, -0.0406,  ...,  0.1045,  0.0276,  0.0365],\n",
      "        [-0.0134, -0.0817,  0.1581,  ..., -0.0643,  0.0431, -0.1055],\n",
      "        ...,\n",
      "        [ 0.1186, -0.0800,  0.0770,  ..., -0.0343,  0.1785, -0.0380],\n",
      "        [ 0.0836, -0.0501,  0.0072,  ...,  0.0294, -0.0882,  0.0962],\n",
      "        [ 0.1570, -0.0030,  0.0507,  ..., -0.0522,  0.0245, -0.0202]])\n",
      "--- torch.Size([384, 128]) tensor([[-0.0528,  0.0629,  0.0776,  ...,  0.0011,  0.0883,  0.0639],\n",
      "        [ 0.0139, -0.0337,  0.0185,  ..., -0.0429, -0.0168, -0.0573],\n",
      "        [-0.0668,  0.0557,  0.0344,  ..., -0.0249,  0.0677,  0.0050],\n",
      "        ...,\n",
      "        [-0.0637,  0.0337,  0.0568,  ...,  0.0729,  0.0156,  0.0299],\n",
      "        [ 0.0790,  0.0108,  0.0805,  ..., -0.0743,  0.0232,  0.0727],\n",
      "        [ 0.0058,  0.0138, -0.0384,  ...,  0.0709,  0.0376,  0.0084]])\n",
      "---> tensor([[-0.1102, -0.1287,  0.0197,  ...,  0.0385, -0.1184, -0.1420],\n",
      "        [ 0.0033, -0.1150, -0.1132,  ..., -0.1894,  0.0552,  0.0457],\n",
      "        [ 0.0963,  0.0121, -0.0632,  ..., -0.1131,  0.1226, -0.1461],\n",
      "        ...,\n",
      "        [ 0.0273, -0.1437,  0.1346,  ...,  0.1242,  0.1126, -0.0134],\n",
      "        [ 0.0956, -0.1558, -0.0501,  ..., -0.1040,  0.0491, -0.1004],\n",
      "        [ 0.4096,  0.1660, -0.0219,  ..., -0.0123,  0.2292, -0.1754]])\n",
      "--- torch.Size([384, 300]) tensor([[-0.0141,  0.0565,  0.0408,  ...,  0.0175, -0.0424,  0.0077],\n",
      "        [ 0.0213, -0.0594, -0.0072,  ..., -0.0236,  0.0692,  0.0062],\n",
      "        [-0.0813, -0.0825, -0.0086,  ...,  0.0202, -0.0852,  0.0684],\n",
      "        ...,\n",
      "        [-0.0746,  0.0776, -0.0499,  ...,  0.0192,  0.0803, -0.0262],\n",
      "        [-0.0666, -0.0116, -0.0590,  ..., -0.0269,  0.0118,  0.0316],\n",
      "        [ 0.0065,  0.0076,  0.0860,  ..., -0.0792, -0.0247,  0.0851]])\n",
      "---> tensor([[ 0.0160, -0.0291,  0.0492,  ...,  0.0410, -0.1945,  0.0261],\n",
      "        [-0.0869, -0.0179,  0.0412,  ...,  0.0322,  0.0949, -0.0789],\n",
      "        [-0.1166,  0.0040,  0.1391,  ..., -0.0653, -0.0673, -0.0318],\n",
      "        ...,\n",
      "        [-0.0345, -0.0556, -0.1167,  ...,  0.0402, -0.0267,  0.0701],\n",
      "        [ 0.0064,  0.1299, -0.2286,  ...,  0.0324, -0.0827,  0.0846],\n",
      "        [-0.0602, -0.0663,  0.1033,  ..., -0.1112,  0.0228,  0.0262]])\n",
      "--- torch.Size([384, 128]) tensor([[ 0.0840, -0.0879,  0.0463,  ..., -0.0880, -0.0247, -0.0155],\n",
      "        [ 0.0764,  0.0781, -0.0535,  ...,  0.0374, -0.0296, -0.0273],\n",
      "        [ 0.0001,  0.0055,  0.0216,  ..., -0.0434, -0.0708, -0.0380],\n",
      "        ...,\n",
      "        [-0.0289,  0.0371, -0.0262,  ...,  0.0742,  0.0022, -0.0312],\n",
      "        [-0.0310,  0.0432,  0.0227,  ..., -0.0663, -0.0011,  0.0674],\n",
      "        [-0.0866,  0.0579,  0.0605,  ...,  0.0519,  0.0405, -0.0630]])\n",
      "---> tensor([[-0.0004, -0.0439,  0.0794,  ..., -0.0844,  0.0494, -0.1976],\n",
      "        [ 0.1918,  0.0604,  0.1045,  ...,  0.2723,  0.1336, -0.0503],\n",
      "        [-0.0342, -0.0101, -0.1299,  ...,  0.0822,  0.0538, -0.0071],\n",
      "        ...,\n",
      "        [-0.0706, -0.0156,  0.0222,  ..., -0.1495,  0.1960,  0.1958],\n",
      "        [ 0.1047, -0.0509, -0.0908,  ...,  0.0039,  0.1050,  0.0171],\n",
      "        [-0.1716,  0.0832, -0.0229,  ...,  0.0471, -0.1752,  0.0966]])\n",
      "--- torch.Size([512, 256]) tensor([[-0.0314, -0.0475, -0.0521,  ...,  0.0546,  0.0501,  0.0113],\n",
      "        [-0.0243,  0.0014, -0.0314,  ..., -0.0568, -0.0390, -0.0527],\n",
      "        [ 0.0595,  0.0360, -0.0401,  ...,  0.0573,  0.0361,  0.0590],\n",
      "        ...,\n",
      "        [ 0.0217,  0.0394,  0.0425,  ..., -0.0269,  0.0274,  0.0085],\n",
      "        [ 0.0197, -0.0227,  0.0406,  ...,  0.0465,  0.0349, -0.0273],\n",
      "        [ 0.0271, -0.0439, -0.0025,  ...,  0.0119,  0.0109,  0.0018]])\n",
      "---> tensor([[-0.0609, -0.1171,  0.0338,  ..., -0.1946, -0.0473,  0.1108],\n",
      "        [-0.0672, -0.0977,  0.0960,  ...,  0.0319,  0.0203,  0.0008],\n",
      "        [ 0.0648,  0.0319, -0.0307,  ...,  0.1379,  0.0816,  0.0006],\n",
      "        ...,\n",
      "        [ 0.0857, -0.0432, -0.0152,  ...,  0.0095,  0.1676, -0.0102],\n",
      "        [ 0.1122, -0.0795, -0.1770,  ..., -0.1394, -0.0458, -0.0216],\n",
      "        [ 0.0233, -0.0750,  0.0074,  ..., -0.1115, -0.0904, -0.1049]])\n",
      "--- torch.Size([1, 512]) tensor([[ 0.0404, -0.0016,  0.0220,  0.0012, -0.0230,  0.0297, -0.0140, -0.0048,\n",
      "          0.0206, -0.0388, -0.0047,  0.0150, -0.0128,  0.0407, -0.0039,  0.0383,\n",
      "          0.0170, -0.0216,  0.0366, -0.0330,  0.0041, -0.0414,  0.0260,  0.0441,\n",
      "          0.0079,  0.0063,  0.0377,  0.0129, -0.0396, -0.0308, -0.0376,  0.0102,\n",
      "         -0.0199,  0.0414, -0.0091,  0.0429, -0.0238,  0.0010,  0.0099,  0.0441,\n",
      "         -0.0348,  0.0032,  0.0437, -0.0280,  0.0214,  0.0254, -0.0330,  0.0145,\n",
      "         -0.0299, -0.0044,  0.0051,  0.0206, -0.0059,  0.0431, -0.0162,  0.0257,\n",
      "          0.0297,  0.0247, -0.0242, -0.0294, -0.0107,  0.0139, -0.0343, -0.0018,\n",
      "         -0.0395,  0.0209,  0.0322, -0.0372,  0.0406, -0.0126, -0.0101, -0.0379,\n",
      "         -0.0038,  0.0342,  0.0097, -0.0102, -0.0116, -0.0042, -0.0273,  0.0041,\n",
      "         -0.0134,  0.0132,  0.0353,  0.0400,  0.0263,  0.0267,  0.0405, -0.0207,\n",
      "          0.0228,  0.0019, -0.0236, -0.0038, -0.0093, -0.0299, -0.0001,  0.0307,\n",
      "         -0.0402, -0.0348,  0.0006,  0.0041,  0.0083,  0.0212,  0.0264, -0.0086,\n",
      "          0.0180, -0.0275,  0.0134, -0.0257, -0.0070,  0.0221, -0.0393, -0.0301,\n",
      "          0.0049, -0.0215,  0.0089,  0.0313,  0.0037, -0.0152, -0.0349, -0.0259,\n",
      "          0.0374, -0.0334, -0.0047,  0.0080, -0.0136,  0.0083, -0.0029, -0.0433,\n",
      "         -0.0039, -0.0102,  0.0126,  0.0367, -0.0358,  0.0166, -0.0037, -0.0086,\n",
      "          0.0084, -0.0419, -0.0113,  0.0150,  0.0094,  0.0018,  0.0316, -0.0160,\n",
      "         -0.0382,  0.0386,  0.0363, -0.0210,  0.0190, -0.0088,  0.0048, -0.0031,\n",
      "         -0.0370,  0.0423,  0.0180,  0.0382, -0.0172, -0.0057, -0.0418, -0.0016,\n",
      "         -0.0233, -0.0398,  0.0048, -0.0307, -0.0160, -0.0371,  0.0278, -0.0396,\n",
      "         -0.0432, -0.0225,  0.0151,  0.0125,  0.0364, -0.0313,  0.0330, -0.0114,\n",
      "         -0.0383,  0.0166,  0.0357, -0.0111, -0.0159, -0.0392,  0.0198,  0.0216,\n",
      "          0.0432,  0.0396, -0.0384,  0.0073,  0.0321, -0.0297, -0.0165,  0.0123,\n",
      "         -0.0203, -0.0257, -0.0373,  0.0407,  0.0439, -0.0191, -0.0163,  0.0245,\n",
      "          0.0411,  0.0204, -0.0260, -0.0370, -0.0268, -0.0373, -0.0175, -0.0045,\n",
      "         -0.0392, -0.0226, -0.0312, -0.0071, -0.0024, -0.0025, -0.0265, -0.0005,\n",
      "          0.0115, -0.0237,  0.0010,  0.0264, -0.0165,  0.0098, -0.0247, -0.0408,\n",
      "         -0.0378, -0.0049, -0.0045,  0.0089,  0.0219, -0.0045,  0.0380,  0.0123,\n",
      "          0.0309,  0.0384,  0.0305, -0.0236,  0.0103,  0.0190, -0.0013,  0.0439,\n",
      "         -0.0014, -0.0314, -0.0298, -0.0243, -0.0430,  0.0397,  0.0135,  0.0039,\n",
      "         -0.0326,  0.0270,  0.0053,  0.0014, -0.0355,  0.0135, -0.0162, -0.0173,\n",
      "          0.0178, -0.0220,  0.0253,  0.0294, -0.0192, -0.0398,  0.0172, -0.0070,\n",
      "         -0.0074, -0.0229, -0.0189,  0.0057,  0.0006, -0.0276, -0.0045,  0.0075,\n",
      "          0.0404,  0.0204,  0.0182, -0.0384, -0.0181,  0.0293,  0.0296, -0.0216,\n",
      "         -0.0056, -0.0406,  0.0362,  0.0093,  0.0341, -0.0387,  0.0147, -0.0400,\n",
      "          0.0371, -0.0147, -0.0370,  0.0167, -0.0026,  0.0315,  0.0010, -0.0175,\n",
      "         -0.0217,  0.0101, -0.0047, -0.0019, -0.0195, -0.0083, -0.0025, -0.0227,\n",
      "         -0.0356, -0.0010, -0.0328,  0.0279, -0.0251, -0.0084, -0.0344,  0.0242,\n",
      "          0.0438, -0.0335, -0.0283, -0.0129,  0.0371,  0.0222, -0.0164,  0.0187,\n",
      "         -0.0316,  0.0083,  0.0074, -0.0273, -0.0300, -0.0229,  0.0387, -0.0425,\n",
      "         -0.0273, -0.0150,  0.0235,  0.0145,  0.0408, -0.0031, -0.0101,  0.0114,\n",
      "          0.0243,  0.0023,  0.0080, -0.0372,  0.0113,  0.0310, -0.0290, -0.0421,\n",
      "          0.0269, -0.0054,  0.0316,  0.0070, -0.0047,  0.0038, -0.0025, -0.0017,\n",
      "          0.0200,  0.0315, -0.0429,  0.0110, -0.0378, -0.0277, -0.0288, -0.0021,\n",
      "          0.0189, -0.0043, -0.0209,  0.0361,  0.0007, -0.0127, -0.0102, -0.0208,\n",
      "         -0.0085,  0.0364, -0.0426, -0.0232, -0.0414, -0.0306,  0.0387,  0.0104,\n",
      "         -0.0311, -0.0321, -0.0043, -0.0013, -0.0393, -0.0088, -0.0276,  0.0084,\n",
      "         -0.0234,  0.0334,  0.0265, -0.0430, -0.0208, -0.0415,  0.0227,  0.0181,\n",
      "         -0.0356,  0.0364, -0.0048,  0.0256,  0.0135, -0.0325, -0.0178,  0.0176,\n",
      "         -0.0182,  0.0056,  0.0385, -0.0415,  0.0232,  0.0122, -0.0435, -0.0228,\n",
      "          0.0157, -0.0295,  0.0066, -0.0410,  0.0426,  0.0125, -0.0403,  0.0132,\n",
      "          0.0178, -0.0420, -0.0025,  0.0140, -0.0256, -0.0151, -0.0020, -0.0324,\n",
      "          0.0247, -0.0144,  0.0009,  0.0304,  0.0299,  0.0173,  0.0120,  0.0366,\n",
      "         -0.0313, -0.0115, -0.0063, -0.0324, -0.0317, -0.0366,  0.0398, -0.0136,\n",
      "         -0.0092, -0.0313, -0.0167, -0.0126,  0.0357, -0.0411, -0.0065, -0.0037,\n",
      "         -0.0407,  0.0040,  0.0110, -0.0101, -0.0123, -0.0165,  0.0395,  0.0409,\n",
      "         -0.0310, -0.0116,  0.0164,  0.0232, -0.0353,  0.0113, -0.0084,  0.0427,\n",
      "          0.0106,  0.0291,  0.0419, -0.0135, -0.0125, -0.0255, -0.0360,  0.0209,\n",
      "         -0.0358,  0.0403, -0.0014,  0.0117,  0.0325, -0.0211, -0.0232, -0.0336,\n",
      "         -0.0067,  0.0432, -0.0396, -0.0401,  0.0286,  0.0237, -0.0073,  0.0439,\n",
      "         -0.0379,  0.0395, -0.0145, -0.0047, -0.0024, -0.0060, -0.0039, -0.0104,\n",
      "          0.0408, -0.0121, -0.0322, -0.0291,  0.0249,  0.0143, -0.0153,  0.0111,\n",
      "         -0.0180,  0.0283,  0.0255,  0.0087, -0.0404, -0.0387, -0.0210,  0.0219]])\n",
      "---> tensor([[-1.6193e-02,  5.2172e-02, -3.0429e-02, -3.4902e-02, -4.1552e-02,\n",
      "         -3.0460e-02, -1.1831e-01,  7.5794e-02, -4.4632e-02, -5.9504e-03,\n",
      "         -3.8777e-02, -1.0396e-01,  8.4719e-02,  6.2083e-02, -1.0815e-01,\n",
      "          2.8301e-02,  8.6226e-02, -5.6168e-02,  7.1774e-02, -6.6283e-02,\n",
      "          2.0178e-03,  3.7073e-02, -5.8241e-02,  1.5031e-01, -6.0183e-02,\n",
      "         -7.0101e-02,  8.3586e-02,  1.8291e-01, -5.6154e-02, -3.8912e-02,\n",
      "          3.8402e-02,  3.8277e-02, -7.2061e-02,  5.8076e-03, -1.7224e-02,\n",
      "          8.7802e-02,  2.5096e-02,  1.6002e-02,  5.7532e-04, -7.3449e-02,\n",
      "          1.0169e-02, -1.3800e-02, -5.6586e-02, -6.4575e-02, -6.9198e-02,\n",
      "          8.9408e-02, -4.6621e-02,  4.6206e-02,  6.2809e-03,  6.7539e-02,\n",
      "          5.4549e-02,  1.3986e-01,  1.0101e-01, -1.2712e-02, -9.6048e-02,\n",
      "          1.9696e-02, -1.3059e-01,  6.8821e-02,  1.6476e-01,  9.3110e-02,\n",
      "          1.0495e-01,  1.6493e-02, -1.2643e-01,  6.6057e-02, -5.9518e-02,\n",
      "          7.5178e-02,  2.1177e-02, -8.0091e-02, -7.0401e-02, -3.6390e-02,\n",
      "          1.0130e-01,  5.3379e-02, -5.6635e-03,  1.3876e-02, -9.8797e-02,\n",
      "         -6.3078e-02, -1.0737e-02, -7.9221e-02,  4.6847e-02,  1.2692e-01,\n",
      "          4.3435e-02,  4.4257e-03, -6.2557e-02,  2.6488e-02, -3.0687e-02,\n",
      "         -1.4132e-02, -9.5607e-02, -2.0959e-01, -2.2986e-02, -1.2493e-02,\n",
      "          4.6166e-02,  4.3294e-02, -5.1239e-02, -7.6915e-02, -1.1908e-01,\n",
      "          6.9985e-03, -1.0467e-01,  6.1341e-02, -9.5785e-02, -5.0541e-02,\n",
      "          9.7860e-03, -4.7259e-02,  8.7359e-02, -1.2130e-02, -5.8473e-02,\n",
      "          6.9514e-02,  1.5723e-02, -1.0184e-01,  2.0190e-03, -4.3877e-02,\n",
      "         -5.1070e-02,  4.3381e-02, -2.8632e-02,  2.3953e-02,  8.0919e-03,\n",
      "          7.1930e-02, -1.0699e-01, -6.5540e-02, -4.8691e-02, -8.4177e-02,\n",
      "          1.7543e-02,  2.8824e-02, -8.5976e-02,  1.6803e-02,  1.3234e-01,\n",
      "          5.2149e-02, -1.3303e-01,  4.2063e-02,  1.4207e-02,  6.9163e-02,\n",
      "         -4.9220e-03, -4.1518e-02, -6.6901e-02, -9.1321e-03, -4.7429e-02,\n",
      "          1.1613e-03,  2.6010e-02, -3.7171e-02,  6.0620e-02,  4.1312e-02,\n",
      "         -9.2529e-02,  3.3978e-02,  7.5801e-02, -4.7729e-02, -1.3615e-01,\n",
      "         -1.4469e-01, -4.0971e-02,  4.5848e-02,  1.3886e-01,  4.7133e-02,\n",
      "         -5.6015e-02, -3.8316e-02,  2.3953e-02, -2.2007e-02, -4.4147e-02,\n",
      "         -6.6729e-02, -3.0500e-02, -5.2849e-02,  6.0429e-02,  1.6849e-02,\n",
      "          1.7858e-02, -6.8520e-02,  9.8712e-03,  7.3053e-02, -6.2685e-02,\n",
      "          4.4886e-02, -8.5299e-02, -6.4215e-02, -4.5414e-02,  2.0796e-04,\n",
      "         -7.6733e-02,  4.1764e-02,  3.4084e-02,  4.1479e-02, -6.1197e-02,\n",
      "         -7.1646e-02, -4.3649e-02,  4.8016e-02, -6.3276e-03, -5.2424e-03,\n",
      "          5.6733e-02,  5.0327e-02,  6.0103e-02, -3.3774e-02,  3.1342e-03,\n",
      "          5.9285e-03, -9.6929e-02,  2.4633e-02, -1.8225e-02,  8.8064e-02,\n",
      "         -5.3032e-03, -1.2096e-02, -6.9283e-02,  9.9418e-02, -2.2916e-04,\n",
      "         -6.9781e-02, -5.1565e-02,  6.6553e-02, -3.8017e-02,  1.2444e-02,\n",
      "          3.5044e-02, -2.3365e-02,  2.9161e-02,  1.1908e-01, -1.9098e-02,\n",
      "          3.3361e-02,  7.9430e-02, -8.0668e-02, -9.4275e-02, -3.7189e-02,\n",
      "          5.1714e-02, -2.0095e-02,  4.4336e-02, -3.2038e-02, -4.4352e-02,\n",
      "         -2.2470e-03,  6.8908e-02, -2.2449e-02,  3.9390e-02,  6.2586e-02,\n",
      "         -7.8384e-03,  1.0360e-01, -5.8524e-02,  8.6651e-02, -2.8116e-02,\n",
      "         -5.5910e-02,  8.7666e-03, -1.3761e-01,  1.1464e-01, -2.3000e-02,\n",
      "         -4.7734e-02,  2.9991e-02, -4.4625e-02, -2.4007e-02,  9.7219e-02,\n",
      "         -4.0755e-02,  9.8556e-02,  1.9960e-02,  1.1188e-01,  1.3457e-01,\n",
      "         -1.2817e-02, -2.6796e-02, -3.0675e-02,  1.5424e-02, -5.4511e-02,\n",
      "          3.6411e-02, -8.3277e-02,  8.8141e-04, -4.9085e-02,  2.4783e-02,\n",
      "          1.5203e-01,  9.3035e-02, -1.4823e-02,  1.0947e-01,  1.0220e-01,\n",
      "         -3.1637e-02,  7.8927e-03, -9.6091e-02,  4.8836e-02, -3.1130e-02,\n",
      "         -2.5383e-02, -4.1474e-02, -5.2759e-02, -7.8681e-02, -5.1199e-02,\n",
      "         -1.1392e-01,  6.7877e-02,  8.0752e-03,  3.5670e-02, -6.8759e-02,\n",
      "          1.3977e-02, -1.7998e-02,  2.3862e-02,  1.1574e-02,  1.5335e-02,\n",
      "          2.4487e-02,  1.0376e-01,  1.0760e-01,  4.2219e-02, -6.5579e-03,\n",
      "         -5.7216e-03,  2.5111e-02,  9.4860e-02, -5.5679e-02, -4.8446e-02,\n",
      "          1.0827e-02, -6.1238e-03,  1.4387e-03,  3.6876e-02, -6.8667e-02,\n",
      "          8.4892e-02, -3.0241e-02,  2.7637e-02,  1.5024e-02,  6.4618e-02,\n",
      "          1.1901e-01,  2.1839e-02, -5.0711e-02,  3.1918e-03,  2.7973e-02,\n",
      "         -5.1598e-02,  4.1134e-02,  1.4906e-02, -3.0764e-02,  8.3518e-03,\n",
      "         -3.4588e-02, -1.3743e-02,  3.1309e-02, -6.8668e-02, -1.3060e-02,\n",
      "          5.2357e-02,  9.7600e-02,  1.7803e-02,  5.5704e-02,  2.1989e-02,\n",
      "          1.8699e-02, -1.6787e-02,  2.0972e-02,  3.2689e-02,  5.4561e-02,\n",
      "          1.6370e-02, -2.4772e-02, -7.0726e-02, -6.3695e-02,  4.2566e-02,\n",
      "         -1.3825e-02, -8.0958e-02,  5.3385e-02, -7.5447e-02,  4.6099e-02,\n",
      "          3.4487e-02,  7.0853e-02,  2.9280e-02,  6.8044e-02, -1.1955e-01,\n",
      "          6.8109e-02, -6.6531e-02, -3.5043e-02, -2.6599e-02, -3.9732e-02,\n",
      "          5.8603e-02, -4.8989e-02, -7.5894e-04,  8.1651e-02, -3.9488e-02,\n",
      "         -6.0892e-02, -3.7733e-02,  6.4192e-02,  8.4146e-02,  3.1410e-02,\n",
      "         -4.4372e-02, -8.2607e-02, -1.8523e-03,  2.3756e-03, -4.8959e-02,\n",
      "         -1.5038e-01,  5.5075e-03,  7.9436e-04,  8.6923e-03,  5.6874e-02,\n",
      "         -2.5199e-02, -6.9281e-02, -8.3307e-03,  3.8094e-03,  1.3369e-01,\n",
      "         -3.9043e-02, -7.3165e-02, -1.0784e-02, -9.9657e-02,  1.1643e-02,\n",
      "         -2.6599e-02,  6.6936e-02,  7.6798e-02, -3.4904e-02,  7.7164e-02,\n",
      "         -6.1469e-02,  7.0202e-02,  2.4007e-02, -2.0260e-01, -2.6740e-02,\n",
      "          8.7617e-02,  4.3818e-03, -9.8891e-02,  2.4801e-02,  6.9977e-02,\n",
      "         -2.7930e-02, -9.2176e-03, -5.2056e-03, -5.0859e-03,  6.8067e-02,\n",
      "         -9.9102e-02, -9.2535e-02, -1.5600e-01, -8.7345e-02, -2.4578e-02,\n",
      "          1.4115e-02, -6.4577e-02, -4.1348e-03,  6.1984e-03, -6.4558e-02,\n",
      "          1.4019e-01,  1.7584e-01,  2.3999e-02,  3.9055e-02, -6.0400e-02,\n",
      "         -5.3193e-02,  7.1599e-02, -8.6039e-02, -1.4258e-02, -9.1803e-03,\n",
      "          1.1562e-01, -1.1449e-01, -7.2636e-03,  4.2073e-02, -8.3585e-02,\n",
      "          1.5919e-02,  5.5775e-02, -9.0630e-02,  1.2626e-01,  7.0223e-02,\n",
      "          2.2713e-02,  2.4288e-02, -1.0736e-01,  2.2846e-02, -4.2926e-02,\n",
      "          8.2342e-02, -2.1733e-02,  3.0679e-02,  1.2451e-01,  1.5161e-02,\n",
      "         -1.7854e-02, -2.0168e-02,  3.1141e-02, -9.0354e-03, -5.3913e-02,\n",
      "         -1.0566e-02, -4.8026e-02,  1.2932e-01, -9.7508e-03,  7.1622e-03,\n",
      "          2.3616e-02,  6.6079e-02, -4.9502e-02, -4.1829e-02, -7.9169e-03,\n",
      "         -3.8361e-02, -1.5280e-01, -5.2459e-02, -7.8576e-02,  8.6412e-02,\n",
      "          3.1962e-02, -3.6058e-03, -4.9740e-02,  1.3069e-01, -1.8920e-02,\n",
      "         -1.4924e-01,  9.0032e-02, -7.3930e-02,  1.0382e-01, -4.2509e-02,\n",
      "         -6.5066e-03, -3.2577e-02, -5.5597e-03,  2.9601e-02, -3.2429e-02,\n",
      "         -1.3836e-01, -8.4868e-02, -4.5221e-02,  9.2763e-02, -8.1917e-02,\n",
      "         -9.9805e-02,  7.7240e-03,  9.0483e-02, -3.5430e-02, -3.0336e-02,\n",
      "         -1.0180e-02,  5.3831e-02,  1.6158e-01, -7.1403e-02,  8.0655e-02,\n",
      "          8.5907e-03, -5.2282e-02,  7.8538e-03,  4.2138e-02, -8.1077e-02,\n",
      "          1.3503e-01,  5.7633e-03,  7.5469e-02, -2.2109e-02, -2.7859e-02,\n",
      "          4.7943e-02,  1.8664e-02, -1.2282e-01, -4.2871e-02, -3.8878e-02,\n",
      "         -1.8928e-02, -2.7728e-02,  4.8631e-03,  2.5129e-02,  9.8416e-02,\n",
      "          1.5946e-02,  2.4543e-02, -4.2688e-02, -2.5881e-02, -7.3051e-02,\n",
      "          2.6862e-02,  3.4673e-02, -1.1444e-03, -7.5902e-02,  9.9845e-02,\n",
      "         -5.0954e-02,  8.2347e-02]])\n",
      "--- torch.Size([384, 256]) tensor([[ 0.0120,  0.0052,  0.0457,  ..., -0.0421, -0.0028, -0.0025],\n",
      "        [ 0.0378,  0.0737,  0.0429,  ...,  0.0604, -0.0671, -0.0344],\n",
      "        [-0.0504, -0.0350, -0.0736,  ..., -0.0327,  0.0122, -0.0294],\n",
      "        ...,\n",
      "        [-0.0093, -0.0395,  0.0215,  ..., -0.0309,  0.0513, -0.0803],\n",
      "        [ 0.0495,  0.0785,  0.0760,  ...,  0.0264,  0.0123, -0.0039],\n",
      "        [ 0.0614,  0.0791, -0.0814,  ..., -0.0064,  0.0620, -0.0214]])\n",
      "---> tensor([[-6.1700e-03, -6.9948e-02, -3.4882e-02,  ...,  1.4989e-01,\n",
      "          9.5200e-02, -9.4194e-02],\n",
      "        [ 3.2866e-02,  4.3310e-02, -7.5836e-02,  ...,  7.5725e-02,\n",
      "          1.3803e-01,  6.5418e-02],\n",
      "        [-1.1404e-02, -3.8093e-02,  1.0803e-02,  ..., -5.8290e-02,\n",
      "         -1.0266e-01, -5.1327e-03],\n",
      "        ...,\n",
      "        [ 9.9703e-02, -2.9127e-04,  2.6282e-01,  ..., -1.0240e-01,\n",
      "          7.1164e-02,  8.9425e-02],\n",
      "        [-5.0966e-02,  1.9433e-02, -9.2554e-02,  ...,  5.6915e-02,\n",
      "         -1.8068e-01, -1.1533e-01],\n",
      "        [ 7.9323e-03,  8.7707e-02, -2.8710e-05,  ..., -1.4442e-02,\n",
      "         -1.6236e-02, -9.8103e-04]])\n",
      "--- torch.Size([384, 128]) tensor([[ 0.0095, -0.0212, -0.0501,  ...,  0.0014, -0.0499, -0.0566],\n",
      "        [-0.0509, -0.0671,  0.0007,  ...,  0.0650,  0.0457, -0.0289],\n",
      "        [-0.0493,  0.0165,  0.0384,  ...,  0.0479, -0.0154, -0.0280],\n",
      "        ...,\n",
      "        [-0.0529,  0.0387, -0.0669,  ...,  0.0482, -0.0532,  0.0469],\n",
      "        [ 0.0447,  0.0390, -0.0089,  ..., -0.0180, -0.0308, -0.0639],\n",
      "        [-0.0469,  0.0158,  0.0206,  ...,  0.0330, -0.0706, -0.0145]])\n",
      "---> tensor([[ 0.0289,  0.1854,  0.2672,  ...,  0.0972, -0.1511,  0.0398],\n",
      "        [ 0.0384, -0.1832, -0.2471,  ...,  0.1930,  0.1562,  0.0524],\n",
      "        [ 0.1298, -0.0287, -0.0546,  ...,  0.0196,  0.0836, -0.0863],\n",
      "        ...,\n",
      "        [-0.1055,  0.0120,  0.0608,  ...,  0.1302,  0.2457,  0.0627],\n",
      "        [-0.0228,  0.1497,  0.1881,  ..., -0.0721, -0.0033, -0.1624],\n",
      "        [-0.1450, -0.3114,  0.0549,  ..., -0.0539,  0.0515,  0.1223]])\n",
      "--- torch.Size([384, 256]) tensor([[-0.0848, -0.0692, -0.0072,  ...,  0.0714, -0.0657,  0.0606],\n",
      "        [ 0.0779,  0.0115,  0.0564,  ...,  0.0067, -0.0557, -0.0180],\n",
      "        [-0.0209,  0.0529,  0.0497,  ..., -0.0585, -0.0199,  0.0812],\n",
      "        ...,\n",
      "        [ 0.0726,  0.0391,  0.0831,  ..., -0.0826,  0.0454, -0.0569],\n",
      "        [-0.0239,  0.0460,  0.0586,  ...,  0.0618,  0.0481,  0.0348],\n",
      "        [ 0.0175,  0.0806,  0.0718,  ..., -0.0583, -0.0015,  0.0054]])\n",
      "---> tensor([[-0.0311,  0.0112, -0.2207,  ...,  0.0335, -0.0056, -0.0371],\n",
      "        [ 0.0147, -0.0740, -0.0008,  ...,  0.0727,  0.0015, -0.0430],\n",
      "        [ 0.0557,  0.0530,  0.0192,  ..., -0.1063, -0.0356,  0.0792],\n",
      "        ...,\n",
      "        [-0.0490,  0.0623,  0.0602,  ..., -0.0296,  0.0567, -0.0140],\n",
      "        [-0.1372, -0.1522, -0.1266,  ..., -0.0881, -0.0349,  0.0382],\n",
      "        [-0.0368, -0.0895,  0.1139,  ...,  0.0292,  0.1320, -0.0188]])\n",
      "--- torch.Size([384, 128]) tensor([[-0.0538,  0.0139,  0.0689,  ..., -0.0794, -0.0835, -0.0095],\n",
      "        [-0.0783, -0.0258,  0.0117,  ...,  0.0271,  0.0545, -0.0526],\n",
      "        [ 0.0287, -0.0292, -0.0099,  ...,  0.0509,  0.0816, -0.0087],\n",
      "        ...,\n",
      "        [ 0.0138,  0.0265,  0.0617,  ..., -0.0057,  0.0509, -0.0378],\n",
      "        [-0.0566, -0.0841,  0.0568,  ..., -0.0618,  0.0239, -0.0650],\n",
      "        [ 0.0073,  0.0202,  0.0721,  ...,  0.0726,  0.0134, -0.0636]])\n",
      "---> tensor([[-0.0541,  0.0776,  0.0745,  ...,  0.0224,  0.0140, -0.0119],\n",
      "        [-0.0876,  0.1222, -0.2020,  ..., -0.0049, -0.0833,  0.2353],\n",
      "        [-0.0636,  0.0366, -0.1502,  ...,  0.0186,  0.1064,  0.1278],\n",
      "        ...,\n",
      "        [-0.1645, -0.0289,  0.0923,  ...,  0.1227, -0.0605, -0.0320],\n",
      "        [ 0.0313,  0.2492, -0.0350,  ..., -0.0959,  0.1595,  0.1772],\n",
      "        [ 0.0745, -0.0055,  0.0871,  ...,  0.1107, -0.0761, -0.1395]])\n",
      "--- torch.Size([512, 256]) tensor([[ 0.0010, -0.0437,  0.0193,  ...,  0.0392, -0.0044,  0.0010],\n",
      "        [-0.0326, -0.0053,  0.0026,  ..., -0.0333,  0.0554, -0.0226],\n",
      "        [ 0.0522,  0.0459,  0.0190,  ...,  0.0271, -0.0079, -0.0275],\n",
      "        ...,\n",
      "        [-0.0389, -0.0104,  0.0173,  ...,  0.0228, -0.0492, -0.0042],\n",
      "        [ 0.0383,  0.0535,  0.0565,  ..., -0.0385,  0.0079,  0.0515],\n",
      "        [-0.0141,  0.0528,  0.0418,  ..., -0.0197,  0.0238,  0.0409]])\n",
      "---> tensor([[ 0.0703, -0.0374, -0.0336,  ..., -0.1062, -0.1246, -0.1118],\n",
      "        [-0.0629, -0.0167,  0.1703,  ...,  0.1959,  0.0627, -0.0330],\n",
      "        [ 0.0309, -0.0339,  0.0812,  ...,  0.0603,  0.2461,  0.0718],\n",
      "        ...,\n",
      "        [ 0.0431, -0.0477, -0.0508,  ...,  0.0323,  0.0165,  0.0057],\n",
      "        [-0.0636, -0.1232,  0.0501,  ...,  0.0761,  0.0265,  0.0722],\n",
      "        [ 0.0163, -0.0823,  0.2116,  ...,  0.0452, -0.0645,  0.0142]])\n",
      "--- torch.Size([1, 512]) tensor([[ 0.0227,  0.0124, -0.0011,  0.0098,  0.0227, -0.0287,  0.0163,  0.0435,\n",
      "         -0.0097, -0.0173, -0.0209, -0.0431,  0.0365, -0.0337, -0.0348,  0.0357,\n",
      "          0.0373, -0.0357, -0.0242, -0.0095, -0.0200,  0.0347,  0.0233,  0.0074,\n",
      "          0.0190, -0.0358,  0.0276, -0.0175,  0.0065, -0.0233, -0.0424, -0.0273,\n",
      "          0.0233, -0.0254, -0.0177, -0.0287,  0.0229, -0.0134,  0.0064,  0.0365,\n",
      "          0.0030, -0.0015, -0.0092,  0.0227, -0.0397,  0.0107,  0.0138,  0.0268,\n",
      "         -0.0094, -0.0290,  0.0095, -0.0261, -0.0054,  0.0213, -0.0221,  0.0163,\n",
      "          0.0222,  0.0269,  0.0056,  0.0070,  0.0132, -0.0327,  0.0393,  0.0101,\n",
      "         -0.0256,  0.0435, -0.0419, -0.0309,  0.0037,  0.0372, -0.0342,  0.0126,\n",
      "          0.0372,  0.0113,  0.0055, -0.0049,  0.0373,  0.0393, -0.0279, -0.0311,\n",
      "          0.0212, -0.0151, -0.0345, -0.0247, -0.0364, -0.0263, -0.0338,  0.0244,\n",
      "         -0.0016, -0.0081, -0.0125, -0.0441,  0.0409, -0.0331,  0.0354,  0.0263,\n",
      "          0.0165,  0.0270, -0.0129, -0.0019,  0.0304,  0.0117, -0.0283, -0.0303,\n",
      "          0.0265,  0.0220,  0.0414,  0.0188,  0.0095,  0.0012,  0.0275, -0.0188,\n",
      "          0.0290,  0.0154,  0.0421, -0.0171, -0.0216,  0.0322,  0.0303,  0.0156,\n",
      "         -0.0045, -0.0035, -0.0416, -0.0235, -0.0115, -0.0332, -0.0149, -0.0339,\n",
      "         -0.0017, -0.0380, -0.0231,  0.0220,  0.0026,  0.0176,  0.0416,  0.0002,\n",
      "         -0.0073,  0.0433, -0.0053,  0.0163,  0.0263,  0.0195, -0.0114, -0.0286,\n",
      "          0.0145,  0.0277,  0.0003,  0.0180,  0.0309, -0.0371,  0.0355,  0.0181,\n",
      "         -0.0402,  0.0151, -0.0005, -0.0409,  0.0377,  0.0283,  0.0325,  0.0191,\n",
      "          0.0158, -0.0028, -0.0336, -0.0157,  0.0224,  0.0440,  0.0183, -0.0171,\n",
      "         -0.0392,  0.0316, -0.0344, -0.0308, -0.0163,  0.0039, -0.0293, -0.0044,\n",
      "          0.0253,  0.0219,  0.0236,  0.0419,  0.0277, -0.0022, -0.0285,  0.0335,\n",
      "         -0.0086, -0.0198,  0.0153,  0.0382,  0.0033,  0.0438,  0.0037,  0.0017,\n",
      "          0.0222,  0.0137, -0.0145,  0.0095, -0.0010,  0.0228,  0.0295,  0.0122,\n",
      "         -0.0327,  0.0124,  0.0360, -0.0159,  0.0160,  0.0317,  0.0153,  0.0297,\n",
      "          0.0373, -0.0016, -0.0243, -0.0088,  0.0345,  0.0032,  0.0029,  0.0126,\n",
      "         -0.0328,  0.0426,  0.0291, -0.0079, -0.0105,  0.0382,  0.0432, -0.0028,\n",
      "          0.0269,  0.0363, -0.0317, -0.0423,  0.0220,  0.0184,  0.0044, -0.0359,\n",
      "          0.0168, -0.0107,  0.0155,  0.0106,  0.0423,  0.0042,  0.0268, -0.0359,\n",
      "         -0.0182,  0.0415,  0.0090,  0.0034, -0.0094,  0.0415, -0.0172, -0.0223,\n",
      "         -0.0298,  0.0277,  0.0369, -0.0161, -0.0296,  0.0292, -0.0010, -0.0120,\n",
      "          0.0242, -0.0133, -0.0131, -0.0411,  0.0171,  0.0157, -0.0217,  0.0338,\n",
      "          0.0207,  0.0238, -0.0073,  0.0285, -0.0382,  0.0034,  0.0204,  0.0373,\n",
      "         -0.0229, -0.0019, -0.0328, -0.0004, -0.0007, -0.0292, -0.0262, -0.0314,\n",
      "         -0.0386, -0.0424, -0.0181, -0.0139, -0.0331, -0.0286, -0.0279,  0.0146,\n",
      "          0.0123, -0.0125,  0.0325, -0.0226, -0.0371, -0.0068, -0.0089,  0.0129,\n",
      "         -0.0151,  0.0389, -0.0231,  0.0367,  0.0246,  0.0135, -0.0137, -0.0054,\n",
      "          0.0137, -0.0013,  0.0279,  0.0382, -0.0426,  0.0069, -0.0356, -0.0170,\n",
      "          0.0047,  0.0078,  0.0376,  0.0069, -0.0206,  0.0139, -0.0127,  0.0210,\n",
      "         -0.0215, -0.0225,  0.0070, -0.0230,  0.0031, -0.0384, -0.0103, -0.0084,\n",
      "          0.0391,  0.0198,  0.0144, -0.0242,  0.0296,  0.0223,  0.0063,  0.0152,\n",
      "          0.0257,  0.0121,  0.0182,  0.0015, -0.0292,  0.0316,  0.0035,  0.0143,\n",
      "          0.0349, -0.0434,  0.0281, -0.0042,  0.0272, -0.0080, -0.0398,  0.0314,\n",
      "          0.0019, -0.0298, -0.0305,  0.0212, -0.0138, -0.0021,  0.0282, -0.0252,\n",
      "          0.0353,  0.0231, -0.0349, -0.0133,  0.0428, -0.0088,  0.0122, -0.0096,\n",
      "          0.0425, -0.0327, -0.0361,  0.0395,  0.0238, -0.0212,  0.0165,  0.0420,\n",
      "         -0.0209, -0.0113,  0.0117, -0.0244,  0.0381,  0.0121,  0.0235,  0.0378,\n",
      "         -0.0298, -0.0325, -0.0153, -0.0269,  0.0142,  0.0390, -0.0110, -0.0375,\n",
      "          0.0415,  0.0364,  0.0099, -0.0032,  0.0249,  0.0112,  0.0316, -0.0039,\n",
      "          0.0125,  0.0283,  0.0409, -0.0129, -0.0035, -0.0350,  0.0160,  0.0065,\n",
      "          0.0247, -0.0349,  0.0310,  0.0349,  0.0329, -0.0428, -0.0026, -0.0140,\n",
      "          0.0271, -0.0391,  0.0253, -0.0008, -0.0073,  0.0077,  0.0104,  0.0289,\n",
      "         -0.0274, -0.0255, -0.0365, -0.0260,  0.0331,  0.0330,  0.0256,  0.0162,\n",
      "         -0.0200, -0.0011,  0.0349, -0.0168, -0.0167, -0.0424, -0.0102, -0.0246,\n",
      "         -0.0104, -0.0013,  0.0344, -0.0129, -0.0004,  0.0269, -0.0034,  0.0379,\n",
      "          0.0142, -0.0430, -0.0297, -0.0099,  0.0332,  0.0319,  0.0258, -0.0354,\n",
      "          0.0139,  0.0425, -0.0065, -0.0045,  0.0055,  0.0275, -0.0014,  0.0354,\n",
      "          0.0307,  0.0091, -0.0408,  0.0212, -0.0225,  0.0046, -0.0026, -0.0182,\n",
      "         -0.0006,  0.0332,  0.0320, -0.0029,  0.0310,  0.0014, -0.0023,  0.0428,\n",
      "         -0.0140,  0.0240,  0.0237,  0.0417,  0.0101,  0.0152, -0.0381, -0.0179,\n",
      "         -0.0086, -0.0282, -0.0014, -0.0169, -0.0200,  0.0016, -0.0097,  0.0098,\n",
      "          0.0245, -0.0297, -0.0384,  0.0378, -0.0391, -0.0263,  0.0022,  0.0275,\n",
      "          0.0371,  0.0211, -0.0244, -0.0056,  0.0029, -0.0234, -0.0020, -0.0335]])\n",
      "---> tensor([[-8.2055e-03, -8.9916e-02, -2.3102e-02, -9.2583e-03, -4.6609e-02,\n",
      "         -5.6599e-02, -5.2021e-02, -1.1702e-02,  8.4207e-02,  1.4667e-02,\n",
      "          2.7710e-02, -2.5129e-02,  9.3671e-02,  1.1782e-02, -6.7004e-02,\n",
      "          1.3353e-02, -4.1922e-02, -2.1349e-02,  1.7414e-02,  4.4165e-04,\n",
      "          3.5523e-02, -6.4939e-02, -5.8698e-02, -1.5628e-02, -3.0501e-02,\n",
      "          1.2056e-02, -3.2917e-02,  2.3250e-02, -5.6292e-03,  1.0172e-01,\n",
      "         -1.0352e-02,  9.4312e-03, -3.3935e-02,  2.3935e-02, -4.1243e-02,\n",
      "         -2.6297e-02, -2.4764e-02, -2.1080e-02,  1.1098e-03, -4.1733e-02,\n",
      "          3.2109e-02,  4.8729e-04,  2.3334e-02, -1.0921e-02, -8.4407e-02,\n",
      "         -4.1982e-02,  2.2746e-02, -6.5828e-02, -4.8967e-02, -1.0354e-02,\n",
      "          5.4851e-02,  2.0364e-02, -1.7237e-02, -7.7973e-02, -7.1889e-03,\n",
      "          8.3079e-02, -2.6450e-02,  9.0554e-02,  8.6839e-02, -4.5175e-02,\n",
      "         -1.4786e-02, -6.3941e-02,  8.3734e-03,  6.3961e-02,  5.2525e-02,\n",
      "         -3.6109e-02,  7.0502e-02,  2.7895e-02,  1.0680e-01,  8.4193e-03,\n",
      "          8.4340e-02, -1.3416e-01, -1.3820e-01, -2.8374e-02, -1.0693e-01,\n",
      "         -2.6634e-02,  9.1599e-02, -7.7958e-03,  1.1776e-01, -5.3280e-02,\n",
      "          8.1853e-02,  1.5665e-02, -2.2120e-02, -6.3057e-02, -1.4994e-03,\n",
      "         -1.5167e-02, -7.2206e-02, -3.8553e-03,  1.2741e-01,  2.3399e-02,\n",
      "          3.7413e-02, -1.0060e-01, -2.6882e-02,  3.5328e-02, -7.4257e-02,\n",
      "          1.0528e-01, -4.4003e-02, -8.1153e-02,  1.2908e-01, -2.4883e-02,\n",
      "          1.4607e-02,  7.9773e-02, -8.7537e-02, -1.1477e-01, -9.5164e-02,\n",
      "          8.7599e-02, -5.6722e-02, -5.0034e-02, -4.7199e-03,  1.9732e-02,\n",
      "         -6.2507e-02,  1.2088e-01,  5.3048e-02, -1.9382e-02,  5.7302e-02,\n",
      "          2.1641e-02,  2.1382e-02,  1.1080e-01,  8.5649e-02,  7.6431e-02,\n",
      "          3.1218e-02, -4.2648e-02, -8.7931e-02,  4.1780e-02, -6.9094e-03,\n",
      "         -1.0564e-02, -2.2363e-02, -1.6423e-01,  1.8103e-02,  7.4164e-03,\n",
      "          4.6351e-02,  8.8558e-02, -8.7543e-02,  6.4117e-02, -6.5793e-02,\n",
      "         -1.2293e-01,  3.4788e-02,  7.3480e-02, -2.6761e-02, -8.9025e-02,\n",
      "         -2.8718e-02, -5.3543e-02,  1.7206e-02,  5.9091e-02,  1.6237e-02,\n",
      "          2.1192e-03, -7.2952e-04, -1.2526e-02, -2.0872e-02,  8.0468e-02,\n",
      "          6.0741e-02,  2.0711e-02,  3.5500e-02,  2.2481e-02,  7.7838e-02,\n",
      "         -1.4529e-02, -4.2616e-02, -2.4080e-02,  5.6322e-02,  6.4884e-02,\n",
      "          7.2610e-03, -1.8690e-03,  1.0984e-01, -9.0276e-02, -4.1349e-02,\n",
      "         -4.8201e-03,  5.4795e-02,  1.1250e-02,  7.0885e-02, -5.5455e-02,\n",
      "          1.6845e-02,  2.5504e-02,  1.9738e-02, -2.7629e-02,  2.5799e-02,\n",
      "          1.1719e-02,  6.4878e-03, -8.1147e-03, -1.5154e-02, -7.4269e-03,\n",
      "          6.0970e-05,  6.1496e-02, -8.4880e-02, -4.7262e-02,  5.9849e-02,\n",
      "         -9.9724e-02,  1.8783e-02,  1.5268e-01,  1.7992e-01, -6.6867e-02,\n",
      "          5.2309e-02, -1.8550e-01,  6.2022e-02,  1.0213e-01, -7.1519e-02,\n",
      "          3.7719e-02, -6.3864e-02,  5.1338e-02, -7.3215e-02, -8.9447e-03,\n",
      "          8.0304e-03,  1.1840e-02,  3.4361e-02,  1.7778e-02,  2.9544e-02,\n",
      "          1.2393e-01,  2.3531e-02,  1.6409e-02, -3.5182e-02, -6.2313e-02,\n",
      "         -1.4892e-01,  1.8942e-02, -6.9514e-02,  2.5299e-02,  3.7961e-02,\n",
      "          1.6865e-02, -5.1052e-02, -3.6047e-02,  3.0311e-02,  1.3446e-02,\n",
      "          1.4445e-01,  2.0861e-02,  8.8970e-03,  5.8427e-02, -1.4490e-01,\n",
      "         -1.6843e-02, -2.7236e-02, -1.0911e-01, -5.5961e-02,  1.2596e-01,\n",
      "         -8.2943e-02,  7.8433e-02, -5.4151e-02, -5.7845e-02,  4.9675e-02,\n",
      "          1.5362e-01, -3.3184e-02, -4.2138e-02, -4.2523e-02, -8.8699e-04,\n",
      "         -6.6449e-02, -4.2348e-02,  1.1466e-01,  6.7322e-02,  9.5331e-02,\n",
      "          7.8677e-02, -8.0296e-02,  1.5753e-02, -4.7345e-02,  3.6295e-02,\n",
      "         -3.5379e-02,  1.8934e-02,  1.6724e-02,  1.2023e-01, -4.1865e-02,\n",
      "          7.2479e-02,  5.1235e-02,  6.5515e-02,  4.7579e-02,  5.5885e-02,\n",
      "          6.2633e-02, -1.2549e-02, -8.6365e-02,  9.3203e-03,  7.9140e-02,\n",
      "         -6.7344e-03, -4.3676e-02, -2.4988e-02,  5.9085e-02, -1.0093e-01,\n",
      "         -8.4647e-03,  7.8279e-03,  2.9192e-02, -2.3352e-02,  3.6132e-02,\n",
      "         -4.4926e-02,  4.5457e-02,  3.2062e-02, -5.5030e-03,  2.8305e-02,\n",
      "          5.9219e-02,  2.7972e-02,  1.4386e-01,  2.9679e-03, -6.1644e-03,\n",
      "         -1.9743e-02, -1.1385e-01,  2.5661e-02,  5.5290e-02,  9.8566e-02,\n",
      "         -1.6673e-02,  1.9173e-02, -6.4964e-02,  5.6480e-02,  6.2034e-02,\n",
      "          6.4106e-02, -3.2906e-02,  4.1693e-02,  3.1575e-02, -2.7367e-03,\n",
      "         -8.4251e-02,  1.5452e-02, -1.0963e-01, -7.1799e-02, -8.2344e-02,\n",
      "          4.8674e-02,  3.4450e-02, -7.6948e-02,  6.2749e-02,  1.3194e-01,\n",
      "         -1.2412e-01,  5.8889e-03, -2.1738e-02, -5.6936e-02, -2.9923e-02,\n",
      "          1.8148e-02,  5.5008e-02,  3.2533e-02, -8.8415e-03, -5.0180e-02,\n",
      "          1.9999e-02, -6.5537e-02, -4.2325e-02,  5.7099e-02,  3.6234e-03,\n",
      "         -4.3815e-02, -2.9127e-02,  9.0508e-02,  3.5477e-02, -5.9433e-06,\n",
      "         -2.8281e-02, -4.6971e-02,  3.6803e-02, -8.1195e-02,  4.8984e-03,\n",
      "          1.8697e-02, -5.0327e-02, -7.8342e-02, -6.9173e-02,  1.5535e-02,\n",
      "          1.0700e-01, -5.1834e-02, -7.8470e-02,  1.3379e-02, -1.0019e-01,\n",
      "          7.1905e-03, -6.5337e-02, -4.3100e-02,  1.0694e-01, -6.4211e-02,\n",
      "         -2.3106e-02,  1.2119e-02,  1.2432e-02,  1.5770e-02, -1.0809e-01,\n",
      "         -3.7865e-02, -5.0320e-04,  8.1648e-02, -3.5775e-02,  5.3693e-02,\n",
      "          5.3942e-02, -6.9131e-02,  5.9774e-02, -1.8966e-02,  4.0870e-02,\n",
      "          1.2555e-01,  1.6097e-02, -6.5302e-02, -2.4653e-02,  8.1272e-02,\n",
      "         -1.2842e-01,  1.0242e-01,  1.4133e-01,  2.0266e-02, -9.4613e-02,\n",
      "         -8.9805e-02, -9.3870e-02, -1.0370e-01, -2.3049e-02,  1.1007e-01,\n",
      "         -4.2328e-02, -3.7174e-03,  2.6768e-02,  2.6066e-02,  1.0310e-01,\n",
      "         -2.3951e-02,  1.2174e-02,  7.8583e-02, -2.6968e-02, -2.7525e-02,\n",
      "          2.3737e-02,  1.0804e-02,  5.5876e-02, -3.8419e-02, -2.9534e-02,\n",
      "         -2.6646e-02, -7.8599e-02, -9.6348e-02,  2.1257e-02, -6.1682e-03,\n",
      "          2.7473e-02,  2.3895e-02, -2.1318e-01, -4.9434e-02,  3.7012e-04,\n",
      "          6.5264e-02, -5.4114e-02,  6.5290e-02, -9.0460e-02,  5.4992e-03,\n",
      "          4.4314e-02, -7.3659e-02, -8.3462e-04, -7.3644e-02,  4.3010e-02,\n",
      "         -1.0416e-01, -3.0873e-02,  8.4828e-02, -8.1526e-03, -1.5735e-02,\n",
      "          1.0093e-01, -8.7351e-02,  4.4875e-02, -7.9769e-02, -2.6618e-02,\n",
      "         -4.2843e-02,  6.3045e-02,  8.3219e-02,  1.2617e-02,  8.0536e-02,\n",
      "          1.8599e-02,  2.5897e-02, -9.5926e-02, -1.4178e-02,  1.3222e-01,\n",
      "          6.1668e-03,  2.6690e-02, -1.1842e-01,  1.9027e-02,  4.7970e-02,\n",
      "          3.2122e-02, -5.8676e-03,  8.0067e-02,  5.3481e-02, -2.7207e-02,\n",
      "          1.0722e-01,  1.7099e-02,  3.5476e-03, -6.7616e-02,  3.2104e-02,\n",
      "          1.1207e-01, -4.3078e-02, -5.1691e-02,  1.0940e-01, -9.0550e-02,\n",
      "         -1.6357e-02,  2.2462e-02, -2.8700e-02, -5.2113e-02,  2.8234e-02,\n",
      "         -1.3835e-02, -1.5025e-02, -3.5389e-02, -1.3532e-02,  1.8446e-02,\n",
      "          6.9489e-03, -3.4788e-02,  2.3421e-02, -2.9380e-02, -5.8540e-03,\n",
      "         -5.1343e-02,  4.7448e-02, -9.7707e-02, -2.7016e-02,  2.0604e-02,\n",
      "          5.9207e-03, -5.6854e-02,  5.0268e-02,  2.9127e-02, -4.4517e-03,\n",
      "          1.6225e-01,  8.2556e-02,  1.7437e-02, -4.7232e-02, -1.6956e-02,\n",
      "         -8.6120e-03, -2.5179e-02, -3.6585e-02, -1.2480e-02,  1.9066e-02,\n",
      "         -3.1737e-02,  9.4139e-03, -7.2766e-02, -7.5901e-02, -3.6120e-02,\n",
      "         -1.0790e-01,  6.4359e-02,  5.9431e-02, -1.0682e-02, -4.4509e-02,\n",
      "         -5.6747e-02,  1.2505e-02, -9.8474e-02,  1.6725e-02,  2.6181e-02,\n",
      "         -4.7916e-03, -1.7029e-03, -1.1962e-01,  6.2596e-04, -7.9247e-02,\n",
      "          2.0234e-02,  2.2450e-02]])\n",
      "--- torch.Size([10, 256]) tensor([[ 0.0449,  0.0513, -0.0122,  ..., -0.0397, -0.0069, -0.0528],\n",
      "        [ 0.0215,  0.0481,  0.0334,  ..., -0.0197, -0.0185,  0.0181],\n",
      "        [-0.0045,  0.0122,  0.0602,  ..., -0.0288,  0.0392,  0.0515],\n",
      "        ...,\n",
      "        [-0.0492, -0.0446,  0.0016,  ...,  0.0132, -0.0476, -0.0348],\n",
      "        [-0.0420,  0.0354, -0.0384,  ..., -0.0141,  0.0175, -0.0386],\n",
      "        [-0.0195,  0.0351,  0.0617,  ..., -0.0139, -0.0536, -0.0568]])\n",
      "---> tensor([[ 0.0391,  0.0591,  0.0175,  ..., -0.1580, -0.0523,  0.0393],\n",
      "        [ 0.0090, -0.0202,  0.0296,  ...,  0.1158, -0.0501,  0.0428],\n",
      "        [ 0.0176, -0.0413,  0.0174,  ..., -0.1442,  0.0177, -0.1501],\n",
      "        ...,\n",
      "        [ 0.0364, -0.0645, -0.0321,  ..., -0.0367, -0.1187,  0.0166],\n",
      "        [ 0.0329, -0.0235, -0.0650,  ...,  0.0805,  0.0721, -0.0888],\n",
      "        [ 0.0379, -0.0586, -0.0090,  ...,  0.0056,  0.0052,  0.0813]])\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  app.launch_new_instance()\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "Embedding(109683, 300)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 16
    }
   ],
   "source": [
    "#创建模型\n",
    "wordEmbedding = torch.FloatTensor(wordEmbedding)\n",
    "num_embeddings = len(word2vec.wv.index2word)\n",
    "model = HAN(num_embeddings, \n",
    "            num_classes = 6, # 0 没用到 1~5\n",
    "            embedding_dim=word2vec.wv.vector_size, \n",
    "            num_words = 100,\n",
    "            hidden_size_gru = 128,\n",
    "            hidden_size_att = 512,\n",
    "            )\n",
    "print(model)\n",
    "\n",
    "modelParams = model.parameters()\n",
    "for param in modelParams:\n",
    "    if len(param.data.shape) > 1:\n",
    "        print('---', param.data.shape, param.data)\n",
    "        torch.nn.init.kaiming_normal(param.data)\n",
    "        print('--->', param.data)\n",
    "        \n",
    "model.embed.from_pretrained(wordEmbedding)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 开始训练过程"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "Epoch 0, 99/634, loss:2.077094 \n",
      "Epoch 0, 199/634, loss:2.086586 \n",
      "Epoch 0, 299/634, loss:2.046643 \n",
      "Epoch 0, 399/634, loss:2.064200 \n",
      "Epoch 0, 499/634, loss:2.061777 \n",
      "Epoch 0, 599/634, loss:2.058427 \n",
      "Epoch 0 cost time: 111.886s\n",
      "Epoch Test 0, 49/112, \n",
      "predicted:tensor([1, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3,\n",
      "        1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3, 1, 1, 3,\n",
      "        1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3,\n",
      "        3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3,\n",
      "        3, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3,\n",
      "        3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3], device='cuda:0'), \n",
      "target:tensor([3, 3, 3, 3, 1, 1, 4, 2, 3, 4, 1, 3, 2, 1, 3, 1, 2, 3, 3, 3, 3, 1, 3, 1,\n",
      "        4, 2, 3, 2, 5, 3, 3, 3, 2, 2, 1, 2, 3, 4, 1, 3, 3, 1, 1, 4, 3, 1, 1, 4,\n",
      "        4, 3, 2, 3, 3, 1, 4, 2, 3, 2, 5, 2, 2, 4, 4, 3, 3, 1, 3, 4, 4, 1, 1, 3,\n",
      "        2, 4, 3, 3, 1, 4, 1, 1, 3, 3, 1, 4, 4, 3, 4, 4, 4, 2, 3, 1, 2, 2, 4, 4,\n",
      "        3, 2, 3, 4, 3, 3, 4, 3, 3, 3, 1, 3, 2, 2, 2, 3, 5, 2, 3, 2, 2, 1, 3, 3,\n",
      "        2, 4, 3, 3, 1, 4, 2, 1, 4, 4, 3, 1, 4, 1, 1, 4, 3, 1, 1, 3, 3, 4, 1, 3,\n",
      "        3, 1, 3, 3, 1, 3, 2, 4, 3, 3, 3, 3, 3, 1, 2, 3, 2, 3, 1, 3, 1, 4, 3, 3,\n",
      "        1, 2, 1, 2, 1, 3, 4, 1, 3, 3, 1, 3, 1, 4, 2, 1, 4, 3, 3, 4, 4, 3, 4, 3,\n",
      "        3, 4, 1, 3, 1, 1, 3, 1, 1, 2, 1, 3, 4, 3, 1, 2, 1, 3, 3, 2, 3, 3, 3, 3,\n",
      "        2, 1, 3, 2, 3, 2, 3, 3, 3, 3, 4, 2, 1, 4, 4, 3, 1, 3, 1, 3, 1, 4, 4, 3,\n",
      "        3, 5, 1, 5, 2, 3, 4, 3, 3, 1, 3, 1, 2, 4, 4, 3], device='cuda:0')\n",
      "Epoch Test 0, 99/112, \n",
      "predicted:tensor([1, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        1, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1,\n",
      "        3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3, 3,\n",
      "        1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1,\n",
      "        3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        1, 3, 3, 3, 1, 3, 1, 1, 3, 3, 3, 3, 1, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3,\n",
      "        1, 1, 3, 3, 3, 3, 1, 3, 3, 1, 1, 3, 3, 3, 3, 3], device='cuda:0'), \n",
      "target:tensor([4, 1, 3, 3, 1, 2, 3, 4, 1, 3, 4, 2, 1, 2, 4, 4, 3, 4, 2, 3, 2, 3, 3, 3,\n",
      "        1, 3, 3, 1, 3, 1, 3, 3, 3, 4, 2, 1, 3, 1, 4, 3, 2, 2, 3, 2, 2, 2, 2, 1,\n",
      "        3, 2, 1, 3, 1, 4, 2, 4, 1, 1, 1, 1, 3, 3, 4, 3, 2, 2, 4, 3, 3, 4, 3, 2,\n",
      "        3, 3, 4, 2, 1, 1, 5, 4, 3, 4, 4, 3, 1, 4, 1, 1, 3, 3, 1, 2, 1, 5, 1, 3,\n",
      "        3, 1, 4, 2, 4, 1, 5, 1, 1, 3, 3, 2, 2, 1, 1, 1, 2, 4, 3, 2, 3, 3, 3, 3,\n",
      "        3, 3, 2, 1, 2, 3, 4, 4, 2, 3, 3, 1, 4, 2, 3, 3, 1, 3, 1, 3, 2, 1, 3, 1,\n",
      "        3, 3, 3, 3, 1, 2, 3, 3, 1, 3, 1, 3, 3, 1, 4, 1, 3, 2, 4, 3, 2, 4, 3, 2,\n",
      "        2, 2, 1, 3, 1, 3, 3, 3, 3, 2, 4, 3, 1, 4, 4, 3, 3, 3, 3, 4, 1, 4, 1, 4,\n",
      "        1, 2, 2, 1, 4, 3, 4, 1, 2, 4, 2, 3, 3, 1, 3, 2, 3, 1, 3, 1, 1, 3, 3, 3,\n",
      "        1, 3, 3, 3, 5, 4, 5, 2, 1, 2, 1, 2, 4, 1, 2, 1, 3, 3, 1, 1, 2, 4, 1, 3,\n",
      "        1, 3, 3, 3, 1, 3, 3, 3, 1, 3, 1, 2, 1, 1, 2, 4], device='cuda:0')\n",
      "Epoch Test 0 cost time: 5.890s\n",
      "准确率： 0.374\n",
      "Epoch 1, 99/634, loss:2.050238 \n",
      "Epoch 1, 199/634, loss:2.055469 \n",
      "Epoch 1, 299/634, loss:2.035779 \n",
      "Epoch 1, 399/634, loss:2.058074 \n",
      "Epoch 1, 499/634, loss:1.985005 \n",
      "Epoch 1, 599/634, loss:2.076848 \n",
      "Epoch 1 cost time: 110.504s\n",
      "Epoch Test 1, 49/112, \n",
      "predicted:tensor([3, 3, 1, 1, 3, 1, 1, 1, 1, 3, 1, 1, 3, 1, 3, 1, 1, 3, 1, 1, 3, 3, 3, 1,\n",
      "        3, 3, 3, 1, 1, 1, 1, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 1, 3, 1, 1,\n",
      "        1, 1, 3, 3, 3, 1, 3, 1, 1, 3, 1, 3, 3, 1, 1, 1, 3, 1, 1, 3, 3, 3, 3, 1,\n",
      "        3, 3, 1, 3, 1, 1, 1, 1, 3, 3, 3, 3, 3, 1, 1, 1, 3, 1, 1, 3, 1, 1, 1, 3,\n",
      "        1, 3, 3, 3, 1, 3, 1, 1, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 1, 3, 1, 1,\n",
      "        3, 1, 1, 1, 1, 1, 3, 1, 1, 3, 1, 1, 1, 1, 3, 1, 3, 1, 1, 3, 1, 3, 3, 3,\n",
      "        3, 1, 3, 1, 1, 3, 1, 3, 1, 3, 3, 3, 1, 1, 1, 1, 3, 1, 1, 3, 3, 1, 3, 3,\n",
      "        3, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 3, 1, 3, 3, 3, 3, 1,\n",
      "        3, 3, 1, 3, 1, 3, 3, 3, 1, 3, 3, 1, 3, 1, 3, 1, 3, 1, 1, 3, 3, 3, 3, 3,\n",
      "        1, 3, 3, 3, 1, 3, 1, 3, 1, 3, 3, 1, 1, 3, 1, 3, 3, 1, 3, 3, 3, 1, 3, 3,\n",
      "        3, 3, 1, 1, 1, 3, 1, 1, 3, 1, 3, 1, 3, 3, 3, 3], device='cuda:0'), \n",
      "target:tensor([3, 1, 3, 3, 3, 1, 2, 1, 1, 2, 4, 1, 4, 2, 1, 1, 0, 2, 1, 2, 1, 1, 3, 3,\n",
      "        4, 3, 3, 1, 2, 3, 3, 3, 1, 1, 3, 4, 3, 1, 3, 2, 4, 4, 1, 3, 2, 1, 2, 3,\n",
      "        2, 2, 1, 3, 1, 1, 2, 1, 1, 3, 2, 3, 3, 4, 3, 3, 4, 2, 1, 4, 2, 3, 3, 1,\n",
      "        3, 1, 3, 2, 1, 3, 4, 1, 1, 1, 1, 4, 4, 1, 4, 3, 3, 2, 4, 3, 3, 4, 1, 2,\n",
      "        1, 3, 4, 4, 3, 1, 2, 0, 1, 4, 1, 3, 4, 1, 1, 3, 3, 4, 2, 3, 1, 2, 1, 5,\n",
      "        5, 1, 4, 3, 0, 2, 3, 3, 2, 3, 1, 3, 2, 1, 3, 2, 1, 0, 2, 4, 4, 3, 3, 1,\n",
      "        3, 3, 2, 3, 1, 3, 3, 2, 2, 2, 4, 3, 2, 2, 1, 3, 3, 3, 2, 3, 2, 1, 3, 3,\n",
      "        2, 2, 5, 3, 4, 1, 3, 2, 1, 1, 3, 2, 5, 2, 2, 4, 1, 3, 3, 1, 2, 1, 2, 3,\n",
      "        5, 5, 2, 3, 2, 3, 4, 4, 3, 3, 3, 1, 1, 1, 3, 3, 4, 3, 4, 4, 4, 4, 4, 2,\n",
      "        2, 4, 1, 1, 3, 1, 1, 3, 1, 3, 3, 1, 2, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 3,\n",
      "        3, 4, 3, 2, 3, 3, 2, 2, 3, 4, 2, 4, 3, 4, 3, 3], device='cuda:0')\n",
      "Epoch Test 1, 99/112, \n",
      "predicted:tensor([3, 1, 1, 3, 3, 1, 1, 1, 3, 1, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3,\n",
      "        1, 3, 3, 3, 1, 3, 1, 3, 1, 1, 3, 1, 3, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 3, 3, 3, 3, 1, 1, 3, 3, 1, 1, 3, 1, 3, 1, 3, 1, 3, 3, 3, 3, 1, 3, 3,\n",
      "        3, 3, 3, 1, 1, 1, 3, 1, 1, 1, 3, 3, 1, 3, 1, 3, 1, 3, 3, 3, 3, 1, 3, 3,\n",
      "        3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3,\n",
      "        3, 1, 3, 3, 3, 3, 3, 1, 3, 1, 1, 3, 1, 1, 3, 1, 1, 3, 1, 3, 1, 3, 1, 1,\n",
      "        3, 3, 3, 1, 3, 3, 3, 1, 3, 1, 1, 3, 3, 1, 3, 3, 1, 1, 1, 1, 3, 3, 3, 3,\n",
      "        3, 1, 1, 1, 1, 3, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 1, 3, 1, 3, 3, 1, 3, 3,\n",
      "        1, 3, 1, 3, 1, 3, 3, 1, 3, 1, 3, 1, 3, 3, 3, 3, 3, 1, 1, 1, 3, 1, 1, 3,\n",
      "        1, 3, 1, 3, 1, 3, 1, 1, 1, 1, 3, 3, 3, 3, 1, 3, 1, 3, 1, 1, 3, 3, 1, 3,\n",
      "        3, 1, 3, 1, 3, 1, 1, 3, 3, 3, 3, 1, 3, 3, 1, 3], device='cuda:0'), \n",
      "target:tensor([5, 2, 1, 1, 4, 5, 1, 2, 1, 3, 3, 3, 3, 1, 3, 3, 4, 1, 1, 1, 1, 2, 3, 3,\n",
      "        4, 3, 3, 2, 4, 4, 1, 4, 3, 3, 3, 1, 3, 1, 4, 3, 1, 3, 1, 4, 1, 2, 3, 1,\n",
      "        2, 4, 3, 1, 1, 5, 4, 1, 3, 3, 2, 3, 1, 3, 2, 4, 3, 4, 4, 3, 1, 4, 4, 4,\n",
      "        2, 3, 4, 3, 3, 5, 5, 1, 2, 2, 1, 4, 2, 3, 1, 1, 3, 3, 2, 3, 2, 1, 1, 2,\n",
      "        3, 2, 3, 4, 1, 2, 4, 2, 3, 3, 1, 1, 2, 3, 1, 3, 4, 4, 1, 4, 3, 3, 3, 3,\n",
      "        4, 1, 1, 3, 2, 2, 2, 1, 4, 4, 2, 4, 1, 4, 3, 1, 1, 2, 1, 3, 2, 3, 5, 3,\n",
      "        1, 2, 4, 1, 3, 3, 1, 3, 3, 1, 4, 3, 3, 4, 4, 3, 4, 3, 2, 2, 3, 3, 1, 3,\n",
      "        3, 1, 3, 2, 2, 3, 2, 3, 1, 3, 1, 3, 1, 3, 2, 2, 1, 1, 3, 3, 4, 3, 3, 4,\n",
      "        1, 1, 3, 3, 2, 3, 3, 3, 1, 1, 1, 2, 4, 3, 2, 4, 1, 4, 4, 2, 1, 1, 3, 2,\n",
      "        1, 3, 1, 4, 3, 3, 1, 3, 2, 3, 3, 3, 4, 5, 1, 5, 4, 3, 2, 4, 4, 1, 1, 1,\n",
      "        3, 2, 2, 4, 2, 2, 3, 4, 3, 5, 4, 4, 3, 3, 3, 2], device='cuda:0')\n",
      "Epoch Test 1 cost time: 5.182s\n",
      "准确率： 0.416\n",
      "Epoch 2, 99/634, loss:1.986974 \n",
      "Epoch 2, 199/634, loss:2.054783 \n",
      "Epoch 2, 299/634, loss:2.013828 \n",
      "Epoch 2, 399/634, loss:2.027976 \n",
      "Epoch 2, 499/634, loss:2.003831 \n",
      "Epoch 2, 599/634, loss:2.027282 \n",
      "Epoch 2 cost time: 109.432s\n",
      "Epoch Test 2, 49/112, \n",
      "predicted:tensor([1, 3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 1,\n",
      "        3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 1, 3, 3, 3, 3, 1, 3, 3, 3,\n",
      "        3, 1, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 3, 1, 3, 3, 1,\n",
      "        1, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 3, 3, 3, 1, 3, 3,\n",
      "        3, 1, 3, 3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 1, 3, 1, 3,\n",
      "        3, 3, 1, 3, 3, 1, 3, 1, 1, 3, 3, 1, 3, 1, 1, 1, 3, 1, 1, 3, 3, 1, 3, 3,\n",
      "        1, 3, 3, 3, 1, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 1, 1, 3, 1, 1, 3, 3, 1, 3,\n",
      "        3, 1, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 3, 3, 1, 3, 3, 1,\n",
      "        3, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1,\n",
      "        3, 3, 3, 3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 1], device='cuda:0'), \n",
      "target:tensor([1, 4, 3, 3, 1, 3, 1, 3, 4, 2, 3, 1, 1, 3, 4, 1, 3, 3, 4, 2, 3, 4, 1, 3,\n",
      "        1, 3, 5, 3, 3, 4, 3, 3, 3, 3, 4, 2, 1, 4, 3, 5, 2, 2, 3, 1, 4, 1, 3, 4,\n",
      "        5, 2, 3, 3, 1, 3, 1, 4, 1, 5, 1, 1, 3, 3, 3, 3, 4, 1, 3, 4, 1, 3, 3, 3,\n",
      "        1, 4, 3, 1, 2, 3, 1, 3, 4, 3, 1, 3, 3, 1, 1, 3, 2, 3, 4, 3, 3, 4, 4, 2,\n",
      "        1, 3, 3, 3, 4, 2, 1, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 1, 4, 1,\n",
      "        1, 2, 3, 3, 3, 1, 1, 3, 3, 4, 4, 2, 3, 3, 3, 1, 1, 2, 3, 3, 3, 3, 1, 5,\n",
      "        1, 3, 1, 2, 3, 1, 4, 2, 1, 1, 3, 1, 3, 1, 1, 2, 1, 2, 1, 3, 1, 1, 3, 3,\n",
      "        1, 3, 3, 1, 3, 1, 4, 4, 4, 3, 3, 3, 3, 4, 3, 1, 2, 4, 3, 2, 3, 4, 3, 1,\n",
      "        2, 2, 3, 3, 2, 1, 2, 1, 4, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 1, 2, 2, 1, 3,\n",
      "        3, 1, 3, 1, 3, 4, 3, 1, 2, 2, 1, 2, 3, 2, 5, 2, 2, 1, 3, 1, 3, 5, 2, 1,\n",
      "        3, 3, 1, 3, 1, 2, 4, 3, 2, 3, 1, 2, 2, 4, 4, 1], device='cuda:0')\n",
      "Epoch Test 2, 99/112, \n",
      "predicted:tensor([3, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 1, 1, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3, 3, 3, 1, 1, 1, 1, 1, 3, 1, 3, 1,\n",
      "        3, 1, 3, 1, 3, 1, 1, 3, 3, 1, 3, 1, 3, 1, 3, 3, 1, 3, 3, 3, 3, 1, 1, 1,\n",
      "        3, 3, 1, 3, 3, 1, 3, 1, 3, 3, 1, 3, 3, 3, 3, 1, 3, 1, 3, 1, 3, 1, 1, 3,\n",
      "        1, 3, 3, 3, 1, 1, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3, 3, 3, 1, 3,\n",
      "        3, 3, 3, 3, 3, 1, 1, 3, 3, 1, 1, 3, 3, 1, 3, 3, 1, 3, 3, 3, 1, 1, 1, 3,\n",
      "        3, 3, 1, 3, 3, 3, 1, 1, 1, 1, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3,\n",
      "        3, 1, 1, 3, 3, 1, 3, 3, 3, 1, 1, 3, 1, 3, 3, 3, 1, 3, 3, 1, 3, 3, 1, 3,\n",
      "        1, 3, 1, 3, 1, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 3, 3, 1, 3, 3, 3, 1, 1, 3,\n",
      "        3, 3, 1, 3, 1, 1, 1, 3, 3, 1, 3, 1, 1, 3, 1, 1, 3, 3, 1, 1, 3, 3, 3, 3,\n",
      "        1, 1, 3, 1, 1, 3, 3, 3, 3, 1, 1, 3, 3, 3, 1, 1], device='cuda:0'), \n",
      "target:tensor([2, 3, 3, 1, 2, 3, 4, 2, 2, 1, 1, 3, 3, 3, 1, 2, 2, 4, 3, 2, 2, 3, 2, 3,\n",
      "        1, 1, 1, 3, 1, 2, 4, 2, 4, 2, 2, 3, 2, 1, 3, 1, 4, 2, 2, 1, 1, 4, 3, 1,\n",
      "        3, 2, 4, 1, 3, 1, 1, 1, 1, 1, 3, 2, 3, 1, 4, 3, 2, 5, 4, 3, 3, 1, 2, 2,\n",
      "        4, 3, 1, 1, 2, 3, 3, 1, 3, 2, 1, 3, 1, 1, 3, 4, 5, 3, 4, 1, 4, 1, 1, 3,\n",
      "        2, 3, 1, 1, 1, 3, 4, 3, 1, 3, 4, 3, 1, 3, 3, 4, 3, 3, 3, 3, 4, 3, 1, 3,\n",
      "        3, 4, 1, 3, 1, 2, 3, 3, 3, 1, 1, 3, 3, 1, 4, 1, 2, 4, 2, 1, 2, 3, 1, 5,\n",
      "        1, 2, 1, 3, 3, 3, 1, 1, 3, 4, 1, 1, 3, 4, 1, 3, 3, 1, 4, 3, 1, 4, 1, 1,\n",
      "        1, 2, 1, 2, 1, 4, 3, 3, 3, 3, 4, 1, 3, 3, 3, 3, 2, 4, 3, 1, 3, 4, 3, 2,\n",
      "        1, 2, 1, 3, 1, 1, 1, 4, 1, 2, 1, 1, 1, 2, 1, 2, 4, 2, 3, 1, 2, 2, 1, 2,\n",
      "        4, 1, 1, 1, 2, 3, 1, 1, 2, 3, 2, 4, 1, 3, 1, 3, 3, 3, 3, 2, 5, 3, 4, 4,\n",
      "        1, 1, 3, 2, 1, 4, 4, 3, 3, 3, 2, 3, 3, 3, 2, 1], device='cuda:0')\n",
      "Epoch Test 2 cost time: 5.001s\n",
      "准确率： 0.423\n",
      "Epoch 3, 99/634, loss:1.973645 \n",
      "Epoch 3, 199/634, loss:1.995101 \n",
      "Epoch 3, 299/634, loss:2.011524 \n",
      "Epoch 3, 399/634, loss:2.019970 \n",
      "Epoch 3, 499/634, loss:2.002741 \n",
      "Epoch 3, 599/634, loss:1.962294 \n",
      "Epoch 3 cost time: 109.658s\n",
      "Epoch Test 3, 49/112, \n",
      "predicted:tensor([1, 3, 1, 3, 3, 1, 1, 3, 3, 3, 1, 1, 1, 1, 3, 1, 1, 1, 3, 3, 3, 3, 3, 3,\n",
      "        1, 1, 1, 1, 3, 3, 3, 3, 1, 3, 3, 1, 1, 1, 3, 3, 1, 1, 1, 3, 3, 3, 1, 3,\n",
      "        1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 1, 1, 3, 1, 1, 1, 1,\n",
      "        1, 3, 3, 3, 3, 3, 3, 1, 1, 3, 1, 1, 3, 3, 3, 1, 3, 1, 1, 3, 1, 3, 1, 3,\n",
      "        1, 1, 3, 1, 3, 3, 3, 3, 1, 1, 1, 1, 3, 3, 3, 1, 3, 1, 3, 1, 3, 3, 3, 1,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 1, 1, 1, 3, 3, 1, 1, 3,\n",
      "        3, 3, 3, 1, 3, 1, 1, 3, 3, 3, 3, 1, 3, 3, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1,\n",
      "        3, 3, 1, 1, 3, 1, 1, 1, 3, 1, 1, 3, 1, 3, 1, 3, 3, 3, 3, 3, 1, 3, 1, 3,\n",
      "        1, 1, 3, 3, 1, 1, 1, 3, 3, 3, 1, 1, 1, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3, 1,\n",
      "        3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 3, 1,\n",
      "        1, 1, 3, 1, 1, 1, 3, 3, 3, 3, 3, 1, 3, 1, 1, 1], device='cuda:0'), \n",
      "target:tensor([1, 3, 2, 3, 3, 3, 2, 3, 1, 3, 1, 4, 1, 1, 3, 1, 1, 3, 2, 3, 5, 3, 3, 2,\n",
      "        1, 3, 3, 2, 3, 1, 3, 3, 1, 3, 2, 2, 1, 3, 4, 3, 1, 1, 1, 4, 3, 2, 2, 2,\n",
      "        1, 3, 2, 2, 3, 4, 4, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 1, 1, 2, 1, 1, 1,\n",
      "        4, 3, 3, 3, 3, 4, 1, 2, 3, 1, 3, 2, 3, 3, 1, 2, 1, 1, 1, 1, 3, 4, 4, 1,\n",
      "        2, 1, 2, 2, 1, 1, 3, 3, 2, 4, 3, 3, 4, 1, 2, 1, 1, 3, 2, 2, 3, 3, 3, 4,\n",
      "        4, 3, 3, 3, 1, 3, 4, 3, 4, 2, 3, 4, 3, 1, 3, 1, 1, 4, 1, 2, 1, 3, 2, 1,\n",
      "        3, 2, 1, 3, 4, 1, 2, 4, 3, 3, 3, 1, 4, 1, 3, 3, 1, 1, 3, 4, 1, 2, 2, 1,\n",
      "        2, 3, 1, 1, 3, 3, 5, 4, 3, 1, 1, 4, 3, 1, 3, 3, 4, 1, 2, 3, 1, 4, 1, 2,\n",
      "        2, 1, 1, 2, 2, 4, 1, 4, 3, 1, 1, 2, 2, 1, 1, 3, 3, 4, 5, 3, 3, 3, 3, 1,\n",
      "        4, 2, 4, 1, 4, 4, 1, 4, 1, 3, 1, 1, 4, 3, 4, 1, 2, 3, 3, 1, 1, 3, 4, 3,\n",
      "        2, 3, 4, 3, 1, 4, 3, 3, 4, 3, 2, 3, 1, 3, 3, 2], device='cuda:0')\n",
      "Epoch Test 3, 99/112, \n",
      "predicted:tensor([1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3, 3, 1, 3, 1, 3, 1, 1, 3, 1, 3, 3, 3,\n",
      "        1, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 1, 1, 3, 1, 3, 1, 1, 3, 3, 1, 3, 3, 1,\n",
      "        1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 3, 3, 1, 1, 3,\n",
      "        1, 3, 3, 1, 1, 3, 3, 1, 3, 3, 1, 1, 3, 3, 3, 1, 1, 3, 3, 3, 1, 1, 1, 1,\n",
      "        3, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, 3, 1, 1, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3,\n",
      "        1, 3, 1, 1, 1, 1, 3, 3, 3, 3, 1, 3, 3, 1, 1, 3, 1, 1, 3, 1, 1, 3, 3, 1,\n",
      "        3, 3, 1, 3, 1, 1, 1, 3, 1, 1, 3, 1, 3, 1, 1, 3, 1, 3, 3, 1, 1, 1, 1, 3,\n",
      "        1, 1, 3, 1, 3, 1, 1, 1, 3, 1, 1, 3, 3, 1, 3, 1, 3, 1, 1, 1, 3, 1, 1, 3,\n",
      "        3, 3, 1, 3, 1, 3, 3, 1, 1, 1, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1,\n",
      "        3, 3, 1, 3, 3, 1, 1, 1, 1, 3, 1, 3, 1, 3, 3, 3, 3, 3, 1, 3, 1, 3, 1, 1,\n",
      "        3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3], device='cuda:0'), \n",
      "target:tensor([2, 1, 2, 3, 1, 4, 1, 5, 3, 4, 1, 3, 2, 3, 2, 2, 4, 5, 1, 2, 3, 1, 3, 3,\n",
      "        1, 2, 4, 2, 1, 2, 3, 3, 4, 3, 3, 5, 1, 4, 1, 1, 3, 4, 1, 1, 1, 1, 3, 1,\n",
      "        2, 3, 2, 1, 3, 3, 1, 1, 2, 3, 1, 3, 3, 3, 3, 1, 2, 4, 3, 4, 4, 3, 1, 1,\n",
      "        1, 1, 2, 1, 1, 3, 4, 1, 4, 3, 2, 3, 1, 3, 1, 2, 1, 3, 3, 3, 1, 2, 3, 1,\n",
      "        1, 1, 1, 3, 1, 2, 2, 1, 2, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 4, 1, 2, 4, 3,\n",
      "        3, 1, 2, 1, 2, 1, 3, 4, 2, 4, 3, 3, 4, 2, 3, 3, 2, 1, 3, 2, 1, 2, 3, 2,\n",
      "        3, 3, 1, 2, 3, 1, 1, 3, 1, 1, 2, 4, 1, 5, 1, 1, 1, 4, 2, 1, 2, 2, 2, 3,\n",
      "        2, 1, 3, 1, 3, 2, 1, 1, 2, 4, 3, 2, 2, 1, 1, 1, 4, 1, 1, 1, 3, 3, 3, 1,\n",
      "        1, 4, 3, 3, 2, 1, 2, 3, 3, 1, 2, 3, 3, 3, 3, 4, 1, 3, 3, 1, 3, 3, 2, 3,\n",
      "        4, 3, 2, 1, 3, 1, 1, 3, 2, 1, 2, 1, 3, 1, 3, 3, 4, 3, 1, 3, 2, 3, 4, 4,\n",
      "        3, 3, 3, 4, 4, 3, 3, 3, 1, 3, 4, 4, 1, 4, 3, 3], device='cuda:0')\n",
      "Epoch Test 3 cost time: 5.102s\n",
      "准确率： 0.422\n",
      "Epoch 4, 99/634, loss:1.957578 \n",
      "Epoch 4, 199/634, loss:2.035723 \n",
      "Epoch 4, 299/634, loss:1.960631 \n",
      "Epoch 4, 399/634, loss:1.986284 \n",
      "Epoch 4, 499/634, loss:1.932954 \n",
      "Epoch 4, 599/634, loss:1.908865 \n",
      "Epoch 4 cost time: 114.196s\n",
      "Epoch Test 4, 49/112, \n",
      "predicted:tensor([3, 3, 1, 3, 1, 1, 3, 1, 1, 3, 3, 1, 1, 1, 3, 3, 1, 1, 1, 1, 3, 3, 3, 3,\n",
      "        3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3, 1, 3, 3, 3,\n",
      "        3, 1, 3, 3, 1, 3, 3, 3, 1, 3, 1, 3, 1, 3, 1, 3, 3, 3, 3, 1, 3, 3, 3, 1,\n",
      "        3, 3, 3, 3, 1, 3, 1, 1, 1, 1, 1, 3, 3, 3, 1, 1, 3, 3, 3, 1, 3, 3, 3, 1,\n",
      "        1, 1, 1, 1, 3, 3, 1, 1, 3, 3, 1, 1, 3, 1, 1, 3, 3, 1, 1, 3, 3, 1, 3, 3,\n",
      "        1, 1, 3, 1, 3, 1, 1, 3, 3, 3, 3, 1, 3, 3, 1, 3, 1, 1, 1, 3, 3, 3, 1, 3,\n",
      "        1, 1, 3, 3, 1, 1, 1, 3, 3, 1, 3, 3, 3, 1, 3, 1, 3, 3, 3, 1, 3, 3, 1, 3,\n",
      "        1, 3, 3, 3, 1, 3, 1, 1, 1, 1, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3,\n",
      "        3, 1, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 1, 3, 1, 3, 1, 3, 1, 3, 3, 3, 3, 1,\n",
      "        1, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 1, 3, 3, 1, 1, 3, 3,\n",
      "        3, 1, 3, 3, 1, 3, 1, 3, 3, 1, 3, 1, 1, 3, 1, 3], device='cuda:0'), \n",
      "target:tensor([2, 3, 3, 3, 1, 1, 3, 2, 2, 3, 1, 1, 4, 2, 2, 3, 2, 1, 3, 1, 3, 3, 1, 5,\n",
      "        2, 2, 3, 0, 2, 3, 4, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 1, 1, 3, 3, 3,\n",
      "        3, 1, 3, 4, 4, 2, 2, 3, 1, 4, 1, 1, 1, 3, 1, 1, 3, 3, 2, 1, 4, 4, 2, 3,\n",
      "        2, 2, 2, 3, 1, 2, 1, 1, 3, 1, 1, 3, 1, 3, 1, 2, 1, 2, 2, 4, 3, 4, 4, 2,\n",
      "        3, 2, 1, 2, 1, 1, 4, 2, 1, 1, 2, 2, 1, 3, 5, 4, 3, 3, 1, 3, 3, 1, 4, 3,\n",
      "        2, 2, 4, 2, 4, 1, 3, 3, 1, 3, 2, 2, 3, 4, 3, 4, 3, 1, 3, 4, 3, 5, 2, 3,\n",
      "        2, 2, 2, 3, 1, 2, 1, 5, 2, 3, 4, 3, 4, 1, 3, 1, 1, 2, 3, 2, 0, 5, 3, 3,\n",
      "        2, 4, 1, 1, 1, 3, 1, 1, 2, 3, 4, 2, 3, 2, 3, 2, 1, 2, 3, 3, 4, 3, 4, 4,\n",
      "        1, 1, 2, 3, 1, 3, 3, 1, 3, 3, 3, 3, 1, 4, 2, 2, 2, 2, 1, 4, 0, 3, 4, 2,\n",
      "        2, 1, 1, 1, 3, 3, 2, 4, 2, 3, 1, 3, 1, 3, 2, 2, 4, 1, 2, 3, 2, 1, 1, 1,\n",
      "        4, 3, 5, 3, 3, 3, 5, 2, 2, 1, 3, 1, 1, 1, 3, 2], device='cuda:0')\n",
      "Epoch Test 4, 99/112, \n",
      "predicted:tensor([3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 3,\n",
      "        3, 1, 1, 1, 3, 1, 3, 3, 3, 1, 3, 1, 3, 1, 3, 3, 3, 1, 1, 3, 3, 3, 1, 3,\n",
      "        1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 3, 3, 3, 1, 3, 3,\n",
      "        3, 3, 1, 3, 1, 1, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 1, 1, 3, 1, 1, 3, 3, 3,\n",
      "        3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 3, 3, 3, 1, 3, 3, 3, 1, 3,\n",
      "        1, 3, 1, 1, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 1, 1, 3, 1, 3, 3, 3, 1, 3, 3,\n",
      "        1, 1, 1, 3, 3, 1, 3, 3, 1, 3, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1,\n",
      "        1, 3, 3, 1, 3, 1, 3, 1, 3, 3, 3, 3, 1, 1, 1, 3, 3, 1, 3, 1, 3, 3, 3, 1,\n",
      "        3, 3, 3, 1, 1, 1, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 1, 3,\n",
      "        3, 1, 1, 1, 3, 1, 3, 3, 3, 1, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 1, 1, 1, 3, 3, 1], device='cuda:0'), \n",
      "target:tensor([3, 3, 3, 3, 3, 5, 3, 1, 4, 2, 3, 3, 3, 4, 3, 5, 1, 3, 1, 2, 2, 4, 3, 3,\n",
      "        3, 2, 1, 1, 3, 3, 1, 1, 3, 1, 1, 2, 4, 1, 1, 3, 3, 2, 3, 3, 3, 1, 1, 1,\n",
      "        4, 4, 3, 1, 3, 3, 2, 3, 0, 1, 1, 1, 1, 3, 2, 2, 4, 2, 3, 1, 1, 1, 3, 3,\n",
      "        2, 4, 1, 2, 3, 4, 3, 3, 3, 3, 3, 4, 1, 4, 1, 3, 2, 0, 5, 3, 4, 3, 3, 1,\n",
      "        3, 2, 4, 1, 3, 1, 1, 3, 3, 1, 1, 1, 3, 3, 4, 3, 3, 2, 3, 4, 4, 1, 1, 3,\n",
      "        1, 4, 1, 2, 1, 1, 3, 3, 3, 1, 3, 4, 1, 4, 1, 1, 4, 1, 2, 3, 3, 1, 2, 3,\n",
      "        1, 1, 2, 3, 3, 1, 2, 1, 1, 3, 2, 4, 3, 2, 4, 3, 3, 3, 1, 3, 2, 4, 3, 1,\n",
      "        2, 3, 1, 2, 5, 1, 3, 3, 3, 3, 1, 2, 3, 2, 1, 3, 1, 1, 3, 3, 2, 4, 3, 2,\n",
      "        4, 3, 3, 2, 3, 3, 2, 4, 2, 3, 3, 1, 1, 3, 2, 4, 2, 3, 1, 3, 4, 3, 3, 3,\n",
      "        2, 4, 2, 3, 3, 2, 1, 2, 1, 1, 1, 3, 1, 1, 5, 1, 1, 1, 1, 1, 3, 1, 1, 3,\n",
      "        3, 3, 1, 4, 1, 2, 2, 4, 1, 3, 1, 1, 1, 1, 3, 1], device='cuda:0')\n",
      "Epoch Test 4 cost time: 5.359s\n",
      "准确率： 0.422\n",
      "Epoch 5, 99/634, loss:1.984359 \n",
      "Epoch 5, 199/634, loss:1.956265 \n",
      "Epoch 5, 299/634, loss:1.905477 \n",
      "Epoch 5, 399/634, loss:1.991112 \n",
      "Epoch 5, 499/634, loss:1.983421 \n",
      "Epoch 5, 599/634, loss:2.034917 \n",
      "Epoch 5 cost time: 117.034s\n",
      "Epoch Test 5, 49/112, \n",
      "predicted:tensor([3, 1, 3, 1, 3, 3, 1, 1, 1, 3, 3, 1, 1, 1, 1, 3, 3, 1, 1, 3, 3, 1, 3, 3,\n",
      "        1, 3, 1, 1, 1, 3, 3, 1, 1, 3, 3, 3, 1, 3, 1, 3, 3, 3, 1, 3, 1, 3, 1, 1,\n",
      "        1, 1, 1, 1, 1, 3, 1, 3, 1, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3, 1,\n",
      "        3, 3, 1, 1, 1, 3, 3, 3, 1, 1, 1, 3, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1,\n",
      "        3, 1, 1, 3, 1, 3, 1, 1, 3, 1, 1, 3, 3, 3, 1, 3, 3, 1, 1, 1, 3, 1, 1, 1,\n",
      "        1, 1, 1, 3, 1, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 1, 3, 3, 1,\n",
      "        1, 1, 1, 3, 3, 1, 1, 1, 3, 3, 3, 3, 1, 1, 3, 3, 3, 1, 3, 1, 3, 3, 1, 1,\n",
      "        3, 3, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 3, 1, 3, 1, 1, 3, 1, 1, 1, 3, 3,\n",
      "        3, 3, 3, 1, 1, 3, 3, 1, 3, 1, 3, 3, 1, 3, 3, 3, 3, 1, 1, 3, 3, 1, 1, 3,\n",
      "        1, 1, 3, 1, 3, 3, 1, 3, 1, 1, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1,\n",
      "        3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 1, 3], device='cuda:0'), \n",
      "target:tensor([5, 3, 1, 3, 1, 4, 1, 1, 3, 2, 3, 3, 2, 4, 1, 3, 5, 1, 1, 1, 2, 5, 3, 4,\n",
      "        3, 1, 3, 3, 3, 1, 1, 3, 5, 3, 4, 2, 1, 4, 1, 4, 3, 2, 3, 3, 3, 3, 4, 4,\n",
      "        3, 2, 1, 4, 4, 2, 2, 3, 1, 2, 3, 2, 3, 3, 4, 1, 1, 4, 3, 3, 3, 3, 3, 3,\n",
      "        2, 3, 1, 4, 1, 3, 4, 4, 1, 1, 3, 3, 3, 1, 2, 1, 4, 3, 2, 3, 2, 1, 1, 3,\n",
      "        2, 4, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 2, 2, 1, 3, 4, 1, 2, 2, 1, 2, 1, 4,\n",
      "        1, 3, 1, 3, 2, 1, 4, 4, 3, 2, 3, 3, 1, 3, 3, 1, 1, 2, 3, 4, 3, 4, 4, 3,\n",
      "        1, 1, 3, 4, 2, 1, 3, 1, 3, 3, 4, 4, 3, 1, 5, 3, 2, 2, 3, 4, 3, 1, 3, 3,\n",
      "        3, 3, 4, 3, 3, 4, 1, 4, 3, 3, 3, 1, 1, 4, 3, 4, 2, 3, 2, 3, 5, 2, 1, 1,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 1, 4, 1, 1, 4, 3, 3, 1, 4, 1, 3, 2, 3,\n",
      "        3, 3, 3, 1, 5, 3, 1, 1, 3, 3, 4, 3, 1, 2, 1, 2, 1, 3, 2, 2, 1, 1, 3, 4,\n",
      "        1, 1, 4, 1, 3, 2, 3, 1, 4, 3, 3, 3, 1, 3, 4, 2], device='cuda:0')\n",
      "Epoch Test 5, 99/112, \n",
      "predicted:tensor([3, 1, 3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 1, 1, 1, 1, 1, 3, 3, 1, 3, 1, 1, 3,\n",
      "        1, 3, 1, 3, 3, 1, 3, 3, 1, 3, 3, 1, 1, 3, 1, 1, 3, 3, 3, 3, 3, 3, 1, 1,\n",
      "        1, 3, 3, 1, 1, 3, 1, 3, 3, 1, 3, 1, 3, 3, 1, 3, 3, 3, 1, 1, 3, 3, 1, 3,\n",
      "        3, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1, 3, 3, 3, 1, 1, 3, 1, 3, 3, 3, 1, 1, 3,\n",
      "        3, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 1, 1, 3, 3, 3, 1, 1, 1, 1, 1, 3, 1, 3,\n",
      "        3, 1, 1, 1, 1, 1, 3, 1, 1, 3, 1, 1, 1, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3,\n",
      "        1, 3, 3, 1, 3, 1, 1, 3, 1, 3, 1, 1, 3, 3, 3, 3, 1, 1, 3, 1, 3, 1, 1, 3,\n",
      "        3, 1, 3, 3, 3, 1, 1, 1, 1, 1, 3, 1, 3, 3, 3, 1, 1, 1, 3, 3, 1, 1, 3, 1,\n",
      "        1, 1, 3, 1, 3, 3, 1, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 1, 3, 1, 3, 1, 3, 3,\n",
      "        3, 3, 1, 1, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        3, 1, 3, 1, 3, 3, 1, 1, 3, 1, 3, 1, 1, 3, 3, 3], device='cuda:0'), \n",
      "target:tensor([3, 2, 3, 1, 3, 3, 1, 2, 1, 1, 4, 1, 1, 3, 2, 2, 3, 3, 4, 1, 3, 4, 2, 3,\n",
      "        2, 1, 2, 1, 4, 3, 1, 1, 3, 1, 1, 1, 4, 4, 3, 1, 4, 1, 1, 3, 1, 4, 2, 1,\n",
      "        2, 2, 1, 4, 3, 3, 1, 4, 4, 1, 1, 3, 2, 3, 3, 3, 2, 2, 1, 3, 3, 4, 3, 3,\n",
      "        1, 3, 3, 1, 1, 2, 3, 1, 3, 1, 3, 3, 3, 1, 1, 1, 3, 1, 3, 2, 4, 2, 2, 3,\n",
      "        2, 1, 4, 4, 2, 1, 2, 2, 3, 3, 3, 1, 4, 1, 4, 3, 3, 1, 1, 3, 2, 4, 4, 4,\n",
      "        4, 4, 3, 2, 1, 4, 4, 1, 1, 2, 1, 1, 1, 1, 4, 3, 2, 4, 2, 3, 1, 3, 4, 3,\n",
      "        1, 5, 3, 1, 3, 1, 1, 1, 1, 4, 3, 4, 2, 3, 3, 1, 3, 1, 3, 1, 4, 2, 1, 3,\n",
      "        2, 4, 4, 1, 3, 3, 3, 1, 3, 1, 3, 2, 3, 3, 3, 2, 4, 2, 1, 3, 3, 1, 3, 3,\n",
      "        1, 3, 2, 4, 1, 4, 1, 4, 4, 1, 4, 4, 4, 3, 4, 1, 5, 4, 2, 1, 1, 2, 3, 4,\n",
      "        3, 3, 1, 3, 3, 1, 2, 3, 1, 4, 4, 1, 4, 2, 1, 2, 1, 3, 3, 3, 2, 1, 1, 2,\n",
      "        3, 1, 4, 1, 3, 1, 1, 3, 4, 1, 3, 4, 1, 3, 4, 3], device='cuda:0')\n",
      "Epoch Test 5 cost time: 5.521s\n",
      "准确率： 0.419\n",
      "Epoch 6, 99/634, loss:1.921317 \n",
      "Epoch 6, 199/634, loss:1.999763 \n",
      "Epoch 6, 299/634, loss:1.990793 \n",
      "Epoch 6, 399/634, loss:1.990119 \n",
      "Epoch 6, 499/634, loss:1.996206 \n",
      "Epoch 6, 599/634, loss:1.988631 \n",
      "Epoch 6 cost time: 117.143s\n",
      "Epoch Test 6, 49/112, \n",
      "predicted:tensor([3, 1, 1, 3, 1, 1, 3, 1, 1, 3, 1, 3, 3, 1, 3, 3, 1, 3, 3, 1, 1, 1, 1, 3,\n",
      "        1, 1, 3, 1, 1, 3, 1, 1, 1, 1, 3, 3, 1, 1, 3, 3, 1, 3, 1, 1, 3, 3, 3, 3,\n",
      "        1, 3, 1, 3, 3, 3, 1, 1, 1, 1, 1, 1, 3, 1, 3, 3, 1, 1, 3, 1, 1, 1, 3, 3,\n",
      "        1, 3, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 3, 1, 3, 3, 3, 1, 1, 1, 1, 3,\n",
      "        3, 3, 3, 3, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 3, 3, 1, 3, 3, 3, 1, 1, 3, 1,\n",
      "        1, 3, 3, 1, 3, 3, 1, 3, 1, 1, 1, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 3, 3, 3,\n",
      "        3, 1, 3, 3, 3, 1, 1, 1, 3, 3, 3, 1, 1, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 1, 1, 1, 3, 3, 1, 3, 3, 1, 3, 3, 3, 1, 1, 3, 1, 1, 1, 1, 3, 3, 3, 3,\n",
      "        3, 3, 1, 3, 1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 3, 1,\n",
      "        1, 3, 3, 3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 1, 3, 1, 1, 1, 1, 1, 3, 1, 3,\n",
      "        3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 3, 1, 3, 3], device='cuda:0'), \n",
      "target:tensor([1, 1, 1, 4, 4, 1, 3, 4, 1, 4, 3, 1, 1, 4, 3, 1, 1, 2, 3, 4, 1, 2, 3, 1,\n",
      "        3, 2, 2, 3, 2, 3, 1, 2, 2, 1, 3, 3, 1, 2, 3, 3, 3, 3, 1, 1, 3, 3, 3, 4,\n",
      "        3, 4, 1, 5, 4, 3, 3, 1, 2, 1, 2, 1, 1, 1, 3, 3, 1, 4, 4, 2, 1, 1, 3, 3,\n",
      "        3, 3, 3, 1, 3, 3, 4, 4, 2, 3, 1, 3, 3, 2, 4, 2, 3, 4, 3, 1, 2, 3, 1, 3,\n",
      "        4, 4, 4, 2, 3, 4, 5, 3, 1, 1, 1, 2, 3, 1, 4, 1, 2, 3, 1, 4, 3, 2, 4, 1,\n",
      "        2, 2, 3, 2, 3, 4, 1, 3, 4, 3, 1, 3, 1, 5, 4, 1, 1, 3, 1, 4, 4, 3, 3, 4,\n",
      "        3, 3, 1, 3, 3, 4, 4, 4, 3, 4, 4, 2, 2, 3, 3, 3, 3, 2, 4, 2, 2, 4, 3, 1,\n",
      "        4, 2, 2, 2, 3, 3, 3, 3, 4, 3, 2, 3, 4, 1, 1, 3, 3, 2, 3, 1, 4, 3, 2, 4,\n",
      "        4, 4, 1, 4, 3, 1, 4, 3, 3, 2, 1, 1, 1, 2, 2, 2, 4, 1, 1, 4, 1, 1, 2, 1,\n",
      "        2, 4, 1, 3, 3, 3, 3, 4, 1, 1, 4, 3, 1, 3, 1, 3, 1, 1, 2, 3, 3, 2, 4, 3,\n",
      "        3, 3, 1, 1, 4, 1, 1, 4, 1, 4, 3, 3, 1, 1, 2, 2], device='cuda:0')\n",
      "Epoch Test 6, 99/112, \n",
      "predicted:tensor([1, 1, 3, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 3, 3, 3, 3, 1, 1, 3, 1, 3, 3, 3,\n",
      "        3, 1, 3, 3, 1, 1, 3, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 3, 1, 3, 1, 1, 3,\n",
      "        1, 3, 1, 3, 3, 1, 1, 3, 1, 3, 3, 1, 3, 1, 1, 1, 1, 1, 3, 1, 3, 1, 3, 3,\n",
      "        1, 1, 3, 3, 1, 3, 1, 3, 3, 1, 3, 1, 1, 3, 1, 1, 3, 3, 1, 1, 1, 1, 3, 1,\n",
      "        1, 1, 3, 1, 3, 1, 3, 1, 1, 1, 3, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1,\n",
      "        1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, 3, 1, 3, 1, 3, 3, 3, 1,\n",
      "        3, 3, 1, 3, 3, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 3, 1,\n",
      "        1, 3, 1, 3, 1, 1, 3, 1, 3, 3, 1, 1, 3, 1, 1, 1, 3, 1, 3, 1, 3, 3, 1, 3,\n",
      "        1, 1, 3, 1, 1, 1, 3, 1, 1, 3, 3, 1, 1, 3, 1, 3, 1, 1, 1, 3, 1, 1, 1, 3,\n",
      "        1, 1, 1, 1, 3, 1, 1, 3, 3, 1, 3, 1, 1, 3, 1, 1, 3, 3, 1, 3, 1, 3, 1, 1,\n",
      "        1, 3, 3, 3, 1, 3, 1, 1, 1, 3, 3, 1, 3, 1, 1, 3], device='cuda:0'), \n",
      "target:tensor([1, 1, 3, 1, 1, 3, 3, 4, 1, 1, 2, 2, 3, 4, 4, 3, 2, 1, 4, 3, 3, 2, 3, 1,\n",
      "        4, 3, 4, 4, 3, 3, 1, 3, 1, 4, 2, 2, 3, 3, 3, 3, 1, 1, 1, 3, 1, 3, 1, 3,\n",
      "        4, 3, 1, 3, 4, 2, 1, 3, 1, 3, 4, 2, 4, 1, 1, 2, 1, 4, 3, 3, 3, 3, 3, 3,\n",
      "        3, 5, 3, 3, 4, 3, 1, 1, 1, 1, 5, 2, 4, 3, 1, 1, 3, 1, 3, 2, 1, 2, 5, 1,\n",
      "        2, 1, 2, 2, 3, 1, 1, 1, 1, 1, 3, 2, 1, 4, 3, 1, 2, 1, 3, 4, 3, 1, 2, 4,\n",
      "        3, 1, 4, 2, 5, 1, 5, 3, 1, 3, 3, 1, 1, 1, 4, 1, 3, 1, 2, 3, 4, 1, 3, 3,\n",
      "        3, 1, 4, 3, 2, 3, 2, 3, 1, 1, 1, 3, 2, 1, 3, 4, 3, 2, 1, 2, 1, 1, 4, 1,\n",
      "        2, 2, 4, 3, 1, 1, 4, 1, 1, 4, 2, 3, 5, 1, 3, 1, 3, 1, 1, 3, 1, 3, 4, 2,\n",
      "        1, 4, 4, 1, 3, 1, 3, 3, 3, 4, 4, 3, 3, 4, 3, 4, 1, 1, 3, 3, 1, 1, 3, 3,\n",
      "        4, 3, 2, 3, 3, 4, 2, 4, 3, 2, 1, 2, 1, 3, 4, 2, 3, 3, 4, 1, 4, 3, 2, 4,\n",
      "        1, 2, 1, 1, 2, 4, 2, 1, 3, 3, 4, 3, 4, 1, 1, 2], device='cuda:0')\n",
      "Epoch Test 6 cost time: 5.455s\n",
      "准确率： 0.415\n",
      "Epoch 7, 99/634, loss:2.022610 \n",
      "Epoch 7, 199/634, loss:1.953480 \n",
      "Epoch 7, 299/634, loss:1.949557 \n",
      "Epoch 7, 399/634, loss:1.932013 \n",
      "Epoch 7, 499/634, loss:1.964200 \n",
      "Epoch 7, 599/634, loss:1.964825 \n",
      "Epoch 7 cost time: 118.331s\n",
      "Epoch Test 7, 49/112, \n",
      "predicted:tensor([1, 1, 3, 3, 1, 1, 3, 3, 3, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3,\n",
      "        3, 1, 3, 3, 1, 3, 3, 1, 3, 3, 1, 3, 1, 3, 1, 3, 3, 3, 3, 1, 1, 3, 3, 1,\n",
      "        1, 3, 3, 3, 1, 3, 1, 3, 1, 3, 1, 1, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 3, 3,\n",
      "        1, 1, 1, 3, 1, 3, 3, 3, 3, 3, 1, 1, 3, 1, 3, 3, 3, 1, 1, 1, 3, 1, 3, 3,\n",
      "        1, 1, 3, 3, 3, 1, 3, 1, 1, 3, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1,\n",
      "        3, 3, 1, 3, 3, 3, 3, 1, 3, 1, 3, 3, 3, 1, 1, 3, 3, 1, 3, 3, 1, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 3, 1, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3,\n",
      "        3, 1, 3, 1, 1, 1, 3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 1, 3, 3, 3, 1, 3,\n",
      "        1, 3, 3, 3, 3, 1, 3, 1, 1, 3, 3, 1, 3, 3, 1, 3, 1, 1, 1, 1, 1, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 3, 3, 3,\n",
      "        3, 3, 1, 1, 3, 3, 1, 3, 3, 1, 1, 1, 3, 3, 3, 1], device='cuda:0'), \n",
      "target:tensor([3, 3, 1, 4, 2, 2, 2, 2, 2, 1, 3, 3, 3, 1, 1, 3, 3, 3, 4, 2, 3, 2, 3, 3,\n",
      "        4, 4, 2, 1, 1, 3, 4, 2, 2, 3, 3, 1, 1, 3, 2, 3, 2, 1, 3, 4, 1, 3, 4, 2,\n",
      "        2, 2, 2, 3, 1, 3, 2, 3, 1, 2, 1, 3, 3, 1, 2, 2, 3, 3, 1, 1, 2, 3, 4, 4,\n",
      "        3, 4, 1, 3, 1, 3, 1, 3, 4, 3, 4, 4, 1, 3, 3, 3, 5, 4, 2, 3, 2, 1, 2, 5,\n",
      "        2, 4, 3, 4, 3, 1, 3, 1, 1, 2, 2, 1, 4, 3, 1, 4, 3, 3, 3, 3, 3, 4, 3, 1,\n",
      "        3, 3, 3, 4, 1, 1, 4, 1, 3, 3, 1, 4, 3, 1, 2, 4, 3, 3, 3, 1, 1, 2, 1, 2,\n",
      "        3, 2, 4, 1, 2, 3, 3, 4, 2, 3, 3, 4, 3, 3, 3, 3, 4, 2, 3, 2, 1, 3, 3, 3,\n",
      "        1, 2, 3, 2, 3, 3, 3, 4, 2, 1, 4, 3, 1, 1, 3, 1, 2, 3, 3, 3, 2, 3, 1, 4,\n",
      "        2, 3, 4, 3, 3, 2, 3, 3, 1, 3, 1, 1, 3, 3, 3, 4, 1, 3, 4, 4, 2, 4, 3, 3,\n",
      "        1, 3, 3, 4, 3, 4, 1, 3, 1, 3, 4, 3, 2, 4, 2, 3, 4, 3, 1, 1, 1, 1, 1, 3,\n",
      "        4, 3, 1, 2, 3, 1, 1, 2, 3, 1, 1, 2, 1, 2, 1, 3], device='cuda:0')\n",
      "Epoch Test 7, 99/112, \n",
      "predicted:tensor([3, 3, 3, 1, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 3, 1, 3, 3, 3, 1, 3, 1, 3, 1,\n",
      "        3, 1, 1, 3, 1, 3, 3, 3, 1, 1, 3, 1, 3, 3, 3, 1, 3, 3, 3, 1, 3, 1, 1, 1,\n",
      "        3, 1, 1, 1, 1, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 3, 3, 3, 3, 3, 3,\n",
      "        1, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 3, 3, 3, 1, 3, 1, 1, 1, 3, 3,\n",
      "        1, 3, 1, 3, 1, 1, 3, 1, 3, 3, 1, 1, 1, 3, 3, 3, 3, 3, 1, 3, 1, 3, 3, 1,\n",
      "        1, 3, 1, 3, 3, 3, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 3, 3, 3, 1,\n",
      "        3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 3, 3, 1, 3, 3, 3, 3,\n",
      "        1, 1, 3, 1, 1, 1, 1, 3, 3, 1, 1, 3, 1, 3, 1, 3, 1, 1, 3, 3, 1, 3, 3, 1,\n",
      "        3, 3, 3, 3, 1, 1, 1, 1, 1, 3, 1, 1, 3, 1, 3, 3, 1, 3, 1, 1, 3, 1, 3, 1,\n",
      "        3, 1, 3, 3, 1, 1, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 1, 1, 1, 3, 3, 3, 3, 3,\n",
      "        1, 3, 1, 1, 1, 3, 1, 3, 1, 3, 1, 1, 3, 1, 1, 1], device='cuda:0'), \n",
      "target:tensor([1, 3, 4, 2, 3, 4, 4, 1, 4, 3, 3, 3, 4, 2, 4, 3, 3, 5, 4, 3, 1, 2, 4, 1,\n",
      "        2, 3, 4, 1, 2, 3, 3, 2, 1, 1, 4, 2, 3, 1, 1, 2, 2, 4, 2, 1, 2, 1, 3, 3,\n",
      "        5, 2, 4, 1, 2, 4, 1, 3, 2, 3, 4, 3, 3, 4, 1, 1, 1, 4, 3, 2, 1, 3, 1, 3,\n",
      "        3, 3, 4, 2, 3, 1, 3, 3, 2, 1, 3, 3, 1, 1, 3, 1, 2, 2, 1, 3, 3, 1, 4, 3,\n",
      "        4, 3, 1, 3, 2, 3, 3, 2, 3, 3, 1, 2, 3, 1, 3, 2, 3, 4, 1, 3, 4, 3, 3, 1,\n",
      "        1, 3, 3, 2, 3, 2, 2, 1, 4, 2, 2, 4, 2, 4, 1, 3, 1, 4, 1, 3, 1, 2, 3, 3,\n",
      "        3, 3, 2, 3, 2, 4, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1, 3, 3, 2, 3, 4, 3, 3, 2,\n",
      "        1, 1, 4, 2, 1, 3, 1, 4, 3, 1, 1, 4, 1, 3, 4, 3, 1, 3, 2, 3, 1, 5, 4, 1,\n",
      "        3, 4, 1, 4, 4, 1, 1, 4, 4, 1, 1, 3, 1, 2, 3, 2, 1, 2, 3, 2, 2, 1, 4, 3,\n",
      "        4, 2, 3, 2, 2, 1, 3, 1, 5, 3, 4, 1, 3, 1, 5, 3, 2, 3, 1, 2, 3, 2, 4, 1,\n",
      "        4, 4, 1, 1, 3, 3, 4, 3, 5, 3, 1, 3, 1, 2, 4, 2], device='cuda:0')\n",
      "Epoch Test 7 cost time: 6.242s\n",
      "准确率： 0.422\n",
      "Epoch 8, 99/634, loss:1.921578 \n",
      "Epoch 8, 199/634, loss:1.988417 \n",
      "Epoch 8, 299/634, loss:1.928022 \n",
      "Epoch 8, 399/634, loss:1.904150 \n",
      "Epoch 8, 499/634, loss:2.005122 \n",
      "Epoch 8, 599/634, loss:1.966469 \n",
      "Epoch 8 cost time: 119.378s\n",
      "Epoch Test 8, 49/112, \n",
      "predicted:tensor([3, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 1, 3, 1, 1, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 1, 3, 3, 1, 3, 1, 1, 3, 3, 3, 1, 1, 3, 1, 3, 3, 1, 3, 3, 1,\n",
      "        1, 3, 1, 3, 1, 3, 1, 1, 3, 1, 3, 3, 1, 1, 3, 3, 3, 3, 3, 1, 1, 1, 1, 3,\n",
      "        3, 3, 1, 3, 1, 3, 1, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3, 1, 1, 3, 1, 3, 1, 1,\n",
      "        3, 3, 3, 3, 3, 1, 1, 3, 1, 1, 1, 3, 3, 1, 3, 1, 3, 1, 3, 3, 1, 3, 1, 3,\n",
      "        3, 1, 1, 1, 3, 1, 1, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 1,\n",
      "        1, 1, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 1, 1, 3, 3, 3, 1, 3, 1, 1, 3, 3, 1,\n",
      "        1, 3, 1, 3, 1, 1, 1, 1, 3, 3, 3, 1, 3, 1, 3, 3, 1, 3, 1, 3, 3, 1, 3, 1,\n",
      "        1, 1, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 1, 3, 3, 1, 1, 3,\n",
      "        1, 1, 3, 1, 1, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 3, 3, 3, 1, 3,\n",
      "        3, 1, 3, 1, 1, 1, 1, 1, 3, 1, 3, 3, 3, 3, 1, 1], device='cuda:0'), \n",
      "target:tensor([4, 2, 1, 1, 3, 2, 1, 3, 2, 4, 5, 4, 2, 3, 2, 1, 2, 3, 2, 3, 3, 4, 3, 3,\n",
      "        2, 1, 5, 4, 1, 2, 1, 1, 3, 1, 1, 3, 4, 3, 1, 1, 4, 1, 4, 3, 1, 3, 1, 1,\n",
      "        3, 4, 3, 4, 3, 3, 3, 3, 3, 2, 4, 3, 1, 5, 4, 3, 3, 4, 1, 1, 3, 3, 4, 3,\n",
      "        4, 3, 1, 1, 1, 3, 3, 1, 2, 4, 5, 1, 1, 1, 2, 3, 2, 5, 1, 3, 3, 2, 1, 1,\n",
      "        3, 1, 1, 2, 3, 2, 3, 3, 4, 3, 3, 2, 3, 1, 1, 3, 3, 4, 3, 2, 1, 4, 2, 2,\n",
      "        1, 4, 2, 1, 1, 4, 3, 1, 2, 3, 1, 4, 2, 1, 4, 4, 3, 3, 2, 3, 3, 1, 3, 2,\n",
      "        3, 1, 1, 1, 3, 3, 1, 1, 1, 3, 3, 3, 4, 4, 3, 3, 3, 3, 1, 1, 3, 2, 2, 2,\n",
      "        2, 1, 2, 1, 1, 1, 3, 3, 1, 3, 1, 1, 3, 3, 3, 1, 3, 5, 1, 3, 1, 1, 3, 2,\n",
      "        2, 1, 3, 3, 4, 4, 4, 1, 1, 2, 3, 3, 1, 2, 1, 1, 3, 2, 1, 1, 3, 4, 2, 1,\n",
      "        1, 2, 2, 1, 1, 3, 3, 4, 3, 3, 3, 3, 2, 3, 3, 1, 3, 3, 2, 4, 3, 4, 3, 3,\n",
      "        4, 2, 1, 2, 1, 1, 2, 1, 3, 1, 3, 1, 3, 1, 3, 2], device='cuda:0')\n",
      "Epoch Test 8, 99/112, \n",
      "predicted:tensor([3, 1, 1, 3, 1, 3, 3, 1, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 1, 3, 3, 1, 3, 1,\n",
      "        3, 1, 1, 1, 3, 3, 1, 1, 3, 3, 3, 1, 1, 3, 3, 1, 3, 3, 1, 3, 3, 1, 1, 3,\n",
      "        3, 3, 3, 3, 1, 3, 3, 1, 1, 1, 1, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 1, 3, 1,\n",
      "        1, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3, 1, 1, 3, 3, 1, 3, 1, 3, 1, 1, 1, 3,\n",
      "        3, 1, 1, 1, 1, 1, 1, 3, 1, 3, 3, 3, 3, 1, 3, 3, 3, 1, 3, 1, 1, 1, 1, 3,\n",
      "        3, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 1, 3, 1, 1, 1, 3, 1, 1, 3, 1, 3, 3, 3,\n",
      "        1, 1, 1, 1, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 1, 3, 1, 3, 3, 1, 3, 3, 1, 3,\n",
      "        1, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 1, 3, 1, 1, 3, 1, 1, 3, 1, 3, 1, 1, 3,\n",
      "        1, 3, 1, 3, 1, 1, 3, 1, 3, 3, 1, 3, 3, 1, 3, 3, 1, 1, 1, 3, 3, 3, 3, 3,\n",
      "        1, 3, 1, 3, 3, 1, 1, 3, 3, 3, 1, 3, 1, 3, 3, 3, 1, 1, 3, 1, 1, 3, 1, 3,\n",
      "        1, 3, 3, 1, 1, 3, 3, 3, 1, 1, 1, 3, 3, 3, 1, 1], device='cuda:0'), \n",
      "target:tensor([4, 2, 1, 3, 4, 3, 4, 1, 3, 4, 3, 2, 2, 3, 3, 3, 2, 2, 3, 1, 2, 2, 5, 1,\n",
      "        3, 3, 3, 2, 3, 4, 2, 1, 2, 1, 3, 1, 3, 3, 5, 1, 3, 2, 3, 3, 3, 2, 2, 3,\n",
      "        3, 3, 2, 1, 1, 4, 4, 1, 2, 3, 2, 1, 2, 3, 1, 3, 1, 4, 3, 3, 2, 1, 2, 1,\n",
      "        1, 1, 4, 1, 4, 3, 1, 4, 3, 1, 2, 3, 2, 2, 2, 3, 4, 3, 2, 3, 1, 2, 1, 2,\n",
      "        2, 1, 3, 1, 2, 2, 3, 3, 4, 2, 3, 3, 3, 1, 4, 3, 3, 4, 3, 3, 1, 1, 5, 3,\n",
      "        2, 3, 1, 2, 2, 1, 1, 3, 4, 2, 2, 1, 4, 1, 2, 1, 2, 3, 1, 3, 3, 2, 1, 3,\n",
      "        1, 3, 1, 2, 1, 4, 2, 3, 3, 3, 1, 4, 3, 2, 4, 1, 5, 3, 2, 2, 1, 3, 2, 4,\n",
      "        3, 3, 2, 4, 1, 1, 3, 3, 1, 3, 4, 1, 3, 3, 1, 3, 2, 3, 1, 3, 3, 2, 1, 4,\n",
      "        2, 4, 1, 2, 2, 3, 3, 2, 3, 1, 1, 1, 5, 1, 1, 4, 4, 1, 4, 3, 1, 3, 4, 3,\n",
      "        5, 3, 3, 2, 1, 1, 3, 1, 2, 3, 3, 1, 3, 3, 4, 1, 1, 2, 3, 1, 1, 4, 3, 3,\n",
      "        2, 4, 3, 2, 4, 2, 3, 1, 3, 1, 1, 4, 3, 3, 1, 3], device='cuda:0')\n",
      "Epoch Test 8 cost time: 5.419s\n",
      "准确率： 0.420\n",
      "Epoch 9, 99/634, loss:1.953321 \n",
      "Epoch 9, 199/634, loss:1.924276 \n",
      "Epoch 9, 299/634, loss:1.959341 \n",
      "Epoch 9, 399/634, loss:1.939758 \n",
      "Epoch 9, 499/634, loss:1.925313 \n",
      "Epoch 9, 599/634, loss:1.961132 \n",
      "Epoch 9 cost time: 119.172s\n",
      "Epoch Test 9, 49/112, \n",
      "predicted:tensor([3, 1, 3, 1, 3, 1, 3, 3, 3, 1, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 1, 3, 3, 3,\n",
      "        1, 1, 3, 3, 1, 3, 1, 1, 1, 1, 3, 1, 3, 3, 3, 1, 3, 3, 3, 1, 1, 3, 3, 1,\n",
      "        3, 1, 1, 3, 1, 3, 1, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3, 1,\n",
      "        3, 3, 3, 3, 1, 3, 3, 1, 3, 1, 1, 3, 3, 3, 1, 1, 3, 3, 3, 1, 3, 1, 1, 1,\n",
      "        3, 3, 3, 3, 3, 3, 1, 3, 1, 3, 1, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 3, 3,\n",
      "        3, 3, 3, 3, 3, 1, 3, 3, 1, 1, 3, 1, 3, 3, 1, 3, 1, 3, 3, 1, 3, 1, 3, 1,\n",
      "        1, 3, 3, 3, 3, 3, 1, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 1, 3,\n",
      "        3, 3, 1, 1, 1, 3, 3, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 1, 3, 3,\n",
      "        1, 3, 1, 3, 3, 1, 1, 1, 3, 1, 3, 1, 1, 3, 1, 1, 3, 3, 1, 1, 1, 1, 3, 3,\n",
      "        1, 3, 1, 3, 1, 1, 3, 1, 3, 3, 1, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 1, 3, 3, 3, 3, 1, 3, 3, 1, 3, 1, 1, 3, 3, 3], device='cuda:0'), \n",
      "target:tensor([4, 2, 3, 3, 3, 5, 4, 1, 3, 3, 1, 4, 2, 5, 2, 4, 4, 2, 2, 4, 5, 1, 4, 1,\n",
      "        3, 1, 3, 2, 2, 4, 3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 4, 1, 2, 2, 3, 2,\n",
      "        4, 1, 5, 1, 2, 2, 1, 3, 1, 4, 3, 3, 1, 3, 3, 2, 3, 3, 5, 3, 4, 1, 3, 2,\n",
      "        3, 2, 2, 2, 3, 4, 3, 1, 4, 3, 1, 2, 1, 2, 2, 3, 5, 3, 1, 4, 1, 1, 3, 2,\n",
      "        4, 2, 4, 1, 3, 3, 1, 4, 2, 1, 1, 2, 1, 1, 2, 3, 2, 1, 3, 3, 3, 3, 4, 3,\n",
      "        2, 2, 3, 2, 2, 2, 3, 3, 4, 1, 3, 1, 3, 2, 4, 2, 3, 2, 4, 3, 3, 2, 2, 2,\n",
      "        1, 3, 2, 3, 1, 1, 1, 2, 5, 3, 3, 3, 4, 1, 3, 3, 3, 3, 2, 4, 1, 4, 2, 3,\n",
      "        2, 5, 1, 1, 1, 3, 2, 2, 1, 3, 2, 1, 1, 1, 2, 3, 3, 2, 1, 3, 3, 3, 2, 1,\n",
      "        3, 5, 1, 3, 1, 2, 1, 3, 3, 4, 3, 2, 1, 4, 3, 3, 3, 1, 1, 2, 3, 3, 3, 1,\n",
      "        3, 3, 3, 3, 2, 1, 3, 2, 4, 3, 2, 4, 2, 4, 1, 3, 4, 2, 2, 3, 1, 1, 3, 1,\n",
      "        3, 1, 3, 2, 3, 3, 3, 1, 4, 1, 3, 1, 3, 3, 1, 2], device='cuda:0')\n",
      "Epoch Test 9, 99/112, \n",
      "predicted:tensor([3, 1, 1, 3, 1, 1, 1, 1, 3, 1, 3, 3, 3, 1, 3, 3, 1, 3, 3, 1, 3, 1, 1, 1,\n",
      "        3, 3, 1, 3, 3, 1, 3, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1,\n",
      "        1, 3, 1, 1, 3, 3, 1, 3, 3, 1, 3, 1, 1, 3, 3, 1, 1, 3, 3, 3, 3, 1, 3, 1,\n",
      "        3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 1, 1, 3, 3,\n",
      "        1, 3, 1, 3, 3, 3, 1, 1, 1, 3, 1, 3, 1, 1, 3, 1, 3, 3, 1, 3, 3, 1, 3, 1,\n",
      "        3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1, 1, 3, 1, 3,\n",
      "        3, 3, 3, 1, 3, 3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 1, 3, 1, 3, 3, 1, 1, 3,\n",
      "        3, 3, 1, 3, 3, 3, 1, 3, 1, 1, 1, 1, 1, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1, 1,\n",
      "        3, 3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 1,\n",
      "        3, 3, 1, 3, 1, 1, 3, 3, 3, 1, 1, 1, 3, 1, 1, 3, 3, 3, 1, 3, 3, 3, 1, 3,\n",
      "        3, 1, 3, 1, 3, 3, 3, 1, 3, 1, 1, 1, 3, 3, 1, 1], device='cuda:0'), \n",
      "target:tensor([1, 2, 1, 3, 1, 4, 1, 1, 2, 1, 3, 3, 2, 1, 3, 5, 4, 2, 2, 1, 4, 1, 3, 2,\n",
      "        3, 2, 1, 3, 3, 3, 3, 1, 3, 1, 1, 1, 1, 2, 3, 1, 3, 2, 2, 3, 3, 3, 3, 1,\n",
      "        1, 3, 1, 4, 3, 3, 1, 3, 2, 1, 3, 2, 2, 3, 3, 3, 1, 4, 1, 3, 3, 3, 2, 2,\n",
      "        1, 4, 3, 1, 1, 2, 1, 3, 3, 3, 4, 3, 1, 3, 1, 1, 1, 2, 3, 3, 1, 2, 3, 3,\n",
      "        3, 4, 4, 1, 3, 3, 2, 3, 3, 3, 3, 1, 2, 5, 2, 1, 4, 1, 1, 1, 3, 3, 4, 4,\n",
      "        4, 3, 2, 3, 2, 1, 1, 2, 2, 3, 3, 2, 1, 3, 1, 4, 3, 3, 3, 2, 2, 3, 2, 3,\n",
      "        5, 4, 3, 1, 4, 2, 3, 2, 1, 3, 3, 3, 1, 1, 2, 3, 2, 3, 3, 4, 5, 2, 1, 3,\n",
      "        1, 3, 3, 3, 3, 2, 1, 3, 2, 1, 3, 3, 1, 3, 3, 3, 3, 3, 4, 2, 4, 2, 2, 3,\n",
      "        3, 1, 1, 3, 2, 3, 4, 3, 3, 2, 1, 2, 3, 3, 2, 2, 3, 2, 3, 1, 4, 3, 4, 3,\n",
      "        3, 2, 3, 1, 1, 3, 3, 4, 2, 1, 1, 3, 3, 1, 2, 3, 3, 1, 1, 1, 4, 1, 3, 3,\n",
      "        3, 1, 3, 2, 3, 4, 3, 1, 2, 3, 1, 2, 1, 3, 3, 3], device='cuda:0')\n",
      "Epoch Test 9 cost time: 5.605s\n",
      "准确率： 0.422\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def trainOneEpoch(epoch, model:HAN, trainLoader, optimizer:Optimizer, lossFunc):\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        lossFunc = lossFunc.cuda()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    startTime = time.time()\n",
    "    for i, (x, y) in enumerate(trainLoader):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        outputs = model(x)\n",
    "        loss = lossFunc(outputs, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 99:\n",
    "            print('Epoch %d, %d/%d, loss:%f ' % (epoch, i, len(trainLoader), loss))\n",
    "        # if i > 2000:\n",
    "        #     break \n",
    "    print('Epoch %d cost time: %.3fs' % (epoch, time.time() - startTime))\n",
    "\n",
    "\n",
    "def testModel(epoch, model:HAN, testLoader):\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    startTime = time.time()\n",
    "    for i, (x, y) in enumerate(testLoader):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        outputs = model(x)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += len(y)\n",
    "        correct += predicted.data.eq(y.data).cpu().sum().numpy()\n",
    "        \n",
    "\n",
    "        if i % 50 == 49:\n",
    "            print('Epoch Test %d, %d/%d, \\npredicted:%s, \\ntarget:%s' % (epoch, i, len(testLoader), \n",
    "                                                                     str(predicted),\n",
    "                                                                     str(y)))\n",
    "        # if i > 2000:\n",
    "        #     break \n",
    "    print('Epoch Test %d cost time: %.3fs' % (epoch, time.time() - startTime))\n",
    "    print('准确率： %.3f' % (correct / total))\n",
    "\n",
    "\n",
    "def train(nepoch, model, modelSavePath):\n",
    "    # optimizer=torch.optim.SGD(model.parameters(), momentum=0.9, lr=0.001)\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    lossFunc =torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(nepoch):\n",
    "        trainOneEpoch(epoch, model, trainLoader, optimizer, lossFunc)\n",
    "        testModel(epoch, model, testLoader)\n",
    "    torch.save(model.state_dict(), modelSavePath)\n",
    "\n",
    "def eval(modelSavePath, isLoad = True):\n",
    "    if isLoad: model.load_state_dict(torch.load(modelSavePath))\n",
    "    testModel(0, model, testLoader)\n",
    "\n",
    "train(10, model, 'EmotionAnalyzeModelData.model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lossFunc =torch.nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "_, predicted = torch.max(input.data, 1)\n",
    "output = lossFunc(input, target)\n",
    "print('input',input, '\\n target',target, '\\n output', output)\n",
    "output = lossFunc(input, predicted)\n",
    "print('predicted',predicted, '\\n target',target, '\\n output', output)\n",
    "\n",
    "print(predicted.data.eq(target).cpu().sum())\n",
    "print(target.data.eq(predicted).cpu().sum())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 何凯明初始化\n",
    "\n",
    "w = torch.Tensor(3, 5, 2)\n",
    "print(w)\n",
    "print(nn.init.kaiming_uniform(w))\n",
    "print(w)\n",
    "w = torch.Tensor(3, 5, 2)\n",
    "print(w)\n",
    "print(torch.nn.init.kaiming_normal(w))\n",
    "print(w)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}