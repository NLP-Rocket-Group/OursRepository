{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from HANEmotionAnalyze import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy\n",
    "import time, math\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-5.1958e-01, -1.3948e-01, -9.0974e-05],\n",
      "         [ 1.8373e-03,  1.2931e+00, -5.4605e-01],\n",
      "         [-1.6162e-01,  4.1798e-01, -3.7759e-01],\n",
      "         [-4.6572e-01,  6.1565e-01, -2.9826e-01],\n",
      "         [ 3.0229e-01,  1.0494e+00,  4.9972e-01]],\n",
      "\n",
      "        [[ 1.8373e-03,  1.2931e+00, -5.4605e-01],\n",
      "         [-4.6572e-01,  6.1565e-01, -2.9826e-01],\n",
      "         [-1.4307e+00,  9.7822e-02, -5.6423e-01],\n",
      "         [-1.6162e-01,  4.1798e-01, -3.7759e-01],\n",
      "         [ 1.0096e+00,  7.0409e-01, -9.4448e-01]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "tensor([[[-0.4019,  2.1143,  0.4684],\n",
      "         [ 1.4231,  0.8628, -1.3925],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 1.2632,  1.2053, -0.3622],\n",
      "         [ 0.2257,  0.9233,  0.7332]],\n",
      "\n",
      "        [[ 1.4231,  0.8628, -1.3925],\n",
      "         [ 1.2632,  1.2053, -0.3622],\n",
      "         [ 0.5502, -1.1008, -0.9046],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 1.4089,  0.4509, -1.3207]]], grad_fn=<EmbeddingBackward>)\n",
      "tensor([[4.0000, 5.1000, 6.3000]])\n",
      "tensor([[1.0000, 2.3000, 3.0000]])\n"
     ]
    }
   ],
   "source": [
    "# import torch.functional as F\n",
    "# \n",
    "embedding = nn.Embedding(10, 3)\n",
    "input = torch.LongTensor([[0, 1,2,4,5],[1, 4,3,2,9]])\n",
    "print(embedding(input))\n",
    "embedding2 = nn.Embedding(10, 3, padding_idx=2)\n",
    "# input = torch.LongTensor([[1,2,0,5, 6,7,8,9]])\n",
    "print(embedding2(input))\n",
    "weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n",
    "embedding3 = nn.Embedding.from_pretrained(weight)\n",
    "input = torch.LongTensor([1])\n",
    "print(embedding3(input))\n",
    "input = torch.LongTensor([0])\n",
    "print(embedding3(input))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAN(\n",
      "  (embed): Embedding(5845, 200, padding_idx=0)\n",
      "  (GRU1): GRU(200, 50, batch_first=True, bidirectional=True)\n",
      "  (self_attention1): SelfAttention(\n",
      "    (W): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (U): Linear(in_features=100, out_features=1, bias=True)\n",
      "  )\n",
      "  (GRU2): GRU(50, 50, batch_first=True, bidirectional=True)\n",
      "  (self_attention2): SelfAttention(\n",
      "    (W): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (U): Linear(in_features=100, out_features=1, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_embeddings = 111111\n",
    "model = HAN(num_embeddings)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 读取词向量\n",
    "建立词语列表"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "file = 'Tencent_AILab_ChineseEmbedding/Tencent_AILab_ChineseEmbedding.txt'\n",
    "wv_from_text = KeyedVectors.load_word2vec_format(file, binary=False)\n",
    "wv_from_text.init_sims(replace=True)  # 神奇，很省内存，可以运算most_similar"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'y', 'yo', 'o', 'ou', 'u']\n",
      "['h', 'hin', ' ', 'i', 'Th', 'nk', 'in', 'I', 'T', 'I T', ' Th', 'k', 'I ', 'hi', 'Thi', 'n', ' T', 'ink']\n",
      "['和', '万', '国万', '中华人', '万岁', '人民共', '民共', '民', '岁', '人民', '人', '共', '国', '华', '和国', '国万岁', '共和国', '共和', '中华', '中', '华人', '华人民', '和国万', '民共和']\n"
     ]
    }
   ],
   "source": [
    "def compute_ngrams(word, num_min, num_max):\n",
    "    ngrams =[]\n",
    "    for ngram_length in range(num_min, min(len(word), num_max) + 1):\n",
    "        for i in range(len(word) - ngram_length + 1):\n",
    "            # print(i, i + ngram_length)\n",
    "            ngrams.append(word[i : i + ngram_length])\n",
    "    # print(ngrams)\n",
    "    return list(set(ngrams))\n",
    "\n",
    "print(compute_ngrams('you',  num_min = 1, num_max = 3))\n",
    "print(compute_ngrams('I Think',  num_min = 1, num_max = 3))\n",
    "print(compute_ngrams('中华人民共和国万岁',  num_min = 1, num_max = 3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 从词向量文本文件 wv_from_text 中获取词向量，如果获取到直接返回，若没有获取到，那么把这个词拆开\n",
    "# 成为 ngrams 的新词组，并在 wv_from_text 中找新词组中的词向量并相加取平均，最后得到平均词向量输出\n",
    "def wordVec(word, wv_from_text, min_n = 1, max_n = 3):\n",
    "    '''\n",
    "    ngrams_single/ngrams_more,主要是为了当出现oov的情况下,最好先不考虑单字词向量\n",
    "    '''\n",
    "    # 确认词向量维度\n",
    "    word_size = wv_from_text.wv.syn0[0].shape[0]\n",
    "\n",
    "    # 如果在词典之中，直接返回词向量\n",
    "    if word in wv_from_text.wv.vocab.keys():\n",
    "        return wv_from_text[word]\n",
    "    else:\n",
    "        # 计算word的ngrams词组\n",
    "        ngrams = compute_ngrams(word, min_n = min_n, max_n = max_n)\n",
    "        # 不在词典的情况下\n",
    "        word_vec = numpy.zeros(word_size, dtype=numpy.float32)\n",
    "        ngrams_found = 0\n",
    "        ngrams_single = [ng for ng in ngrams if len(ng) == 1]\n",
    "        ngrams_more = [ng for ng in ngrams if len(ng) > 1]\n",
    "        # 先只接受2个单词长度以上的词向量\n",
    "        for ngram in ngrams_more:\n",
    "            if ngram in wv_from_text.wv.vocab.keys():\n",
    "                word_vec += wv_from_text[ngram]\n",
    "                ngrams_found += 1\n",
    "                #print(ngram)\n",
    "        # 如果，没有匹配到，那么最后是考虑单个词向量\n",
    "        if ngrams_found == 0:\n",
    "            for ngram in ngrams_single:\n",
    "                word_vec += wv_from_text[ngram]\n",
    "                ngrams_found += 1\n",
    "        if word_vec.any():\n",
    "            return word_vec / max(1, ngrams_found)\n",
    "        else:\n",
    "            raise KeyError('all ngrams for word %s absent from model' % word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def isNan(a):\n",
    "#     return a != a\n",
    "\n",
    "# class RatingData4Bert(data.Dataset):\n",
    "#     def __init__(self, path, tokenizer:Tokenizer, max_row = -1, trainTestRate = 0.85, isTrain = True):\n",
    "#         self.token_list = []\n",
    "#         self.label_list = []\n",
    "#         self.token_positions = torch.tensor([i for i in range(100)])\n",
    "#\n",
    "#         print('地址不应该包含 ratings.csv   path:',path) # 地址不应该包含 ratings.csv\n",
    "#\n",
    "#         ratings_clean_filename = os.path.join(path, 'ratings_clean.csv')\n",
    "#         ratings_filename = os.path.join(path, 'ratings.csv')\n",
    "#         if os.path.isfile(ratings_clean_filename):\n",
    "#             clean_pd = pd.read_csv(ratings_clean_filename)\n",
    "#         else:\n",
    "#             print('没有找到缓存的文件%s, 读取源文件%s'%(ratings_clean_filename, ratings_filename))\n",
    "#             ratings_pd = pd.read_csv(ratings_filename)\n",
    "#             print('开始生成缓存文件%s'%(ratings_clean_filename))\n",
    "#             clean_pd = pd.DataFrame({\n",
    "#                 'userId':[],\n",
    "#                 'restId':[],\n",
    "#                 'rating':[],\n",
    "#                 'comment':[],\n",
    "#             })\n",
    "#             nonRatingCount = 0\n",
    "#             for i, row in ratings_pd.iterrows():\n",
    "#                 if max_row != -1 and i > max_row:\n",
    "#                     break\n",
    "#                 if not isinstance(row['comment'], str) or row['comment'] == '':\n",
    "#                     # print(i + 1, row['comment'])\n",
    "#                     nonRatingCount += 1\n",
    "#                     continue\n",
    "#                 r0 = row['rating']\n",
    "#                 r1 = row['rating_env']\n",
    "#                 r2 = row['rating_flavor']\n",
    "#                 r3 = row['rating_service']\n",
    "#                 if r0 == '' or isNan(r0): r0 = 0 # 假设总评分为 0 表示未评分\n",
    "#                 if r1 == '' or isNan(r1): r1 = 3\n",
    "#                 if r2 == '' or isNan(r2): r2 = 3\n",
    "#                 if r3 == '' or isNan(r3): r3 = 3\n",
    "#                 r0 = round(r0 * 0.5 + (r1 + r2 + r3) * 0.1666666)\n",
    "#                 if i % 10000 == 9999:\n",
    "#                     print(i + 1, r0)\n",
    "#\n",
    "#                 token = tokenizer.encode(text=str(row['comment']), max_length=100, pad_to_max_length = True)\n",
    "#                 # print('token', token)\n",
    "#                 # token = [101] + token + [102]\n",
    "#\n",
    "#                 newRow = DotDict()\n",
    "#                 newRow.userId = [row['userId']]\n",
    "#                 newRow.restId = [row['restId']]\n",
    "#                 newRow.rating = [r0]\n",
    "#                 newRow.comment = [json.dumps(token)]\n",
    "#\n",
    "#                 clean_pd = clean_pd.append(pd.DataFrame(newRow), ignore_index=True)\n",
    "#             print('空的评论数量： %d'%(nonRatingCount))\n",
    "#             clean_pd.to_csv(ratings_clean_filename)\n",
    "#\n",
    "#         # 读取\n",
    "#         if isTrain:\n",
    "#             temp_pd = clean_pd[ : int(len(clean_pd) * trainTestRate)]\n",
    "#         else:\n",
    "#             temp_pd = clean_pd[int(len(clean_pd) * trainTestRate) : ]\n",
    "#\n",
    "#         for i, row in temp_pd.iterrows():\n",
    "#             if max_row != -1 and i > max_row:\n",
    "#                 break\n",
    "#\n",
    "#             self.label_list.append(torch.tensor(row['rating']).long())\n",
    "#             self.token_list.append(torch.from_numpy(numpy.array( json.loads(row['comment']) ) ).long())\n",
    "#\n",
    "#     def __getitem__(self, index):\n",
    "#         # print(self.token_list[index], self.label_list[index], self.token_positions)\n",
    "#         return self.token_list[index], self.label_list[index], self.token_positions\n",
    "#\n",
    "#     def __len__(self):\n",
    "#         return len(self.label_list)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}