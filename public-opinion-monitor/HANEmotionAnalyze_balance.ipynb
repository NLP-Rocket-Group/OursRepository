{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "HANEmotionAnalyze_balance.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "zBDjArJoT65A",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "outputId": "94b527ab-8019-484c-f8d8-59c5225e5b69",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1587880420272,
     "user_tz": -480,
     "elapsed": 80857,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd /content/drive/My\\ Drive/Colab Notebooks/OursRepository/public-opinion-monitor\n",
    "# \n",
    "# !pip install thulac\n",
    "# !pip install jieba"
   ],
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "9Eush51XTobh",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "outputId": "9d1cc6cf-f7de-4709-f37a-fa7ea19586df",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1587880427236,
     "user_tz": -480,
     "elapsed": 87782,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    }
   },
   "source": [
    "from HANModel import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import numpy\n",
    "import time, math\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import thulac\n",
    "thulac = thulac.thulac()\n",
    "import jieba"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Model loaded succeed\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "DUFj-U3PTobn",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "outputId": "6670b640-c07c-4509-d56a-50951c1ae68e",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1587880427238,
     "user_tz": -480,
     "elapsed": 87759,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    }
   },
   "source": [
    "# import torch.functional as F\n",
    "# \n",
    "embedding = nn.Embedding(10, 3)\n",
    "input = torch.LongTensor([[0, 1,2,4,5],[1, 4,3,2,9]])\n",
    "print(embedding(input))\n",
    "embedding2 = nn.Embedding(10, 3, padding_idx=2)\n",
    "# input = torch.LongTensor([[1,2,0,5, 6,7,8,9]])\n",
    "print(embedding2(input))\n",
    "weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n",
    "embedding3 = nn.Embedding.from_pretrained(weight)\n",
    "input = torch.LongTensor([1])\n",
    "print(embedding3(input))\n",
    "input = torch.LongTensor([0])\n",
    "print(embedding3(input))\n",
    "\n"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([[[-0.5285, -0.8154, -0.1730],\n",
      "         [ 1.1920,  0.5945,  0.3039],\n",
      "         [ 0.0969,  0.9607,  0.0874],\n",
      "         [-0.6594,  1.2062, -0.6054],\n",
      "         [-0.9235, -0.7378, -1.2172]],\n",
      "\n",
      "        [[ 1.1920,  0.5945,  0.3039],\n",
      "         [-0.6594,  1.2062, -0.6054],\n",
      "         [-0.6973, -0.2179,  0.1628],\n",
      "         [ 0.0969,  0.9607,  0.0874],\n",
      "         [-1.0904,  0.2862,  0.1861]]], grad_fn=<EmbeddingBackward>)\n",
      "tensor([[[ 1.0822,  0.6495, -0.0792],\n",
      "         [ 2.4958,  1.7958,  0.2695],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [-0.3829,  0.0271, -0.0781],\n",
      "         [-0.3386, -0.4504,  0.8223]],\n",
      "\n",
      "        [[ 2.4958,  1.7958,  0.2695],\n",
      "         [-0.3829,  0.0271, -0.0781],\n",
      "         [-1.3836,  1.1781,  2.2553],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.2275, -0.1155,  0.3388]]], grad_fn=<EmbeddingBackward>)\n",
      "tensor([[4.0000, 5.1000, 6.3000]])\n",
      "tensor([[1.0000, 2.3000, 3.0000]])\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "3HmMThKsTobr",
    "colab_type": "text"
   },
   "source": [
    "## 读取词向量\n",
    "建立词语列表"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "hhmz_drpTobs",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "outputId": "db3c278a-9a7f-429f-e1a3-f50128ed457b",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1587880442335,
     "user_tz": -480,
     "elapsed": 102836,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    }
   },
   "source": [
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "\n",
    "# file = '../../PretrainedData/Tencent_AILab_ChineseEmbedding/Tencent_AILab_ChineseEmbedding.txt'\n",
    "# file = '../../DataSets/Word2Vect/xingrong_50_thulac/word2vect_50_w5.model'\n",
    "# file = '../../DataSets/Word2Vect/xiejunjie_300_jieba/wiki_han_word2vec_300维度.model'\n",
    "file = '../../DataSets/Word2Vect/Tencent_AILab_ChineseEmbedding/Tencent_AILab_ChineseEmbedding_Min.txt'\n",
    "# word2vec = Word2Vec.load(file)\n",
    "word2vec = KeyedVectors.load_word2vec_format(file, binary=False,limit=100000)\n",
    "# word2vec = KeyedVectors.load_word2vec_format(file, binary=False)\n",
    "word2vec.init_sims(replace=True)  # 神奇，很省内存，可以运算most_similar\n",
    "word2vec.vector_size"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "200"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 6
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "_zCpfSvDTocC",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "outputId": "e8348067-6650-4aa3-dbff-9e47a1d03b12",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1587880442337,
     "user_tz": -480,
     "elapsed": 102821,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    }
   },
   "source": [
    "print(word2vec)\n",
    "# print(word2vec.wv.vocab)\n",
    "# print(len(word2vec.index2word))\n",
    "print(len(word2vec.wv.index2word))\n",
    "print(word2vec.wv.index2word[0])\n",
    "print(word2vec.wv.index2word[1])\n",
    "print(word2vec.wv.index2word[2])\n",
    "print(word2vec.wv.index2word[1522])\n",
    "print(word2vec.wv.index2entity[1522])\n",
    "print(word2vec.similar_by_word('中国'))\n",
    "print(word2vec.similar_by_word('天才'))\n",
    "print(word2vec.wv)\n",
    "print('word2vec.wv.vocab ---- >', word2vec.wv.vocab)\n",
    "print(word2vec.wv.index2word)\n"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n",
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  import sys\n",
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  del sys.path[0]\n",
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "lzs2-uceTocG",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "6269c5c3-0989-4c94-e42d-729c7b8d7974",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1587880442339,
     "user_tz": -480,
     "elapsed": 102812,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    }
   },
   "source": [
    "wordEmbedding = [word2vec.wv[word]  for word in word2vec.wv.index2word]\n",
    "word2index = { word:i for i, word in enumerate(word2vec.wv.index2word)}\n",
    "print(wordEmbedding[:2])\n",
    "print(word2index['中国'])\n",
    "print(word2index['天才'])\n"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "[array([ 0.0991139 ,  0.10946613, -0.09485413, -0.08118325,  0.03383048,\n",
      "        0.07484313,  0.02328013,  0.00525041, -0.08925699,  0.05493119,\n",
      "       -0.1078811 ,  0.03095761, -0.0186241 , -0.02372592, -0.08212436,\n",
      "       -0.04660979,  0.06389652,  0.07494219,  0.07355529,  0.03957621,\n",
      "        0.03823885, -0.09416068, -0.10144191,  0.12308748,  0.09416068,\n",
      "        0.07652722, -0.01495872,  0.09946062, -0.01223445,  0.01817831,\n",
      "       -0.00371491, -0.07390202,  0.03249311, -0.03313703, -0.09475507,\n",
      "        0.11773799,  0.10847548, -0.02714363, -0.00559714,  0.01263071,\n",
      "       -0.09009904, -0.09926249,  0.11278479,  0.00158503, -0.06394605,\n",
      "       -0.07533845, -0.0761805 ,  0.04200329,  0.00500275,  0.03298843,\n",
      "       -0.10436431, -0.09430927, -0.00321959,  0.02833241,  0.06315354,\n",
      "       -0.07850851,  0.10104565,  0.0229334 ,  0.02773802, -0.01505778,\n",
      "        0.07395155, -0.05666482, -0.05195926,  0.05344522, -0.01867363,\n",
      "        0.02550908,  0.04467803, -0.10124378, -0.04913592,  0.07216839,\n",
      "        0.1048101 ,  0.09738027, -0.07543752, -0.05220692, -0.02253714,\n",
      "        0.04958172, -0.09381396,  0.07424875,  0.06904786, -0.03957621,\n",
      "       -0.03843697, -0.05542652,  0.10471103,  0.09688495, -0.02922399,\n",
      "        0.10436431,  0.11937256,  0.04497522,  0.12338468, -0.07707208,\n",
      "       -0.03794165, -0.05220692, -0.07201979,  0.06969178,  0.03913043,\n",
      "        0.01050082, -0.0543368 ,  0.03774352,  0.07578424,  0.00480462,\n",
      "        0.05646669, -0.12264168,  0.10684092,  0.01188772, -0.04537148,\n",
      "       -0.05161254, -0.01852504, -0.07271325, -0.10822783, -0.07028617,\n",
      "        0.10594934, -0.04383598, -0.06637312,  0.0574078 , -0.04220142,\n",
      "        0.11778753, -0.03972481, -0.11362683,  0.06726471, -0.00183269,\n",
      "       -0.08638413,  0.0241717 ,  0.0357127 , -0.01193726,  0.04517335,\n",
      "       -0.09802419,  0.02184369,  0.04497522, -0.07454593,  0.00351679,\n",
      "       -0.00148597, -0.05943862, -0.07013757, -0.11625203,  0.00054485,\n",
      "        0.00376445,  0.00024766, -0.0974298 , -0.12288935, -0.11753987,\n",
      "       -0.10713811, -0.01357182,  0.0344744 ,  0.02932305, -0.0788057 ,\n",
      "        0.12378092, -0.04982937, -0.031552  , -0.03477159,  0.11719315,\n",
      "       -0.09321957,  0.0287782 , -0.0330875 ,  0.0789543 ,  0.00099064,\n",
      "        0.10575121, -0.06984038, -0.06454044, -0.10381945, -0.09079249,\n",
      "       -0.07925149, -0.07712161,  0.06052833, -0.04621353,  0.06637312,\n",
      "        0.09138688,  0.03358282,  0.07305998,  0.06132084,  0.05795265,\n",
      "       -0.08791962, -0.08504676, -0.09054483, -0.00326912,  0.10228395,\n",
      "       -0.06776003, -0.07578424, -0.10391852,  0.0818767 , -0.10347273,\n",
      "       -0.01436433,  0.05394055, -0.1143698 , -0.11090256,  0.0357127 ,\n",
      "        0.08727571,  0.10560261,  0.05314803,  0.07831039, -0.07746834,\n",
      "       -0.09014858,  0.09842045, -0.06845348,  0.04343972,  0.01421574,\n",
      "        0.0443313 , -0.02927352,  0.00490369, -0.04175563, -0.02788662],\n",
      "      dtype=float32), array([ 9.56364945e-02, -7.56792128e-02, -2.65532918e-02,  1.28606960e-01,\n",
      "        4.71028872e-02,  4.56785783e-02,  2.16285773e-02,  5.19278906e-02,\n",
      "        9.27608833e-02,  1.09993950e-01,  1.19049344e-02,  3.36199589e-02,\n",
      "        4.97319642e-03,  4.66583017e-03, -2.56412569e-02, -2.92107239e-02,\n",
      "       -1.18591953e-02, -7.26480931e-02, -8.89348146e-03, -6.61431029e-02,\n",
      "       -5.70918173e-02,  2.28237375e-04, -2.33136024e-02,  5.21346293e-02,\n",
      "        4.03188877e-02,  3.67366113e-02, -6.67678937e-02,  4.26858738e-02,\n",
      "        6.38950318e-02, -3.78178805e-02, -1.57140754e-02,  2.80109923e-02,\n",
      "       -4.12350371e-02,  2.43175700e-02, -7.86668807e-02, -8.59133005e-02,\n",
      "        5.28742261e-02,  1.00417584e-01, -9.15849283e-02, -4.88967709e-02,\n",
      "        1.54762315e-02,  2.61032186e-03,  2.21467093e-01,  6.74109831e-02,\n",
      "       -7.55945966e-02,  4.33984920e-02, -9.25212055e-02, -2.91984200e-01,\n",
      "       -5.85092679e-02, -9.71211791e-02, -1.14684932e-01, -1.02505563e-02,\n",
      "       -1.44081369e-01,  7.74072334e-02, -1.28023326e-03,  8.74757487e-03,\n",
      "        7.98693579e-03,  1.28128529e-02,  8.92184004e-02,  1.66421179e-02,\n",
      "       -2.36383490e-02,  7.04549104e-02,  9.82404053e-02, -8.23232532e-02,\n",
      "       -9.34401061e-03, -2.04997417e-02, -3.42689976e-02,  4.82276082e-02,\n",
      "       -3.73755842e-02, -1.55965257e-02, -4.41463217e-02, -1.96220097e-03,\n",
      "        4.36454788e-02,  1.07264705e-01, -6.32716045e-02,  6.17096238e-02,\n",
      "        3.75379585e-02,  2.36534420e-02,  7.28745013e-02,  2.82749049e-02,\n",
      "        1.69650335e-02,  1.09437302e-01,  3.35147604e-02,  7.81953111e-02,\n",
      "        1.02288760e-01, -8.58478993e-02, -9.46110263e-02, -2.33268645e-02,\n",
      "       -1.23255961e-01, -5.35008535e-02,  9.74555314e-02, -4.39651944e-02,\n",
      "        1.61742084e-02, -1.16473325e-01,  1.00525068e-02,  3.27888802e-02,\n",
      "        5.02533875e-02, -4.78626117e-02, -8.03418383e-02,  4.43946831e-02,\n",
      "       -3.14190015e-02,  8.98194090e-02,  3.52372881e-03,  3.30999047e-02,\n",
      "       -1.26196057e-01,  9.93825123e-02, -2.59175207e-02, -1.47043407e-01,\n",
      "       -2.20141583e-03, -1.88293532e-02, -5.41549213e-02, -7.31535032e-02,\n",
      "        2.98647899e-02, -4.23259102e-02,  6.39201840e-03, -1.00189798e-01,\n",
      "       -2.67257262e-02, -8.10750350e-02, -1.97450481e-02, -6.93617463e-02,\n",
      "       -2.76674936e-03, -1.27883822e-01, -2.51015369e-03,  4.42446582e-02,\n",
      "        6.73364326e-02,  9.04153883e-02, -4.03111093e-02,  2.44543310e-02,\n",
      "        1.77183561e-02,  2.72901449e-02, -6.07500188e-02,  8.96757934e-03,\n",
      "        1.02878794e-01, -8.05627629e-02, -1.88429847e-01, -2.01576129e-02,\n",
      "       -5.49773052e-02, -4.93655950e-02, -7.50118808e-04,  1.67948864e-02,\n",
      "       -1.11205570e-01, -1.25076368e-01, -1.45183668e-01, -3.62417176e-02,\n",
      "        2.50841565e-02, -6.58389330e-02,  7.69278854e-02, -5.97213488e-03,\n",
      "       -6.66251928e-02,  6.18747398e-02,  1.34687498e-02, -6.44983202e-02,\n",
      "       -8.41134787e-02, -3.66423912e-02, -5.19310907e-02,  3.25492099e-02,\n",
      "        6.17329478e-02,  6.49214089e-02,  6.60493374e-02, -1.13942139e-01,\n",
      "        2.07953975e-01, -3.52519266e-02, -6.95689465e-04,  1.36417329e-01,\n",
      "        7.33081028e-02,  3.93089689e-02, -9.75900069e-02,  3.79733928e-02,\n",
      "       -1.82955802e-04,  6.16680011e-02, -3.11482261e-04, -7.92564545e-03,\n",
      "       -1.22356275e-02,  5.11832573e-02,  4.71431389e-03, -5.70469946e-02,\n",
      "        1.43949632e-02,  3.73673514e-02,  3.26800235e-02,  5.25357621e-03,\n",
      "       -4.18836176e-02, -1.79840997e-02, -5.15734144e-02,  7.82620907e-02,\n",
      "        1.22895995e-02, -3.52455229e-02, -2.42178608e-02,  1.15557171e-01,\n",
      "       -1.60411075e-02,  1.86935104e-02,  1.27075613e-01,  3.89663875e-02,\n",
      "        3.18297371e-03, -2.23722924e-02,  1.27672508e-01,  7.75343850e-02,\n",
      "        3.11738383e-02, -1.27439693e-01, -7.93150067e-02,  1.62094273e-02],\n",
      "      dtype=float32)]\n",
      "221\n",
      "5559\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "cy9vhIUoTocL",
    "colab_type": "text"
   },
   "source": [
    "## 读取训练数据"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "gZGmiumwTocN",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "outputId": "541b26b5-dfe1-43ed-c870-f0c7f03db531",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1587880442340,
     "user_tz": -480,
     "elapsed": 102795,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    }
   },
   "source": [
    "def compute_ngrams(word, num_min = 1, num_max = 3):\n",
    "    ngrams =[]\n",
    "    for ngram_length in range(num_min, min(len(word), num_max) + 1):\n",
    "        for i in range(len(word) - ngram_length + 1):\n",
    "            # print(i, i + ngram_length)\n",
    "            ngrams.append(word[i : i + ngram_length])\n",
    "    # print(ngrams)\n",
    "    return list(set(ngrams))\n",
    "\n",
    "print(compute_ngrams('you'))\n",
    "print(compute_ngrams('I Think'))\n",
    "print(compute_ngrams('中华人民共和国万岁'))"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['ou', 'y', 'o', 'you', 'yo', 'u']\n",
      "['h', 'I ', 'Th', 'Thi', 'hin', 'I', 'in', 'k', 'nk', 'n', 'T', 'I T', ' Th', 'ink', 'i', ' T', ' ', 'hi']\n",
      "['民共', '华人', '华', '人民', '和国', '和', '和国万', '万岁', '人', '国万', '万', '中华', '国万岁', '民共和', '共', '华人民', '人民共', '中华人', '国', '民', '中', '岁', '共和国', '共和']\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "ulsoTSc2TocT",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# 从词向量文本文件 word2vec 中获取词向量，如果获取到直接返回，若没有获取到，那么把这个词拆开\n",
    "# 成为 ngrams 的新词组，并在 word2vec 中找新词组中的词向量并相加取平均，最后得到平均词向量输出\n",
    "def wordVec(word, word2vec, min_n = 1, max_n = 3):\n",
    "    # 确认词向量维度\n",
    "    word_size = word2vec.wv.syn0[0].shape[0]\n",
    "\n",
    "    # 如果在词典之中，直接返回词向量\n",
    "    if word in word2vec.wv.vocab.keys():\n",
    "        return word2vec[word]\n",
    "    else:\n",
    "        # 计算word的ngrams词组\n",
    "        ngrams = compute_ngrams(word, min_n, max_n)\n",
    "        # 不在词典的情况下\n",
    "        word_vec = numpy.zeros(word_size, dtype=numpy.float32)\n",
    "        ngrams_found = 0\n",
    "        ngrams_single = [ng for ng in ngrams if len(ng) == 1]\n",
    "        ngrams_more = [ng for ng in ngrams if len(ng) > 1]\n",
    "        # 先只接受2个单词长度以上的词向量\n",
    "        for ngram in ngrams_more:\n",
    "            if ngram in word2vec.wv.vocab.keys():\n",
    "                word_vec += word2vec[ngram]\n",
    "                ngrams_found += 1\n",
    "                #print(ngram)\n",
    "        # 如果，没有匹配到，那么最后是考虑单个词向量\n",
    "        if ngrams_found == 0:\n",
    "            for ngram in ngrams_single:\n",
    "                word_vec += word2vec[ngram]\n",
    "                ngrams_found += 1\n",
    "        if word_vec.any():\n",
    "            return word_vec / max(1, ngrams_found)\n",
    "        else:\n",
    "            # 不抛出异常，而是打印提示，并返回0向量。\n",
    "            print(KeyError('all ngrams for word %s absent from model' % word))\n",
    "            return word_vec"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "08tJP4HHTocX",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class DotDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        dict.__init__(self, *args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "        "
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "XGUX_N8KToca",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "outputId": "b297c760-6990-4081-f0ed-9c8571cdb46b",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1587880442798,
     "user_tz": -480,
     "elapsed": 103220,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "dic = {'a':[1, 2, 3, 4], 'b':[5, 6, 7, 8],\n",
    "'c':[9, 10, 11, 12], 'd':[13, 14, 15, 16]}\n",
    "df1=pd.DataFrame(dic)\n",
    "print(df1)\n",
    "df2=df1.sample(frac=0.75)\n",
    "print(df2)\n",
    "# rowlist=[]\n",
    "# for indexs in df2.index:\n",
    "#     rowlist.append(indexs)\n",
    "df3=df1.drop(df2.index.to_list(),axis=0)\n",
    "print(df3)"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "   a  b   c   d\n",
      "0  1  5   9  13\n",
      "1  2  6  10  14\n",
      "2  3  7  11  15\n",
      "3  4  8  12  16\n",
      "   a  b   c   d\n",
      "2  3  7  11  15\n",
      "1  2  6  10  14\n",
      "0  1  5   9  13\n",
      "   a  b   c   d\n",
      "3  4  8  12  16\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "1oqKk8CETocd",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def isNan(a):\n",
    "    return a != a\n",
    "\n",
    "class RatingData(data.Dataset):\n",
    "    def __init__(self, path, \n",
    "                 word2index, \n",
    "                 max_row = -1, \n",
    "                 trainTestRate = 0.85, \n",
    "                 isTrain = True, \n",
    "                 wordCuter = thulac,\n",
    "                 clean_file_name = 'ratings_clean_4_HAN.csv',\n",
    "                 ):\n",
    "        self.token_list = []\n",
    "        self.label_list = []\n",
    "        # self.token_positions = torch.tensor([i for i in range(100)])\n",
    "\n",
    "        print(' balance_data.csv 所在path:',path) # 地址不应该包含 ratings.csv\n",
    "\n",
    "        ratings_clean_filename = os.path.join(path, clean_file_name)\n",
    "        ratings_filename = os.path.join(path, 'balance_data_means.csv')\n",
    "        if os.path.isfile(ratings_clean_filename):\n",
    "            clean_pd = pd.read_csv(ratings_clean_filename)\n",
    "        else:\n",
    "            print('没有找到缓存的文件%s, 读取源文件%s'%(ratings_clean_filename, ratings_filename))\n",
    "            ratings_pd = pd.read_csv(ratings_filename)\n",
    "            print('开始生成缓存文件%s'%(ratings_clean_filename))\n",
    "            clean_pd = pd.DataFrame({\n",
    "                'labels':[],\n",
    "                'data':[],\n",
    "            })\n",
    "            nonRatingCount = 0\n",
    "            for i, row in ratings_pd.iterrows():\n",
    "                if max_row != -1 and i > max_row:\n",
    "                    break\n",
    "                if not isinstance(row['data'], str) or row['data'] == '':\n",
    "                    # print(i + 1, row['comment'])\n",
    "                    nonRatingCount += 1\n",
    "                    continue\n",
    "                r0 = row['labels']\n",
    "                if r0 == -1:\n",
    "                    r0 = 2\n",
    "                    \n",
    "                if i % 10000 == 9999:\n",
    "                    print(i + 1, r0)\n",
    "\n",
    "                words = list(wordCuter.cut(row['data']))\n",
    "                # 0 在词向量集中是‘，’，换个词向量集可能表示其他\n",
    "                token = [ word2index[words[i]] if i < len(words) and words[i] in word2index else 0 \n",
    "                          for i in range(100)] \n",
    "                \n",
    "                newRow = DotDict()\n",
    "\n",
    "                newRow.labels = [r0]\n",
    "                newRow.data = [json.dumps(token)]\n",
    "\n",
    "                clean_pd = clean_pd.append(pd.DataFrame(newRow), ignore_index=True)\n",
    "            print('空的评论数量： %d'%(nonRatingCount))\n",
    "            clean_pd.to_csv(ratings_clean_filename)\n",
    "\n",
    "        # 读取\n",
    "        if isTrain:\n",
    "            temp_pd = clean_pd.sample(frac=trainTestRate)\n",
    "        else:\n",
    "            temp_pd = clean_pd.sample(frac=trainTestRate)\n",
    "            temp_pd = clean_pd.drop(temp_pd.index.tolist(), axis=0)\n",
    "\n",
    "        for i, row in temp_pd.iterrows():\n",
    "            if max_row != -1 and i > max_row:\n",
    "                break\n",
    "\n",
    "            self.label_list.append(torch.tensor(row['labels']).long())\n",
    "            self.token_list.append(torch.from_numpy(numpy.array( json.loads(row['data']) ) ).long())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.token_list[index], self.label_list[index]#, self.token_positions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_list)\n",
    "    "
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "BB-910swToci",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "outputId": "9396afad-2af2-4158-bd31-cb5842153ac5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1587880624698,
     "user_tz": -480,
     "elapsed": 285102,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    }
   },
   "source": [
    "ratingData = RatingData('../../DataSets/yf_dianping',\n",
    "                             word2index = word2index,\n",
    "                             # max_row= 200000,\n",
    "                             isTrain=True,\n",
    "                             trainTestRate = 0.8,\n",
    "                             wordCuter= jieba,\n",
    "                             clean_file_name='balance_data_4_HAN_means.csv',\n",
    "                             )\n",
    "trainLoader = torch.utils.data.DataLoader(dataset=ratingData,\n",
    "                                          batch_size=128,\n",
    "                                          shuffle = True,\n",
    "                                          num_workers = 8,\n",
    "                                          )\n",
    "ratingData2 = RatingData('../../DataSets/yf_dianping',\n",
    "                             word2index = word2index,\n",
    "                             # max_row= 200000,\n",
    "                             isTrain=False,\n",
    "                             trainTestRate = 0.8,\n",
    "                             wordCuter= jieba,\n",
    "                             clean_file_name='balance_data_4_HAN_means.csv',\n",
    "                             )\n",
    "testLoader = torch.utils.data.DataLoader(dataset=ratingData2,\n",
    "                                          batch_size=128,\n",
    "                                          shuffle = True,\n",
    "                                          num_workers = 8,\n",
    "                                          )\n",
    "print(len(ratingData.label_list))\n",
    "print(len(ratingData2.label_list))\n"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      " balance_data.csv 所在path: ../../DataSets/yf_dianping\n",
      "没有找到缓存的文件../../DataSets/yf_dianping\\balance_data_4_HAN_means.csv, 读取源文件../../DataSets/yf_dianping\\balance_data_means.csv\n",
      "开始生成缓存文件../../DataSets/yf_dianping\\balance_data_4_HAN_means.csv\n",
      "10000 2\n",
      "20000 2\n",
      "30000 2\n",
      "40000 2\n",
      "50000 2\n",
      "60000 2\n",
      "70000 2\n",
      "80000 2\n",
      "90000 2\n",
      "100000 2\n",
      "110000 2\n",
      "120000 2\n",
      "130000 2\n",
      "140000 2\n",
      "150000 2\n",
      "160000 2\n",
      "170000 2\n",
      "180000 2\n",
      "190000 2\n",
      "200000 2\n",
      "210000 2\n",
      "220000 2\n",
      "230000 2\n",
      "240000 2\n",
      "250000 2\n",
      "260000 2\n",
      "270000 2\n",
      "280000 2\n",
      "290000 2\n",
      "300000 2\n",
      "320000 2\n",
      "330000 2\n",
      "340000 2\n",
      "350000 2\n",
      "360000 2\n",
      "370000 2\n",
      "380000 2\n",
      "390000 2\n",
      "400000 2\n",
      "410000 2\n",
      "420000 2\n",
      "430000 2\n",
      "440000 2\n",
      "450000 2\n",
      "460000 2\n",
      "470000 2\n",
      "480000 2\n",
      "490000 2\n",
      "500000 2\n",
      "510000 2\n",
      "520000 2\n",
      "530000 2\n",
      "540000 2\n",
      "550000 2\n",
      "560000 2\n",
      "570000 2\n",
      "580000 2\n",
      "590000 2\n",
      "600000 2\n",
      "610000 2\n",
      "620000 2\n",
      "630000 2\n",
      "640000 2\n",
      "650000 2\n",
      "660000 2\n",
      "690000 2\n",
      "700000 2\n",
      "710000 2\n",
      "720000 2\n",
      "730000 2\n",
      "740000 2\n",
      "750000 2\n",
      "760000 2\n",
      "770000 2\n",
      "780000 2\n",
      "800000 2\n",
      "810000 2\n",
      "820000 2\n",
      "830000 2\n",
      "840000 2\n",
      "850000 2\n",
      "860000 2\n",
      "870000 2\n",
      "880000 2\n",
      "890000 2\n",
      "900000 2\n",
      "910000 2\n",
      "920000 2\n",
      "930000 2\n",
      "940000 2\n",
      "950000 2\n",
      "960000 1\n",
      "970000 1\n",
      "980000 1\n",
      "990000 1\n",
      "1000000 1\n",
      "1010000 1\n",
      "1020000 1\n",
      "1030000 1\n",
      "1040000 1\n",
      "1050000 1\n",
      "1070000 1\n",
      "1090000 1\n",
      "1100000 1\n",
      "1110000 1\n",
      "1120000 1\n",
      "1130000 1\n",
      "1140000 1\n",
      "1150000 1\n",
      "1160000 1\n",
      "1170000 1\n",
      "1180000 1\n",
      "1190000 1\n",
      "1200000 1\n",
      "1210000 1\n",
      "1220000 1\n",
      "1230000 1\n",
      "1240000 1\n",
      "1250000 1\n",
      "1260000 1\n",
      "1270000 1\n",
      "1280000 1\n",
      "1290000 1\n",
      "1300000 1\n",
      "1310000 1\n",
      "1320000 1\n",
      "1340000 1\n",
      "1350000 1\n",
      "1360000 1\n",
      "1370000 1\n",
      "1380000 1\n",
      "1390000 1\n",
      "1410000 1\n",
      "1420000 1\n",
      "1430000 1\n",
      "1440000 1\n",
      "1450000 1\n",
      "1460000 1\n",
      "1470000 1\n",
      "1480000 1\n",
      "1500000 1\n",
      "1510000 1\n",
      "1520000 1\n",
      "1530000 1\n",
      "1540000 1\n",
      "1550000 1\n",
      "1560000 1\n",
      "1570000 1\n",
      "1580000 1\n",
      "1590000 1\n",
      "1600000 1\n",
      "1610000 1\n",
      "1620000 1\n",
      "1630000 1\n",
      "1640000 1\n",
      "1660000 1\n",
      "1670000 1\n",
      "1680000 1\n",
      "1690000 1\n",
      "1700000 1\n",
      "1710000 1\n",
      "1720000 1\n",
      "1730000 1\n",
      "1740000 1\n",
      "1750000 1\n",
      "1760000 1\n",
      "1770000 1\n",
      "1780000 1\n",
      "1790000 1\n",
      "1800000 1\n",
      "1810000 1\n",
      "1830000 1\n",
      "1850000 1\n",
      "1860000 1\n",
      "1870000 1\n",
      "1880000 1\n",
      "1890000 1\n",
      "1900000 1\n",
      "1910000 1\n",
      "1920000 0\n",
      "1930000 0\n",
      "1940000 0\n",
      "1950000 0\n",
      "1960000 0\n",
      "1970000 0\n",
      "1980000 0\n",
      "1990000 0\n",
      "2000000 0\n",
      "2010000 0\n",
      "2020000 0\n",
      "2030000 0\n",
      "2040000 0\n",
      "2050000 0\n",
      "2060000 0\n",
      "2070000 0\n",
      "2080000 0\n",
      "2090000 0\n",
      "2110000 0\n",
      "2130000 0\n",
      "2140000 0\n",
      "2150000 0\n",
      "2160000 0\n",
      "2170000 0\n",
      "2180000 0\n",
      "2190000 0\n",
      "2200000 0\n",
      "2210000 0\n",
      "2220000 0\n",
      "2230000 0\n",
      "2240000 0\n",
      "2250000 0\n",
      "2260000 0\n",
      "2280000 0\n",
      "2290000 0\n",
      "2300000 0\n",
      "2310000 0\n",
      "2320000 0\n",
      "2330000 0\n",
      "2340000 0\n",
      "2350000 0\n",
      "2360000 0\n",
      "2370000 0\n",
      "2380000 0\n",
      "2390000 0\n",
      "2400000 0\n",
      "2410000 0\n",
      "2420000 0\n",
      "2430000 0\n",
      "2440000 0\n",
      "2450000 0\n",
      "2460000 0\n",
      "2470000 0\n",
      "2480000 0\n",
      "2490000 0\n",
      "2500000 0\n",
      "2530000 0\n",
      "2550000 0\n",
      "2560000 0\n",
      "2570000 0\n",
      "2580000 0\n",
      "2590000 0\n",
      "2600000 0\n",
      "2610000 0\n",
      "2620000 0\n",
      "2630000 0\n",
      "2640000 0\n",
      "2650000 0\n",
      "2660000 0\n",
      "2670000 0\n",
      "2680000 0\n",
      "2690000 0\n",
      "2700000 0\n",
      "2720000 0\n",
      "2730000 0\n",
      "2750000 0\n",
      "2770000 0\n",
      "2780000 0\n",
      "2790000 0\n",
      "2800000 0\n",
      "2810000 0\n",
      "2820000 0\n",
      "2830000 0\n",
      "2840000 0\n",
      "2850000 0\n",
      "2860000 0\n",
      "空的评论数量： 219352\n",
      " balance_data.csv 所在path: ../../DataSets/yf_dianping\n",
      "2119418\n",
      "529854\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\fuliu\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.518 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "pQYgCURnTocn",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "64f5e1b3-44a1-4364-c8e4-a5177cd8a8b4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1587880629552,
     "user_tz": -480,
     "elapsed": 289943,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    }
   },
   "source": [
    "#创建模型\n",
    "wordEmbedding = torch.FloatTensor(wordEmbedding)\n",
    "num_embeddings = len(word2vec.wv.index2word)\n",
    "model = HAN(num_embeddings, \n",
    "            num_classes = 3,\n",
    "            embedding_dim=word2vec.wv.vector_size, \n",
    "            num_words = 100,\n",
    "            hidden_size_gru = 300,\n",
    "            hidden_size_att = 600,\n",
    "            )\n",
    "print(model)\n",
    "\n",
    "modelParams = model.parameters()\n",
    "for param in modelParams:\n",
    "    if len(param.data.shape) > 1:\n",
    "        print('---', param.data.shape, param.data)\n",
    "        torch.nn.init.kaiming_normal(param.data)\n",
    "        print('--->', param.data)\n",
    "        \n",
    "model.embed.from_pretrained(wordEmbedding)\n"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "C:\\Users\\fuliu\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "HAN(\n",
      "  (embed): Embedding(99999, 200, padding_idx=0)\n",
      "  (GRU1): GRU(200, 300, batch_first=True, bidirectional=True)\n",
      "  (self_attention1): SelfAttention(\n",
      "    (W): Linear(in_features=600, out_features=600, bias=True)\n",
      "    (U): Linear(in_features=600, out_features=1, bias=True)\n",
      "  )\n",
      "  (GRU2): GRU(600, 300, batch_first=True, bidirectional=True)\n",
      "  (self_attention2): SelfAttention(\n",
      "    (W): Linear(in_features=600, out_features=600, bias=True)\n",
      "    (U): Linear(in_features=600, out_features=1, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=600, out_features=3, bias=True)\n",
      ")\n",
      "--- torch.Size([99999, 200]) tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.3421,  0.6240,  0.7641,  ...,  0.1275,  0.0196, -0.3185],\n",
      "        [ 1.4047,  0.1457,  0.0804,  ...,  0.4322, -0.7157, -0.5866],\n",
      "        ...,\n",
      "        [ 0.1949, -0.3447, -0.3022,  ..., -0.1634,  0.4749, -0.8786],\n",
      "        [-0.1259,  0.0478,  1.8187,  ...,  0.9310,  1.2421, -0.5992],\n",
      "        [-0.6094, -0.7478,  2.0046,  ..., -0.3269,  0.3940,  0.3218]])\n",
      "---> tensor([[ 0.0863, -0.2403, -0.0113,  ..., -0.0726,  0.0562,  0.1239],\n",
      "        [ 0.0974, -0.0738, -0.1205,  ...,  0.2184, -0.0804,  0.0184],\n",
      "        [-0.0557,  0.1320, -0.1772,  ...,  0.1484,  0.0722, -0.0769],\n",
      "        ...,\n",
      "        [-0.0131,  0.0730,  0.0176,  ...,  0.1493, -0.0733,  0.0939],\n",
      "        [ 0.0061,  0.1129, -0.0311,  ...,  0.0588,  0.1337,  0.0691],\n",
      "        [ 0.0297,  0.0565,  0.2045,  ..., -0.1557,  0.1038, -0.0295]])\n",
      "--- torch.Size([900, 200]) tensor([[-0.0113, -0.0314, -0.0239,  ...,  0.0575, -0.0053,  0.0161],\n",
      "        [-0.0474,  0.0377,  0.0353,  ..., -0.0163,  0.0571,  0.0439],\n",
      "        [ 0.0559,  0.0189,  0.0416,  ...,  0.0127,  0.0212, -0.0394],\n",
      "        ...,\n",
      "        [-0.0474,  0.0134, -0.0010,  ...,  0.0158,  0.0407,  0.0345],\n",
      "        [ 0.0425,  0.0247,  0.0446,  ...,  0.0532,  0.0379,  0.0338],\n",
      "        [ 0.0433,  0.0110, -0.0347,  ...,  0.0253,  0.0471,  0.0467]])\n",
      "---> tensor([[-0.0393,  0.1160, -0.1169,  ..., -0.0211,  0.0033,  0.1463],\n",
      "        [-0.1020,  0.0614,  0.0282,  ..., -0.3698,  0.2402, -0.0529],\n",
      "        [ 0.0463,  0.0341, -0.0571,  ..., -0.1217,  0.1270, -0.1070],\n",
      "        ...,\n",
      "        [ 0.0155, -0.0721, -0.3228,  ..., -0.0797, -0.1904, -0.0901],\n",
      "        [-0.0360, -0.0980, -0.0128,  ..., -0.0601,  0.0769,  0.3085],\n",
      "        [ 0.0228,  0.0405, -0.0299,  ...,  0.1575,  0.0149, -0.0992]])\n",
      "--- torch.Size([900, 300]) tensor([[-0.0485, -0.0143,  0.0482,  ..., -0.0134,  0.0174, -0.0305],\n",
      "        [-0.0261, -0.0200, -0.0218,  ..., -0.0425, -0.0012,  0.0333],\n",
      "        [ 0.0420, -0.0295,  0.0131,  ...,  0.0449,  0.0051,  0.0492],\n",
      "        ...,\n",
      "        [-0.0420, -0.0061, -0.0282,  ..., -0.0483,  0.0494, -0.0180],\n",
      "        [-0.0316, -0.0471, -0.0133,  ..., -0.0419, -0.0536,  0.0124],\n",
      "        [ 0.0544, -0.0135,  0.0199,  ...,  0.0051, -0.0233,  0.0160]])\n",
      "---> tensor([[-0.0045,  0.0636, -0.0992,  ..., -0.0384,  0.1303, -0.0139],\n",
      "        [ 0.0068, -0.1106, -0.0224,  ...,  0.0141,  0.0773, -0.0596],\n",
      "        [-0.0424,  0.0252,  0.0081,  ..., -0.0496,  0.0928,  0.0477],\n",
      "        ...,\n",
      "        [-0.0211,  0.0358, -0.0214,  ..., -0.0679,  0.0881,  0.0021],\n",
      "        [-0.1513, -0.1067,  0.0120,  ...,  0.0976,  0.1293, -0.0253],\n",
      "        [ 0.0937,  0.0795,  0.0525,  ..., -0.0975, -0.0034,  0.0834]])\n",
      "--- torch.Size([900, 200]) tensor([[ 0.0147, -0.0447,  0.0095,  ...,  0.0155, -0.0430, -0.0127],\n",
      "        [-0.0465,  0.0353, -0.0160,  ..., -0.0061,  0.0392,  0.0399],\n",
      "        [ 0.0241, -0.0414,  0.0518,  ...,  0.0062,  0.0021, -0.0244],\n",
      "        ...,\n",
      "        [-0.0489, -0.0095,  0.0495,  ...,  0.0097,  0.0220,  0.0236],\n",
      "        [ 0.0099, -0.0484,  0.0431,  ...,  0.0300,  0.0472, -0.0093],\n",
      "        [-0.0438, -0.0304,  0.0421,  ...,  0.0467,  0.0100,  0.0139]])\n",
      "---> tensor([[-0.0105,  0.0782, -0.0804,  ...,  0.0707, -0.0553, -0.0408],\n",
      "        [-0.0659,  0.0294, -0.2064,  ..., -0.0732, -0.0716,  0.0777],\n",
      "        [ 0.1221,  0.2110, -0.0048,  ...,  0.0259, -0.1669, -0.0238],\n",
      "        ...,\n",
      "        [-0.1971, -0.0901, -0.0539,  ...,  0.1028,  0.1388,  0.0287],\n",
      "        [ 0.0119, -0.0220,  0.1263,  ...,  0.0720, -0.0461,  0.0622],\n",
      "        [ 0.0309,  0.1428,  0.0515,  ...,  0.0585,  0.0823,  0.2353]])\n",
      "--- torch.Size([900, 300]) tensor([[-0.0284, -0.0061, -0.0072,  ...,  0.0046, -0.0267,  0.0153],\n",
      "        [ 0.0283, -0.0154, -0.0423,  ...,  0.0485, -0.0187,  0.0430],\n",
      "        [ 0.0004, -0.0562,  0.0137,  ..., -0.0338,  0.0260,  0.0363],\n",
      "        ...,\n",
      "        [ 0.0382,  0.0561, -0.0308,  ...,  0.0387, -0.0488,  0.0285],\n",
      "        [-0.0236, -0.0001,  0.0081,  ..., -0.0562, -0.0497,  0.0019],\n",
      "        [ 0.0556, -0.0371, -0.0268,  ...,  0.0028, -0.0062,  0.0444]])\n",
      "---> tensor([[-0.0388,  0.0428, -0.0726,  ...,  0.0427,  0.0236, -0.0492],\n",
      "        [-0.0391,  0.1634,  0.2122,  ...,  0.0428,  0.1535, -0.0281],\n",
      "        [-0.0189, -0.1014, -0.0641,  ..., -0.0598, -0.1614, -0.0085],\n",
      "        ...,\n",
      "        [ 0.0840, -0.0140,  0.1827,  ...,  0.0098,  0.0221,  0.0371],\n",
      "        [ 0.0339, -0.0198,  0.0425,  ...,  0.0413,  0.0436,  0.0832],\n",
      "        [-0.0116, -0.0104, -0.0721,  ...,  0.0801,  0.0616, -0.1095]])\n",
      "--- torch.Size([600, 600]) tensor([[-0.0207, -0.0021,  0.0400,  ...,  0.0142, -0.0077, -0.0222],\n",
      "        [ 0.0259, -0.0031, -0.0341,  ..., -0.0374,  0.0186, -0.0045],\n",
      "        [-0.0123,  0.0232, -0.0222,  ...,  0.0386, -0.0327, -0.0275],\n",
      "        ...,\n",
      "        [-0.0203, -0.0167, -0.0003,  ...,  0.0384,  0.0094,  0.0305],\n",
      "        [-0.0286,  0.0231,  0.0287,  ..., -0.0066, -0.0025, -0.0070],\n",
      "        [ 0.0367,  0.0261, -0.0238,  ..., -0.0030, -0.0207, -0.0131]])\n",
      "---> tensor([[-0.0558,  0.0149, -0.0392,  ...,  0.0507,  0.0795, -0.0126],\n",
      "        [ 0.0201,  0.0673, -0.0162,  ...,  0.0063, -0.0170,  0.0526],\n",
      "        [-0.0217, -0.0098, -0.0270,  ...,  0.0974, -0.0097, -0.0600],\n",
      "        ...,\n",
      "        [-0.0642,  0.0356, -0.0117,  ...,  0.0070, -0.0447, -0.0974],\n",
      "        [ 0.0250,  0.0747, -0.0961,  ..., -0.0104, -0.0462,  0.0137],\n",
      "        [-0.0245,  0.0061,  0.0547,  ..., -0.0639, -0.0640, -0.0667]])\n",
      "--- torch.Size([1, 600]) tensor([[-1.6624e-02,  3.6064e-02,  2.2691e-02, -2.9042e-02,  3.0417e-02,\n",
      "         -2.2361e-02, -2.7087e-02, -1.9378e-02,  3.0204e-02,  9.9396e-03,\n",
      "         -6.5811e-03, -3.6244e-02, -1.3965e-02, -4.5547e-03,  1.2691e-02,\n",
      "         -2.9286e-02, -7.7810e-03, -2.4784e-02,  4.1361e-03,  8.1740e-03,\n",
      "          3.7747e-02, -2.6184e-02, -3.5445e-02, -2.6029e-02, -3.3124e-02,\n",
      "          1.3700e-02, -3.6647e-03,  1.1087e-02, -1.7370e-02,  1.8996e-02,\n",
      "         -6.4045e-04,  2.9201e-02, -2.1601e-02,  2.4112e-02,  2.2842e-02,\n",
      "         -1.8086e-02, -8.7679e-03, -3.4454e-02,  3.1728e-02, -3.7158e-02,\n",
      "          7.5442e-03,  1.0250e-02, -2.7843e-02, -2.9655e-02,  3.5576e-02,\n",
      "         -2.1051e-03,  2.2158e-02, -3.0704e-02, -2.4805e-02,  1.7283e-02,\n",
      "         -1.2995e-03, -2.7288e-02,  2.9390e-02,  1.8878e-02,  1.6133e-02,\n",
      "         -3.0380e-02, -3.1533e-02,  2.7533e-02, -2.7579e-02, -2.0378e-03,\n",
      "          1.2409e-02, -3.9223e-02, -6.7178e-03, -4.3567e-03, -1.0458e-02,\n",
      "         -2.5875e-02,  6.8691e-03,  1.2783e-02, -6.5400e-03,  2.2431e-02,\n",
      "         -1.3133e-02,  2.6313e-02, -1.0540e-02,  1.5171e-02, -2.1970e-02,\n",
      "         -2.3087e-02,  3.7203e-02,  2.9720e-02,  3.8258e-02,  1.0028e-02,\n",
      "         -2.1011e-02,  1.4945e-02, -3.3661e-02, -3.8956e-02,  1.0792e-02,\n",
      "         -3.7151e-02,  1.7523e-02, -2.2534e-02,  1.4503e-02, -3.3402e-02,\n",
      "          2.0642e-02, -4.0307e-02,  3.8834e-02,  8.0673e-03,  3.9373e-02,\n",
      "          2.3765e-02,  1.1685e-02,  1.6309e-02, -8.4751e-03, -3.9797e-02,\n",
      "          2.1751e-02, -3.9931e-02, -2.7273e-02,  7.1248e-04,  2.4006e-02,\n",
      "         -8.6177e-04, -9.8411e-03,  2.9982e-03, -3.5109e-02,  7.3671e-03,\n",
      "         -1.2587e-02, -3.3205e-02, -3.3036e-02,  9.0598e-03, -3.5897e-02,\n",
      "         -1.1863e-02, -2.0791e-02, -5.4269e-03, -6.9795e-04,  3.0633e-02,\n",
      "         -1.7889e-02,  4.0277e-02, -2.8737e-02, -4.6424e-03,  3.4394e-02,\n",
      "          2.6127e-02, -2.6044e-02, -6.6147e-03, -3.5917e-02,  2.1942e-02,\n",
      "          2.9715e-03,  2.3119e-03, -1.2082e-02, -1.9029e-02,  2.8906e-02,\n",
      "          3.3780e-02, -7.6207e-03,  8.8852e-03,  3.2244e-02, -2.3524e-02,\n",
      "          5.2505e-04, -1.4704e-02, -2.0475e-02,  1.8961e-02,  3.5877e-02,\n",
      "          2.7625e-02,  1.6061e-02,  1.7889e-02, -1.6085e-03, -1.5643e-02,\n",
      "          2.8599e-02, -1.8432e-02,  4.0617e-02, -1.0920e-02,  3.9085e-02,\n",
      "         -1.0850e-02, -3.2693e-02, -4.9684e-03, -3.1652e-02,  3.2362e-02,\n",
      "         -3.0138e-02, -1.0686e-02, -3.5171e-02, -8.0219e-03, -2.5426e-02,\n",
      "          1.7963e-02,  2.0203e-02, -3.8841e-02,  2.3061e-02,  1.4006e-02,\n",
      "          3.0041e-02, -3.1968e-02,  1.1916e-02,  2.6788e-02,  2.0470e-02,\n",
      "         -3.9852e-02, -3.4013e-02,  9.5650e-03, -2.0568e-02,  3.0304e-02,\n",
      "          3.5383e-02,  1.6021e-02,  1.8368e-02,  1.3735e-02,  6.8524e-04,\n",
      "          1.7762e-03, -3.4430e-02,  3.4635e-02, -3.2159e-02,  2.8286e-02,\n",
      "          7.2736e-03,  1.7765e-02, -6.9944e-03, -1.4027e-02, -3.1570e-02,\n",
      "         -9.4408e-03,  3.0176e-02,  2.7398e-02, -8.6059e-03, -3.5210e-02,\n",
      "         -3.2462e-02,  3.3991e-02,  1.0129e-02,  2.4367e-02,  1.2083e-02,\n",
      "         -5.2924e-04,  1.7095e-02, -2.3082e-02,  2.3408e-02, -3.0427e-02,\n",
      "          1.5222e-02, -1.1040e-02, -9.0587e-03, -1.6628e-02,  1.4188e-02,\n",
      "         -3.2860e-02,  8.2254e-03,  2.2072e-02,  2.4627e-02,  3.6560e-02,\n",
      "         -4.0788e-02, -3.0418e-02, -6.7202e-03, -3.7630e-02,  3.9739e-02,\n",
      "          6.8663e-03,  2.6169e-03,  3.5358e-02, -1.8869e-02, -3.4124e-02,\n",
      "         -1.1741e-02, -3.4814e-02, -2.9336e-02,  5.7310e-03, -3.4264e-02,\n",
      "         -1.0474e-02, -1.8171e-02, -1.7311e-02,  4.0460e-02,  1.0347e-02,\n",
      "          3.2492e-02,  2.7389e-02, -9.9051e-04, -3.7903e-02, -9.2416e-03,\n",
      "         -3.0321e-03, -3.5287e-02, -2.8856e-02, -1.5805e-02,  1.5520e-02,\n",
      "         -1.8704e-02, -2.0815e-02,  1.9847e-02, -4.0218e-02, -7.8243e-03,\n",
      "          2.1951e-02, -3.4111e-02,  3.3350e-02,  3.7459e-02, -4.0238e-02,\n",
      "          1.8466e-02,  4.0393e-02,  3.4354e-02,  1.1736e-02,  3.6270e-02,\n",
      "          1.8159e-03, -3.8144e-02,  2.6284e-02, -7.7406e-03,  1.0982e-02,\n",
      "          9.2739e-03, -3.2668e-02, -5.4142e-04,  2.4328e-02, -1.2576e-02,\n",
      "         -3.9250e-02, -2.9500e-02,  2.8761e-02,  2.1490e-02,  3.8087e-02,\n",
      "          2.1692e-02, -3.0085e-02, -3.6060e-02,  6.7265e-03, -4.0775e-02,\n",
      "          4.8935e-03,  1.8281e-02,  2.4184e-04,  3.4991e-02,  3.9980e-02,\n",
      "          6.5082e-03,  2.3382e-02, -3.2620e-02,  2.9384e-02,  2.0013e-02,\n",
      "         -1.0126e-02, -1.2118e-02, -3.9948e-02, -3.2427e-02, -9.6708e-03,\n",
      "          2.2412e-02, -2.3229e-02, -1.0626e-02,  8.1445e-03, -1.5920e-02,\n",
      "          2.6049e-02, -3.7901e-02, -3.4762e-02,  2.9697e-03,  1.9695e-02,\n",
      "         -3.7097e-02,  1.0315e-02,  3.0322e-02,  3.7440e-02,  3.6278e-02,\n",
      "          4.9629e-03,  1.4593e-02,  1.0691e-02, -6.5847e-03,  3.0156e-02,\n",
      "          7.8888e-03, -2.9733e-02,  3.9356e-02, -3.8066e-02, -3.6087e-05,\n",
      "          3.8112e-02,  2.7094e-02, -2.9101e-02,  5.6984e-03, -4.7967e-03,\n",
      "         -7.9654e-03, -7.9925e-03,  3.0177e-02,  3.5002e-02,  3.7016e-02,\n",
      "         -5.7469e-03, -1.9656e-02, -1.3885e-02, -3.0941e-02, -2.0634e-02,\n",
      "          1.1884e-06, -2.6582e-02, -3.0120e-02,  3.1894e-02,  2.1473e-02,\n",
      "          1.0579e-03,  1.1428e-02,  2.8516e-02, -2.5109e-03, -2.2079e-02,\n",
      "          7.7942e-03,  2.4569e-02, -3.0664e-02,  1.2882e-03,  3.6691e-02,\n",
      "         -1.1414e-02, -1.1329e-02, -1.8733e-02,  1.8220e-02,  3.5026e-02,\n",
      "         -2.4023e-03,  2.3113e-02,  3.2855e-02, -7.3870e-04, -5.8913e-03,\n",
      "         -3.3414e-02,  3.6516e-02,  2.2900e-03, -3.1446e-02, -3.9456e-02,\n",
      "          2.2444e-02,  3.7617e-02, -3.1340e-02, -2.5412e-03, -3.3892e-02,\n",
      "          2.5485e-02, -1.5189e-02,  7.2069e-04,  4.0419e-02,  5.4209e-04,\n",
      "         -3.6646e-02,  3.4379e-02, -3.2752e-03,  3.8101e-02, -1.7109e-02,\n",
      "         -3.1101e-02,  1.4210e-02,  3.5154e-02,  3.4308e-02,  6.2058e-03,\n",
      "          1.8597e-02, -6.0083e-04, -2.2924e-02, -3.1774e-02,  2.1390e-02,\n",
      "          3.8070e-02,  1.0344e-03,  2.1994e-03,  1.0293e-02,  3.8692e-02,\n",
      "          2.2468e-02,  1.7961e-02,  2.3481e-02, -2.4829e-02, -9.3555e-03,\n",
      "          7.2393e-03, -2.3439e-02, -2.3814e-02, -3.2574e-02, -1.1028e-03,\n",
      "          3.7860e-02,  4.0023e-02,  2.3630e-02,  1.1233e-02, -2.0067e-02,\n",
      "          2.2658e-02, -1.3585e-02, -3.0003e-02,  4.0683e-02,  3.1071e-02,\n",
      "          1.4823e-02, -5.5305e-03,  2.0654e-02, -2.4097e-02, -9.1392e-03,\n",
      "         -5.4998e-03, -6.2910e-03,  2.2019e-02, -3.1913e-02,  2.4639e-02,\n",
      "         -6.0289e-03,  1.3782e-02, -3.6272e-02, -1.8857e-02, -7.7424e-03,\n",
      "         -1.0463e-02, -2.8926e-02,  3.8187e-02,  2.0422e-02, -2.8474e-02,\n",
      "          1.9861e-02,  3.8073e-03, -7.9086e-03, -2.3696e-03,  6.4520e-03,\n",
      "          3.7895e-02, -2.3415e-03,  3.6497e-02,  2.7371e-02,  2.6295e-02,\n",
      "         -3.0898e-02, -6.4888e-03, -5.7074e-03, -3.7358e-02, -3.3849e-02,\n",
      "          2.3363e-02, -1.1540e-03, -2.4389e-02, -1.2773e-02,  3.5837e-02,\n",
      "         -3.4395e-02,  1.4451e-02, -3.2525e-02, -1.9813e-02, -2.8307e-02,\n",
      "         -2.1808e-02, -7.2457e-03, -3.9395e-02, -9.5904e-03, -1.2817e-02,\n",
      "         -3.2750e-02, -2.0069e-02, -4.0465e-02,  3.1452e-02,  2.6017e-02,\n",
      "         -1.7914e-02, -1.9162e-02,  3.5856e-02,  1.9164e-02,  1.8590e-02,\n",
      "         -2.2449e-02,  4.0777e-02, -9.4453e-03, -3.1457e-02, -1.4628e-02,\n",
      "         -1.8968e-02, -8.8705e-03, -6.5309e-03,  7.4498e-03,  2.4554e-02,\n",
      "          3.2800e-02,  1.7984e-02, -3.1817e-02,  3.9742e-02,  1.1944e-02,\n",
      "          8.1868e-03,  3.9420e-02,  3.3015e-02, -2.0054e-02, -1.3916e-02,\n",
      "          3.4907e-03, -8.3836e-03, -1.5702e-02, -3.6062e-02,  3.5884e-02,\n",
      "         -2.5433e-02, -2.2801e-02, -1.5651e-03, -2.3450e-02,  2.8171e-02,\n",
      "          2.5226e-02,  2.4704e-02, -2.5562e-02, -2.2900e-02, -2.7809e-02,\n",
      "         -2.9480e-02, -2.1071e-02,  3.4699e-02,  1.1130e-02,  2.1623e-02,\n",
      "         -2.2291e-02, -3.9808e-02,  8.5584e-03, -2.5280e-02, -3.4160e-03,\n",
      "          1.2617e-02,  8.2235e-03, -1.3720e-02,  1.9658e-02, -3.7324e-02,\n",
      "         -3.9785e-04, -2.1106e-03,  1.7612e-02,  3.8785e-02, -5.9658e-03,\n",
      "         -3.6809e-03, -3.8546e-02, -2.6087e-02, -2.3543e-02,  1.4135e-02,\n",
      "          8.6093e-04,  3.7010e-02,  9.0040e-03,  3.6977e-02,  2.0618e-02,\n",
      "         -2.8354e-02,  6.0707e-03,  6.2764e-03,  9.7639e-04, -3.3824e-02,\n",
      "          3.0188e-03,  3.3481e-02,  3.2110e-02,  8.2307e-03,  4.0823e-02,\n",
      "          3.2685e-02,  3.1159e-02, -3.5070e-02,  2.0406e-02, -2.1299e-02,\n",
      "         -2.8979e-02,  1.9945e-02, -3.0920e-02, -2.5227e-02,  2.6432e-02,\n",
      "         -1.5720e-02, -1.6382e-02,  1.6838e-02,  1.9186e-02,  3.6871e-02,\n",
      "         -2.3634e-02, -3.4317e-02, -3.6768e-02,  4.6352e-03, -1.7333e-02,\n",
      "          3.0877e-02, -1.1518e-02, -9.7030e-03,  4.2428e-03, -1.9264e-03,\n",
      "          3.1685e-03, -3.3725e-02, -6.1559e-03,  2.4656e-02, -4.8309e-03,\n",
      "         -3.8064e-02, -1.3668e-02,  2.1516e-02,  1.2069e-02, -3.2449e-02,\n",
      "          8.7758e-03,  1.7480e-02,  4.0385e-02, -1.0299e-02, -3.9301e-02,\n",
      "          1.9107e-02, -1.7159e-03,  1.0540e-02, -2.0860e-02,  3.9430e-02]])\n",
      "---> tensor([[ 7.1711e-02,  2.4224e-02,  1.4169e-01, -3.3104e-02,  9.4045e-03,\n",
      "          3.9628e-02, -3.0812e-02,  3.2131e-02,  8.1913e-02, -3.4023e-02,\n",
      "         -2.1658e-02,  1.9310e-02, -7.3426e-02, -4.7471e-02,  3.5770e-02,\n",
      "         -1.4116e-02,  1.0819e-02,  4.3910e-02, -1.8074e-02, -1.6532e-02,\n",
      "         -9.9859e-04,  3.7191e-02,  2.8585e-02,  5.1316e-02,  5.3539e-02,\n",
      "          3.9441e-02, -7.5218e-02, -8.1871e-02,  6.1965e-02, -5.5089e-02,\n",
      "          8.7573e-02,  1.7699e-02, -3.7717e-02, -6.0717e-02,  3.3279e-02,\n",
      "          2.1431e-02, -7.9359e-02, -2.4559e-02, -6.4475e-02,  1.2293e-03,\n",
      "         -1.6456e-02,  2.8457e-02,  1.9029e-01, -1.5731e-02, -2.6051e-02,\n",
      "          7.0318e-02,  6.3268e-02, -1.2752e-02,  3.8529e-02, -3.8033e-02,\n",
      "         -8.7047e-02,  8.1470e-02, -7.8293e-02, -8.1079e-02,  2.0786e-02,\n",
      "          6.3957e-02,  2.7919e-02,  5.5655e-02, -2.9894e-02, -4.9247e-02,\n",
      "         -4.6494e-02,  1.1128e-01, -6.6124e-02, -2.5132e-02,  2.3435e-02,\n",
      "         -3.8978e-02,  2.0207e-02, -1.1425e-02, -7.1113e-02,  1.9191e-02,\n",
      "         -1.1498e-02,  6.4409e-02,  5.0856e-02, -1.3573e-01,  1.0317e-02,\n",
      "         -4.0437e-02, -3.2415e-02, -3.1794e-02, -8.7666e-02, -7.5872e-02,\n",
      "          7.2736e-03,  1.5891e-01, -7.8166e-02,  1.5755e-03,  3.2465e-02,\n",
      "         -5.7043e-02, -1.3282e-02,  9.0794e-03, -6.7401e-03, -7.5777e-03,\n",
      "          3.4001e-02,  7.3972e-03,  9.4276e-03, -4.9149e-02,  7.2876e-02,\n",
      "          2.6933e-03,  5.1422e-02,  5.5403e-03,  7.5056e-02,  1.4128e-01,\n",
      "         -2.5597e-02,  4.9933e-02,  3.1205e-03, -2.1752e-02,  6.5205e-02,\n",
      "          2.6980e-02,  1.4304e-02, -2.8975e-02, -5.6366e-03,  5.4934e-02,\n",
      "         -5.5675e-02, -3.4110e-02,  1.1830e-02,  2.4411e-02, -1.8094e-02,\n",
      "         -5.5384e-02, -9.5239e-02,  1.5739e-01, -2.5626e-02, -9.6021e-02,\n",
      "         -5.1289e-02, -2.9807e-02, -3.5560e-02,  9.6475e-02, -6.2131e-02,\n",
      "          8.9641e-02, -2.7213e-02,  1.6937e-03, -3.4117e-02, -6.9292e-03,\n",
      "         -2.7241e-02, -1.3886e-02, -7.8245e-02, -1.5351e-03,  5.9327e-02,\n",
      "          2.2015e-02,  8.3959e-02,  2.5141e-02, -8.5366e-02,  1.2442e-01,\n",
      "         -7.5707e-02, -4.8026e-02, -8.5631e-02,  5.8062e-03, -2.0351e-02,\n",
      "          6.4817e-02,  5.4963e-02,  3.0950e-02, -8.6405e-02, -4.0523e-02,\n",
      "         -1.5630e-02,  9.3099e-02,  2.4249e-02,  1.6543e-02, -8.3292e-02,\n",
      "          1.0318e-01,  2.8211e-02,  4.8318e-02, -1.7081e-02,  7.9229e-02,\n",
      "          1.1713e-01,  7.6369e-02,  1.2879e-02, -4.2296e-02, -2.1475e-02,\n",
      "         -1.8777e-02,  3.3424e-02,  2.2839e-02,  3.4243e-02, -5.2146e-02,\n",
      "          4.8133e-02,  2.4948e-02,  1.5198e-01,  4.4829e-02,  1.9407e-02,\n",
      "          4.5908e-02,  1.3858e-02, -8.5991e-02, -5.0839e-03,  6.2786e-02,\n",
      "          4.3232e-02, -1.6821e-03,  6.9491e-02, -4.4027e-02,  9.0301e-02,\n",
      "          5.5309e-02, -6.2535e-02, -6.5004e-02,  1.1193e-01,  5.6933e-02,\n",
      "         -4.3319e-02, -1.6843e-02, -4.6240e-02, -2.9246e-02, -1.0758e-01,\n",
      "         -4.4898e-02,  4.1444e-04, -9.6122e-03,  2.0573e-02,  8.7508e-02,\n",
      "          5.7960e-02, -4.4654e-02,  1.5242e-01,  4.5337e-02,  1.8233e-02,\n",
      "         -9.2785e-03,  1.5960e-02,  1.1168e-02,  4.8536e-02, -1.8829e-02,\n",
      "          3.1119e-02,  3.8397e-02, -9.1536e-02, -8.1518e-02,  4.5691e-02,\n",
      "         -6.2445e-03, -6.0031e-02,  2.1229e-02,  1.0038e-01, -1.6718e-02,\n",
      "          9.3341e-02, -8.7781e-02, -1.8796e-02,  6.6535e-02,  1.5802e-04,\n",
      "         -9.9193e-03, -2.9978e-02,  4.5582e-02,  4.6200e-02, -1.5372e-02,\n",
      "          4.3797e-02,  1.0666e-01,  2.3061e-02,  1.2058e-02, -1.7305e-02,\n",
      "         -2.6160e-02, -3.5284e-03,  2.9104e-04, -2.2085e-02,  8.0264e-02,\n",
      "         -3.8496e-02, -1.2249e-01, -2.7826e-02, -4.4689e-02,  3.4217e-03,\n",
      "         -4.6242e-02, -4.5701e-02, -1.3961e-02,  3.3057e-03,  9.1320e-02,\n",
      "         -3.9512e-02, -1.4168e-01,  9.2905e-02,  1.6686e-02, -5.9753e-02,\n",
      "         -1.5442e-02,  3.2580e-02,  5.4414e-02,  2.2760e-02,  1.9306e-02,\n",
      "         -4.0858e-02, -1.8943e-02,  7.2855e-03,  7.9922e-02, -8.6922e-02,\n",
      "         -2.7926e-02,  6.2198e-02,  8.4569e-03,  7.2011e-02,  1.3722e-02,\n",
      "          2.1539e-02, -7.3319e-03,  3.1642e-02, -4.7133e-02, -9.8123e-02,\n",
      "          8.8517e-03, -9.7340e-03, -2.3388e-02, -1.3493e-01, -5.9484e-03,\n",
      "          1.5321e-02,  5.0469e-02,  4.4274e-03, -6.6685e-02,  1.0517e-01,\n",
      "          5.9852e-02, -4.9993e-02, -5.5963e-02,  1.9373e-02,  2.4812e-02,\n",
      "          6.3226e-02, -1.2044e-01, -2.4059e-03,  5.6326e-02,  5.4498e-02,\n",
      "          1.3971e-02, -4.1758e-02,  9.6108e-03,  5.0350e-02,  1.6474e-02,\n",
      "          7.5485e-02, -2.1794e-02,  1.0071e-01,  2.1815e-02, -7.1260e-02,\n",
      "          4.1869e-02,  1.1013e-01,  9.8049e-02,  7.5594e-03,  1.3419e-02,\n",
      "         -1.6264e-02, -5.8038e-02, -2.0101e-02, -1.9320e-02, -2.2828e-02,\n",
      "          5.3255e-02,  3.7091e-02,  4.3741e-02,  2.6881e-02,  4.2504e-02,\n",
      "          7.0959e-02, -4.6702e-02, -5.0934e-02, -4.2387e-02, -3.4462e-02,\n",
      "          2.6785e-02, -3.0754e-02, -7.3837e-02, -4.5990e-02, -5.7450e-02,\n",
      "         -5.7362e-03, -6.9816e-02, -9.2699e-03, -4.5658e-02, -6.4816e-02,\n",
      "          9.4750e-04,  1.2162e-02, -2.1760e-02, -4.7899e-02,  8.8886e-02,\n",
      "          1.0482e-01, -2.4831e-02, -1.3975e-01,  5.9970e-02,  1.9613e-02,\n",
      "          5.1760e-02, -1.5403e-02,  7.7131e-02, -1.9214e-03,  1.4161e-04,\n",
      "         -2.9007e-02,  1.4991e-02, -7.7508e-02,  5.2381e-02,  4.0035e-02,\n",
      "         -6.9909e-02, -4.4678e-02,  1.9798e-02,  2.0687e-02, -8.0942e-02,\n",
      "          4.7907e-02, -5.6053e-02, -3.4856e-02,  3.6408e-02,  5.7031e-02,\n",
      "          4.1074e-02, -6.3850e-02, -1.6982e-02, -6.8937e-02,  7.9417e-02,\n",
      "          1.3838e-02, -8.3572e-02, -7.5005e-03,  1.0960e-01,  1.5016e-02,\n",
      "         -5.1638e-02, -2.9739e-02,  2.0063e-01, -4.3564e-02, -1.3613e-01,\n",
      "         -7.8699e-03, -6.2376e-02, -9.7206e-02,  5.0519e-02, -1.0744e-01,\n",
      "          5.9464e-02,  2.0163e-02,  4.5448e-02,  9.3448e-03,  1.0428e-02,\n",
      "          3.3948e-03, -1.0393e-02,  2.1352e-02,  5.4695e-02,  3.0944e-02,\n",
      "         -7.0942e-02, -1.5040e-02, -7.8225e-02,  5.7509e-02, -5.9013e-02,\n",
      "         -3.3677e-02, -2.5673e-03,  4.8541e-02, -8.3774e-02,  1.0992e-01,\n",
      "          5.9659e-02, -1.2658e-01, -2.0574e-02, -1.5748e-01, -6.8293e-03,\n",
      "         -4.0103e-02, -2.0355e-02,  1.3581e-01,  1.0258e-01, -7.4486e-02,\n",
      "          7.6370e-02,  2.2718e-02,  3.6857e-03, -6.4644e-03, -1.8528e-02,\n",
      "          9.5470e-02,  3.3199e-02, -1.8532e-03,  3.4520e-02, -6.7208e-02,\n",
      "         -8.7670e-02,  4.2608e-02, -1.1491e-02, -1.4785e-02, -5.9485e-02,\n",
      "          4.4703e-02,  2.3684e-02,  8.8396e-02,  1.6482e-02,  4.4235e-02,\n",
      "          7.1782e-03, -4.7343e-02, -1.2875e-02,  7.6958e-02, -5.8862e-02,\n",
      "         -4.2565e-02,  9.5502e-03, -1.0343e-03,  2.8708e-02,  7.3485e-02,\n",
      "         -2.3690e-02,  8.4649e-02,  2.5448e-02, -4.8317e-02, -1.0345e-01,\n",
      "          1.5828e-01,  9.1651e-02,  4.7017e-02,  3.7224e-02,  1.6992e-02,\n",
      "          4.0223e-02,  2.2222e-02, -6.3424e-02,  5.4046e-02, -6.3744e-02,\n",
      "         -2.2069e-02, -4.3148e-02, -2.0016e-02, -2.2650e-02,  1.4853e-01,\n",
      "          6.8086e-02, -8.1228e-02, -2.3095e-02,  5.3424e-02,  7.8062e-03,\n",
      "          4.9717e-02, -1.4487e-02,  2.0602e-02,  3.8577e-02, -4.6556e-02,\n",
      "         -4.9958e-02, -3.5238e-02, -1.3486e-02,  5.3828e-02, -4.5853e-02,\n",
      "          4.0012e-02,  4.7728e-02, -4.7383e-02,  1.3268e-01,  1.0813e-01,\n",
      "          2.3918e-02,  5.0412e-02, -2.5793e-02,  5.5453e-02, -3.1261e-02,\n",
      "         -2.5557e-02,  3.8030e-02, -2.8870e-02, -3.6287e-02, -3.2133e-02,\n",
      "          6.0712e-03,  5.9313e-02,  7.6693e-02,  4.3617e-02, -1.4428e-02,\n",
      "          8.4656e-02,  5.7515e-02, -7.6885e-03, -3.7013e-02,  1.7602e-02,\n",
      "         -1.6341e-02, -4.0321e-02, -3.9016e-02,  8.0652e-02, -2.2606e-02,\n",
      "         -1.5513e-02, -5.4712e-02, -3.0846e-02, -6.5934e-03, -8.9657e-02,\n",
      "         -3.2367e-03, -1.8851e-02,  6.0347e-02,  2.0995e-02, -3.4033e-02,\n",
      "          5.1126e-02,  9.2699e-02, -2.4374e-02,  2.5041e-02, -5.2177e-02,\n",
      "          3.1042e-02,  3.8442e-02, -5.5268e-02, -9.3829e-03,  3.3674e-02,\n",
      "         -1.3918e-02,  5.9779e-02,  4.2135e-02, -5.6241e-02,  6.1166e-02,\n",
      "          2.6255e-02, -3.0002e-02,  1.3180e-01, -2.0880e-02,  9.5368e-02,\n",
      "          1.1915e-02, -3.8733e-03, -1.6140e-03, -2.9783e-02, -6.8986e-02,\n",
      "         -3.2497e-02,  4.6048e-02,  3.8759e-02,  4.2095e-02, -1.7614e-02,\n",
      "         -5.8519e-03, -3.6611e-02,  6.7977e-02, -6.0198e-02,  1.0849e-02,\n",
      "          6.5321e-02, -2.4982e-02,  1.3903e-01, -1.3727e-01, -7.6253e-02,\n",
      "         -2.2949e-02,  5.5709e-02,  2.3365e-02,  4.2736e-02, -3.6708e-03,\n",
      "         -2.9839e-02,  3.9000e-02, -3.9964e-03, -1.4626e-01, -4.6783e-02,\n",
      "          2.5169e-02,  1.0028e-01, -7.6407e-02, -4.3391e-02,  5.0554e-02,\n",
      "          3.7915e-03,  1.7734e-02,  1.2959e-02, -3.2814e-02, -8.8495e-02,\n",
      "          1.2766e-01,  1.5174e-02,  9.3940e-02, -4.3202e-02,  9.0685e-03,\n",
      "         -4.6935e-02, -5.8054e-02, -4.7463e-02, -2.2941e-03,  6.9428e-02,\n",
      "          1.6339e-02, -3.9851e-02,  3.3316e-03, -8.9047e-03, -4.2731e-02,\n",
      "         -4.1398e-02, -6.9023e-02, -5.9456e-02,  7.1872e-02,  4.7966e-03]])\n",
      "--- torch.Size([900, 600]) tensor([[ 0.0103, -0.0281, -0.0042,  ..., -0.0480,  0.0311, -0.0299],\n",
      "        [ 0.0387,  0.0358, -0.0181,  ...,  0.0366,  0.0207,  0.0191],\n",
      "        [-0.0539,  0.0322, -0.0105,  ..., -0.0101,  0.0462,  0.0533],\n",
      "        ...,\n",
      "        [ 0.0364,  0.0347, -0.0157,  ...,  0.0022, -0.0201,  0.0435],\n",
      "        [-0.0229,  0.0213,  0.0111,  ..., -0.0292, -0.0001, -0.0373],\n",
      "        [-0.0185,  0.0044,  0.0013,  ...,  0.0002,  0.0343,  0.0228]])\n",
      "---> tensor([[-0.0132, -0.0022,  0.0249,  ...,  0.0268,  0.0830, -0.0197],\n",
      "        [ 0.0149,  0.0792, -0.0024,  ...,  0.0033, -0.0177,  0.0439],\n",
      "        [-0.0376, -0.0175, -0.0504,  ...,  0.0818,  0.1205, -0.0080],\n",
      "        ...,\n",
      "        [ 0.0756,  0.0331, -0.0529,  ...,  0.0176, -0.0698, -0.0742],\n",
      "        [-0.0187, -0.0470, -0.0072,  ...,  0.0641,  0.1384,  0.0416],\n",
      "        [-0.0358, -0.0043, -0.0564,  ...,  0.0166,  0.0286, -0.0400]])\n",
      "--- torch.Size([900, 300]) tensor([[ 0.0231,  0.0305,  0.0451,  ..., -0.0101,  0.0285, -0.0115],\n",
      "        [ 0.0237, -0.0475, -0.0105,  ..., -0.0115, -0.0575,  0.0304],\n",
      "        [ 0.0180,  0.0295, -0.0259,  ..., -0.0097,  0.0360, -0.0314],\n",
      "        ...,\n",
      "        [ 0.0018, -0.0367, -0.0244,  ...,  0.0478, -0.0269,  0.0304],\n",
      "        [-0.0043, -0.0325,  0.0039,  ...,  0.0191, -0.0371,  0.0543],\n",
      "        [ 0.0355, -0.0004,  0.0531,  ...,  0.0351, -0.0533,  0.0280]])\n",
      "---> tensor([[-0.0133,  0.0418,  0.0181,  ..., -0.0653, -0.0894,  0.1972],\n",
      "        [-0.0620, -0.1015, -0.0577,  ...,  0.0366,  0.0806,  0.0275],\n",
      "        [-0.0132,  0.0475,  0.0694,  ...,  0.0269, -0.1425, -0.1185],\n",
      "        ...,\n",
      "        [ 0.0529, -0.0092, -0.0414,  ...,  0.0065, -0.0920,  0.0308],\n",
      "        [-0.1319,  0.0151,  0.0452,  ...,  0.0501, -0.0567, -0.0208],\n",
      "        [ 0.0791, -0.0608,  0.0012,  ...,  0.0164, -0.1359,  0.0248]])\n",
      "--- torch.Size([900, 600]) tensor([[-0.0486, -0.0089,  0.0046,  ...,  0.0018, -0.0110,  0.0015],\n",
      "        [-0.0160,  0.0057,  0.0285,  ...,  0.0383, -0.0544,  0.0171],\n",
      "        [ 0.0016, -0.0111, -0.0172,  ..., -0.0065,  0.0139,  0.0284],\n",
      "        ...,\n",
      "        [-0.0527, -0.0309, -0.0028,  ..., -0.0247,  0.0443,  0.0364],\n",
      "        [-0.0467, -0.0521, -0.0509,  ..., -0.0094,  0.0469, -0.0564],\n",
      "        [ 0.0517, -0.0409, -0.0408,  ...,  0.0560,  0.0577, -0.0496]])\n",
      "---> tensor([[ 0.0274,  0.0798,  0.0210,  ...,  0.0053, -0.0565, -0.0524],\n",
      "        [-0.0426,  0.0278, -0.0538,  ..., -0.0095, -0.0573,  0.1013],\n",
      "        [ 0.0815, -0.0087, -0.0393,  ..., -0.0626,  0.0128, -0.0244],\n",
      "        ...,\n",
      "        [-0.0783, -0.0672,  0.0256,  ...,  0.0445,  0.0181, -0.0132],\n",
      "        [ 0.0207, -0.0571, -0.0515,  ...,  0.1127,  0.0096, -0.0680],\n",
      "        [-0.0338,  0.0820, -0.0163,  ..., -0.0071, -0.0424,  0.0374]])\n",
      "--- torch.Size([900, 300]) tensor([[ 0.0445, -0.0405, -0.0219,  ...,  0.0195, -0.0236, -0.0314],\n",
      "        [ 0.0545,  0.0409,  0.0486,  ...,  0.0231, -0.0435,  0.0135],\n",
      "        [-0.0142, -0.0287, -0.0361,  ...,  0.0577,  0.0471,  0.0186],\n",
      "        ...,\n",
      "        [-0.0281, -0.0099,  0.0278,  ..., -0.0285, -0.0388, -0.0564],\n",
      "        [-0.0182,  0.0414,  0.0069,  ...,  0.0062,  0.0448, -0.0505],\n",
      "        [ 0.0186,  0.0078,  0.0284,  ...,  0.0298, -0.0228,  0.0429]])\n",
      "---> tensor([[-0.2227,  0.0969,  0.1307,  ...,  0.0339,  0.0848, -0.0170],\n",
      "        [ 0.0345,  0.0355, -0.1266,  ..., -0.0295,  0.0875, -0.0128],\n",
      "        [ 0.0013, -0.0284, -0.0317,  ...,  0.0478,  0.0861, -0.0831],\n",
      "        ...,\n",
      "        [ 0.0441,  0.0334, -0.1449,  ..., -0.0521,  0.1430, -0.1533],\n",
      "        [ 0.1671,  0.0506,  0.0401,  ...,  0.1843,  0.0898, -0.0618],\n",
      "        [ 0.0364,  0.0211,  0.0114,  ..., -0.0528,  0.0471,  0.0308]])\n",
      "--- torch.Size([600, 600]) tensor([[-0.0237, -0.0269,  0.0151,  ..., -0.0176,  0.0089, -0.0055],\n",
      "        [ 0.0090,  0.0350, -0.0346,  ..., -0.0375, -0.0051, -0.0075],\n",
      "        [-0.0363, -0.0395, -0.0392,  ...,  0.0331, -0.0004,  0.0334],\n",
      "        ...,\n",
      "        [-0.0058, -0.0132, -0.0383,  ..., -0.0010, -0.0153,  0.0220],\n",
      "        [ 0.0277, -0.0078, -0.0258,  ..., -0.0020, -0.0239, -0.0297],\n",
      "        [-0.0277,  0.0239,  0.0105,  ..., -0.0199, -0.0194, -0.0231]])\n",
      "---> tensor([[-0.1179,  0.0558,  0.0916,  ...,  0.0255,  0.0256, -0.0580],\n",
      "        [ 0.0137, -0.1098,  0.0411,  ...,  0.0859, -0.0188, -0.1127],\n",
      "        [-0.0366, -0.0525,  0.0802,  ..., -0.0230, -0.0324,  0.0918],\n",
      "        ...,\n",
      "        [-0.0071,  0.0016,  0.0995,  ...,  0.0128,  0.0641, -0.0241],\n",
      "        [-0.0072,  0.0007,  0.0312,  ...,  0.0520,  0.0220,  0.0508],\n",
      "        [ 0.0692, -0.0373, -0.0558,  ..., -0.0112, -0.0826, -0.0606]])\n",
      "--- torch.Size([1, 600]) tensor([[ 1.9055e-02, -3.6738e-03,  2.6919e-02, -6.4756e-03, -1.8661e-02,\n",
      "          3.5882e-02, -1.9473e-02, -1.8891e-02, -2.3891e-02,  3.3283e-03,\n",
      "          3.0172e-02,  2.1845e-02,  1.0944e-02, -1.3778e-02,  1.6533e-02,\n",
      "          1.4934e-02,  9.5231e-03, -2.6251e-02,  2.4063e-02,  1.4129e-02,\n",
      "         -3.3957e-02, -3.1946e-02,  3.1162e-02, -3.9228e-02,  2.3737e-02,\n",
      "          2.2455e-02,  6.7868e-03, -3.7820e-02, -1.2112e-02,  2.3283e-02,\n",
      "          3.1841e-02, -2.7549e-02,  8.0721e-03,  3.8561e-02, -2.3103e-02,\n",
      "         -4.3128e-03, -6.1739e-03, -3.4566e-02,  2.7212e-02, -3.8435e-02,\n",
      "         -3.5939e-02,  2.7453e-02,  3.2947e-02, -3.9669e-02,  8.3969e-03,\n",
      "         -5.4421e-03,  1.8068e-02,  2.5246e-02, -4.5820e-03, -3.1830e-02,\n",
      "          3.7481e-02, -1.6376e-02, -2.2270e-02, -2.9281e-02,  3.5421e-03,\n",
      "         -2.6057e-02,  1.0115e-03, -3.7918e-02,  1.7681e-02,  3.6283e-02,\n",
      "          2.2379e-02,  2.6963e-02, -2.0332e-02, -4.7791e-03,  1.8626e-02,\n",
      "          3.5378e-02,  2.8891e-02, -2.6930e-02,  3.2481e-02, -2.3215e-02,\n",
      "          2.9263e-03, -3.2895e-02, -4.3588e-03, -1.5197e-02,  1.6529e-02,\n",
      "          8.9209e-03, -2.0971e-02,  1.8002e-02, -2.1023e-02, -1.4816e-02,\n",
      "         -3.7722e-02,  2.9537e-02, -3.1585e-02,  1.8796e-02, -9.5064e-04,\n",
      "          2.9457e-02, -1.8514e-02, -1.2958e-02,  1.1296e-02, -3.2605e-02,\n",
      "         -2.8391e-02,  3.7852e-02,  2.4963e-02, -3.7537e-03,  1.5723e-02,\n",
      "          3.2575e-02,  1.9947e-03, -1.9563e-02, -7.5683e-03,  1.2162e-02,\n",
      "          3.3090e-02,  3.3189e-02,  1.3118e-03, -2.0246e-02, -3.7329e-02,\n",
      "          1.6851e-02, -1.5053e-02,  1.7288e-02,  2.7912e-02, -2.9100e-02,\n",
      "          1.2226e-02,  6.4948e-03, -2.8602e-02, -3.1761e-02,  8.9688e-03,\n",
      "         -1.3025e-02, -1.4895e-02, -3.1970e-02, -3.9338e-02, -3.4307e-02,\n",
      "         -2.4318e-02, -3.2087e-02,  5.7345e-03,  9.8258e-03, -2.7880e-02,\n",
      "         -3.7312e-02,  2.8487e-02,  3.7734e-02,  1.5210e-02,  9.6158e-03,\n",
      "          3.9124e-02,  3.6935e-02,  3.1796e-02,  2.9082e-02, -3.0070e-02,\n",
      "         -3.2560e-02, -2.8553e-02, -2.4196e-02, -2.7277e-02, -9.0846e-03,\n",
      "         -2.6063e-03,  1.9864e-02,  2.4179e-02, -2.4801e-02, -2.6352e-02,\n",
      "          7.9125e-03, -3.4200e-02,  3.9342e-02, -2.5735e-02,  8.9035e-03,\n",
      "          3.1115e-02,  3.4138e-02,  8.3880e-03, -2.3655e-02,  1.6511e-02,\n",
      "         -2.3036e-02,  2.7115e-02, -3.7118e-02, -2.4056e-02, -6.0300e-03,\n",
      "         -3.1746e-02, -5.2509e-03, -8.0873e-03, -8.1148e-03, -1.9278e-03,\n",
      "          3.8491e-02,  7.3363e-03,  3.7520e-02,  2.0396e-02,  2.2418e-02,\n",
      "          1.1056e-02,  3.9401e-02,  3.1048e-02,  3.4701e-03, -2.9135e-02,\n",
      "          7.3371e-03,  2.2851e-02, -1.4351e-02,  2.0213e-02,  2.8884e-02,\n",
      "         -1.9066e-02,  9.3521e-03, -3.3989e-03, -5.9844e-03, -1.9507e-02,\n",
      "          3.0194e-02,  2.4564e-03, -2.6632e-02, -3.8236e-02,  9.2023e-03,\n",
      "          2.9117e-02, -2.1596e-02, -9.3867e-03, -3.2871e-02, -3.3367e-02,\n",
      "         -2.1559e-02, -1.7208e-02,  2.7325e-02, -1.8598e-03, -1.9224e-02,\n",
      "          2.6506e-02, -3.6494e-02, -3.7636e-02,  2.3749e-02,  3.4677e-02,\n",
      "          1.5383e-02, -2.0025e-02,  1.0417e-02,  3.8920e-02,  1.3809e-02,\n",
      "         -3.2883e-02,  3.4334e-02, -1.7579e-02, -2.4264e-02, -2.8904e-02,\n",
      "          3.8632e-02,  2.6894e-02,  1.4718e-02, -2.3738e-02,  7.3558e-03,\n",
      "         -3.0805e-02, -3.9925e-02,  2.5783e-02, -2.8849e-02,  1.1183e-02,\n",
      "         -1.4678e-02, -7.7229e-03,  3.5949e-02,  3.6408e-03, -2.5270e-02,\n",
      "          1.0811e-02, -3.6504e-02,  1.7781e-02,  1.8335e-02,  4.2221e-03,\n",
      "         -4.0327e-03, -1.6948e-03, -2.6328e-02, -3.4378e-02, -2.4557e-02,\n",
      "         -2.3382e-02,  7.4172e-03,  2.0859e-03, -2.6422e-02, -2.9851e-02,\n",
      "          1.0983e-02, -3.1115e-02,  8.7081e-03,  1.2114e-02, -1.2892e-02,\n",
      "          3.4713e-02,  7.4051e-03,  4.5043e-03, -3.6432e-02, -1.3152e-02,\n",
      "         -6.0790e-03, -7.3040e-03,  1.6964e-02,  2.6194e-03,  3.5545e-02,\n",
      "          1.3830e-02, -3.9122e-02,  2.1959e-02,  3.1203e-02, -9.0615e-03,\n",
      "          1.7116e-02,  4.9923e-03,  2.5149e-02,  3.5840e-02, -9.1274e-03,\n",
      "          1.6701e-02, -2.5250e-02, -9.8480e-03,  8.6391e-03,  2.5735e-02,\n",
      "          4.4747e-03,  1.1036e-03, -9.0313e-03, -1.2001e-02, -3.4252e-02,\n",
      "          1.2024e-02,  2.6669e-03, -1.6793e-02, -3.7920e-02,  1.7272e-02,\n",
      "         -1.5484e-02,  1.9777e-02,  3.8854e-02, -3.6779e-02, -3.2517e-02,\n",
      "          3.1841e-02, -2.6452e-02, -1.9704e-02,  1.6920e-02,  4.0192e-02,\n",
      "          1.8868e-02,  3.8679e-02, -3.5679e-02,  2.5320e-03,  2.5681e-02,\n",
      "         -6.6636e-03,  3.0842e-02, -2.1435e-02,  3.4232e-02,  2.6010e-02,\n",
      "         -1.2481e-02, -1.6107e-02,  9.1394e-03,  2.5230e-02, -3.8469e-02,\n",
      "         -3.5842e-03,  5.7935e-03, -3.0179e-02,  3.1355e-02,  3.3338e-02,\n",
      "          2.2963e-02, -1.6795e-02,  1.0502e-02,  1.6906e-03, -1.4253e-02,\n",
      "          1.8211e-03, -8.6788e-04,  2.8524e-02,  2.9670e-02, -3.6909e-02,\n",
      "          4.2912e-04, -2.6925e-02,  3.5367e-02,  1.4268e-02, -2.3771e-02,\n",
      "          1.4633e-02,  1.1614e-02, -1.8699e-02,  7.6018e-05,  3.3502e-02,\n",
      "          2.7139e-03, -1.2052e-02, -2.0701e-02,  7.4411e-03,  3.7210e-02,\n",
      "          2.7172e-02, -3.8499e-02, -7.4915e-03, -2.9930e-02, -2.2856e-02,\n",
      "          1.1781e-02, -9.0611e-03,  2.7199e-02, -5.5889e-03,  1.1234e-02,\n",
      "          2.0175e-02, -9.4409e-03, -9.9147e-03, -2.9823e-02,  4.0330e-02,\n",
      "         -5.7871e-04,  1.7210e-02,  1.9838e-02,  7.6854e-03, -3.8877e-02,\n",
      "          1.3235e-02, -3.0556e-02, -3.1533e-02,  3.5702e-02, -6.1723e-03,\n",
      "         -2.5107e-02, -1.2422e-02, -1.6511e-02,  1.8059e-02, -4.7870e-03,\n",
      "          2.2503e-02, -3.5310e-02, -2.9868e-02,  2.7727e-03, -6.0877e-03,\n",
      "          1.3653e-02, -1.6825e-02, -9.4713e-03,  3.1672e-02, -2.2831e-02,\n",
      "          1.2378e-02,  9.9384e-03, -1.8052e-02,  9.7950e-04,  2.4145e-02,\n",
      "          3.4593e-02,  1.6365e-02, -3.1023e-02, -2.2185e-02, -2.9078e-02,\n",
      "         -3.4258e-02, -2.7371e-02, -4.6509e-04, -2.9085e-02, -1.7478e-02,\n",
      "         -2.4040e-02,  3.8895e-02, -6.9001e-03, -2.3602e-02, -9.2370e-03,\n",
      "          2.5680e-02,  7.8474e-03,  1.2007e-02, -2.4705e-02,  1.7694e-02,\n",
      "         -1.3130e-02, -3.6059e-02,  3.6677e-02,  1.5816e-02, -3.3055e-02,\n",
      "          3.0313e-02, -1.7319e-02,  3.4482e-02, -1.1524e-02, -2.7838e-02,\n",
      "         -3.7645e-02, -3.9424e-02,  1.3392e-02, -3.9403e-03, -4.4379e-03,\n",
      "          1.0805e-02,  2.6214e-02,  2.9725e-03,  1.1495e-02,  2.4600e-02,\n",
      "          3.7281e-03,  2.8529e-02, -1.1748e-02, -2.5630e-02, -3.4546e-02,\n",
      "          4.0270e-02, -6.8679e-03,  2.0944e-02, -1.5956e-03,  3.6399e-02,\n",
      "         -1.8648e-02,  1.9579e-03,  1.9999e-02,  1.6306e-02, -3.7273e-02,\n",
      "         -2.4720e-02,  2.1477e-02,  3.3944e-02,  2.0852e-02,  1.8339e-02,\n",
      "          3.7328e-02,  4.0922e-05, -2.8344e-02,  9.5906e-03,  1.1104e-02,\n",
      "         -3.9667e-02, -3.4994e-03,  1.7132e-02, -1.0345e-02, -2.7205e-02,\n",
      "         -1.6876e-02,  3.0869e-02, -4.7121e-03,  1.6092e-02, -3.7008e-02,\n",
      "         -9.2870e-03, -7.5512e-03,  3.2027e-02, -3.7946e-02,  5.5896e-03,\n",
      "         -1.4816e-02,  9.6164e-03,  1.1334e-02,  2.4525e-03,  7.3620e-03,\n",
      "          6.4971e-04,  6.1885e-03, -1.2370e-02,  2.3693e-02,  1.0926e-02,\n",
      "          1.8513e-02,  2.5155e-02,  1.0577e-02, -2.3397e-02,  9.4754e-03,\n",
      "          4.0138e-02,  2.3161e-02,  3.2114e-04, -3.3522e-02, -2.2982e-02,\n",
      "         -3.5430e-02,  2.3860e-02, -1.3072e-02,  1.8744e-02,  3.6277e-03,\n",
      "          2.8106e-02, -2.2047e-02,  3.7910e-02, -2.4770e-02, -1.2150e-02,\n",
      "         -1.4941e-02,  1.8054e-02, -1.7741e-02, -2.1601e-02, -1.6555e-02,\n",
      "         -8.6023e-03, -3.4543e-02, -1.9292e-03, -2.8095e-02, -1.4895e-02,\n",
      "          7.5102e-03,  2.3539e-02, -3.5139e-02, -1.4663e-02, -2.6447e-02,\n",
      "          3.7112e-02, -1.9633e-02, -2.1683e-02, -2.9427e-02,  1.7782e-02,\n",
      "          3.7171e-02,  1.3374e-03, -1.2563e-03, -3.5982e-02, -8.0177e-03,\n",
      "         -3.3824e-02,  1.7153e-02, -7.2608e-03, -2.9609e-02, -1.9150e-02,\n",
      "         -1.5328e-02, -3.0440e-02,  2.9420e-02,  9.6374e-03,  2.5367e-02,\n",
      "         -1.1938e-02, -3.0264e-02, -9.2421e-03,  2.0253e-02,  1.8444e-02,\n",
      "          5.1333e-03, -4.9886e-03,  1.7545e-02, -1.3111e-03, -3.9972e-02,\n",
      "         -2.2749e-03,  8.7128e-03,  3.6724e-02, -2.7460e-02, -1.5916e-02,\n",
      "          1.9632e-02,  1.4029e-02, -3.2239e-02,  2.6050e-02, -1.9626e-02,\n",
      "          1.9148e-02, -2.6854e-02, -3.0173e-02, -2.7943e-02, -1.5791e-02,\n",
      "          2.9378e-02, -1.8802e-02, -3.3468e-02,  1.6587e-02, -2.1436e-02,\n",
      "          3.1647e-02,  2.4804e-02,  6.3578e-03,  3.3494e-02,  2.8710e-02,\n",
      "         -8.4223e-04,  4.7907e-03,  3.5647e-02, -1.4977e-02,  3.5986e-02,\n",
      "         -2.0812e-03, -2.2967e-02, -1.5166e-02, -3.0294e-02,  3.7498e-02,\n",
      "         -3.4064e-02, -3.4603e-02,  6.1423e-03,  6.3283e-03, -2.1675e-02,\n",
      "         -1.4687e-02, -7.1764e-04,  2.5886e-02,  2.8300e-02,  2.2726e-04,\n",
      "         -1.7122e-02,  3.3809e-02, -1.9308e-02, -3.7472e-02,  3.1477e-03,\n",
      "          2.0642e-02,  3.4171e-02,  6.2390e-03, -1.8057e-02, -2.3431e-02,\n",
      "         -1.3711e-02,  1.9568e-02, -1.5833e-04, -3.7263e-03, -3.4041e-02]])\n",
      "---> tensor([[-9.1213e-03, -9.0327e-02, -5.4871e-02, -1.9326e-03,  1.0964e-01,\n",
      "          9.0017e-02, -4.5075e-02, -3.6825e-02,  1.0799e-01,  1.2938e-02,\n",
      "          3.7475e-02, -6.7585e-02, -5.2093e-02,  1.6138e-02, -1.1379e-01,\n",
      "         -8.9705e-02, -2.5897e-02, -4.4789e-02,  4.0925e-02, -5.6473e-02,\n",
      "         -6.0756e-02,  4.9132e-02,  2.2340e-02, -6.8998e-02,  5.7027e-02,\n",
      "          9.8694e-02, -2.2154e-02,  1.3016e-01, -8.5590e-02, -3.9163e-02,\n",
      "         -1.4470e-02, -6.0288e-02,  6.2981e-02, -6.4950e-02,  1.1110e-01,\n",
      "         -1.9732e-02,  2.7030e-02, -5.9379e-02,  3.0070e-02, -8.9399e-02,\n",
      "         -8.8916e-02, -3.5484e-02, -9.8686e-02, -2.5815e-03,  4.9006e-04,\n",
      "         -1.1606e-02, -3.4273e-02, -1.3396e-01, -1.1110e-01,  4.3975e-02,\n",
      "          5.3185e-02, -1.0788e-02, -2.0894e-03, -5.4779e-03,  2.4697e-02,\n",
      "         -9.7490e-02,  1.1674e-01,  5.4570e-02, -9.7007e-02,  1.0769e-02,\n",
      "         -3.7226e-02,  6.5158e-02, -2.6854e-02, -1.7187e-02, -5.9102e-02,\n",
      "          3.0946e-02,  1.3596e-01, -1.5690e-02, -3.3331e-02, -1.1051e-02,\n",
      "         -1.1796e-02, -3.5775e-03,  2.6551e-02,  1.1629e-02, -3.4719e-02,\n",
      "          3.6558e-02, -7.4962e-03, -1.4828e-02,  7.7906e-02,  5.0473e-02,\n",
      "          4.6989e-02,  1.5260e-01,  4.1105e-02, -3.4258e-02,  1.0044e-01,\n",
      "          1.0612e-01, -4.1888e-02,  4.1095e-02,  3.4398e-02,  1.7105e-03,\n",
      "          3.8563e-02,  2.0343e-02,  1.3366e-02, -5.7440e-02, -3.7721e-02,\n",
      "          1.3236e-01,  1.9169e-02,  1.7819e-02, -1.9136e-03, -1.1239e-02,\n",
      "         -1.5320e-02, -4.3226e-03,  6.0521e-02,  4.0851e-02,  8.0763e-02,\n",
      "         -9.2326e-04, -1.7557e-02,  8.1067e-02, -4.5214e-02,  2.9257e-02,\n",
      "          1.2044e-01,  2.0242e-03, -8.6406e-02, -8.5924e-02, -6.0563e-03,\n",
      "         -5.6895e-02,  1.4525e-02,  9.4978e-02,  3.8582e-02,  4.9842e-02,\n",
      "         -6.7534e-02,  8.8155e-02,  1.1076e-02, -5.9810e-03,  3.1533e-02,\n",
      "          3.5862e-02,  2.6120e-02, -5.5110e-02,  1.0259e-01, -8.7671e-02,\n",
      "         -7.1944e-02,  6.1232e-02,  1.4417e-02, -1.1094e-01,  2.7268e-02,\n",
      "         -4.8914e-02, -1.2054e-02, -4.6100e-02, -9.4190e-02, -2.8473e-02,\n",
      "          3.0407e-02, -4.5354e-03,  1.5155e-02, -5.4477e-03, -3.0748e-02,\n",
      "         -1.8627e-02,  5.1383e-02, -3.3617e-02, -7.1824e-02,  5.3347e-02,\n",
      "         -6.2441e-02, -4.3780e-02, -5.0950e-02,  1.1459e-02, -5.5779e-03,\n",
      "          7.5964e-02, -6.2830e-02, -4.5029e-02,  1.5345e-02, -9.5133e-03,\n",
      "          3.0235e-03,  1.5032e-02, -8.2110e-02, -1.0395e-01, -2.2248e-02,\n",
      "          7.7741e-02,  4.1321e-02,  2.2177e-02,  4.8938e-02, -3.4069e-02,\n",
      "         -2.7390e-02, -9.0015e-02, -5.3779e-02,  6.0428e-02,  3.0376e-02,\n",
      "         -4.9501e-02, -6.1376e-02,  3.8107e-02, -7.7131e-02,  4.0318e-02,\n",
      "          4.6624e-03,  3.7533e-02,  2.9921e-02,  1.1645e-02,  1.7185e-02,\n",
      "          8.1157e-02, -6.9952e-02,  5.4534e-02, -4.4953e-02, -1.0987e-01,\n",
      "          2.9472e-03,  3.4099e-02, -7.0564e-02,  2.2376e-02,  4.1311e-02,\n",
      "         -7.1414e-02, -8.6298e-02, -3.2035e-02,  2.5914e-03, -2.1639e-02,\n",
      "         -1.7583e-02, -1.2931e-01, -5.3213e-02,  1.8034e-02,  2.1190e-02,\n",
      "          4.3820e-03, -8.4743e-03,  6.5445e-02,  8.6238e-03, -7.9718e-02,\n",
      "         -1.2185e-02,  2.4806e-02, -8.6730e-02,  1.6981e-03, -7.5047e-02,\n",
      "         -1.3415e-01, -6.1068e-02,  1.3428e-01,  5.4834e-02,  9.0501e-03,\n",
      "         -5.9186e-02,  7.5365e-04, -9.1883e-02, -4.5437e-02, -4.0023e-02,\n",
      "         -2.0503e-02,  1.1996e-02, -1.3101e-02, -3.6079e-04,  2.2786e-02,\n",
      "         -1.1814e-01,  3.2366e-02, -4.4604e-02, -4.6555e-02,  4.7662e-02,\n",
      "          1.0209e-01,  1.0197e-01, -3.8898e-02,  6.6261e-03, -1.3166e-02,\n",
      "          6.5418e-02,  2.0857e-02, -8.3364e-02, -8.8690e-02, -1.1003e-01,\n",
      "          8.7478e-02,  1.5898e-02,  2.2489e-02,  9.7216e-02,  1.5725e-02,\n",
      "          2.8218e-02, -2.4946e-03, -6.1339e-02,  7.5012e-03, -3.0187e-02,\n",
      "          1.1600e-02,  4.6174e-03,  2.4580e-02, -4.0629e-02,  5.8612e-02,\n",
      "          5.6495e-02,  5.2307e-02,  6.3778e-02, -2.8270e-02,  1.1038e-01,\n",
      "         -2.2635e-02, -9.0790e-02, -5.0293e-02,  3.7288e-02, -8.0809e-02,\n",
      "          1.1914e-01,  1.3428e-02, -4.5202e-02,  6.3594e-02, -4.4252e-02,\n",
      "         -2.3922e-02, -7.1113e-02, -1.2195e-01, -1.3756e-02,  2.2667e-03,\n",
      "          1.3890e-02, -9.6597e-02,  1.9132e-02, -1.5555e-02,  3.0933e-02,\n",
      "         -1.8826e-03, -4.7224e-02, -8.5833e-03,  1.8901e-02,  6.5666e-02,\n",
      "          3.0599e-02, -3.4484e-02,  5.0946e-02, -7.2921e-02,  7.6056e-02,\n",
      "          3.2235e-02, -6.2100e-02, -6.4709e-02,  2.8282e-02, -1.0079e-01,\n",
      "          1.0299e-02,  5.1537e-02, -1.8366e-02, -1.9501e-02, -1.4456e-01,\n",
      "         -8.1741e-03, -2.6406e-02,  2.6630e-02, -1.1107e-01, -1.3983e-02,\n",
      "         -3.8111e-03,  5.4921e-03, -1.3602e-02, -5.1165e-02, -4.3431e-02,\n",
      "          3.5660e-02,  1.0476e-02, -7.5418e-02,  4.0667e-02,  6.9296e-02,\n",
      "         -3.0916e-02,  8.1653e-03,  5.6260e-02, -6.2517e-02, -5.3522e-02,\n",
      "         -1.1323e-02,  6.6237e-02, -4.3016e-02,  7.4841e-02, -5.9258e-02,\n",
      "          7.5722e-03,  9.3082e-02,  2.8351e-02,  7.9389e-03, -1.0028e-01,\n",
      "         -7.6618e-02, -1.4082e-01,  3.7812e-02,  7.9772e-02, -3.5545e-02,\n",
      "          5.6867e-02, -1.0127e-01,  8.8543e-02, -5.4674e-02,  6.2259e-02,\n",
      "         -6.0778e-03, -1.3950e-02,  6.0501e-02, -1.9464e-02, -1.7878e-02,\n",
      "          5.5468e-02, -3.5891e-02,  2.0533e-02,  4.1911e-02, -8.8621e-02,\n",
      "         -6.6004e-02, -1.3667e-02, -2.4549e-04, -2.6571e-02,  2.0809e-02,\n",
      "         -2.4494e-02, -7.1814e-02, -6.5372e-02, -2.6998e-02,  2.5257e-02,\n",
      "          9.6921e-02,  1.5621e-02, -1.1178e-02, -1.2339e-02,  5.7064e-02,\n",
      "          5.1080e-02,  4.1285e-02,  7.4496e-02, -5.9694e-02,  2.6872e-02,\n",
      "          2.9668e-02, -5.9952e-03,  7.5657e-02,  2.0899e-02,  7.9787e-02,\n",
      "          2.5315e-02, -1.7524e-03,  2.1686e-02, -9.3091e-02, -7.9634e-02,\n",
      "          8.2060e-02, -2.8243e-02,  7.1747e-02,  4.0762e-02,  1.2375e-02,\n",
      "         -1.1856e-01, -8.5081e-03, -1.0387e-01, -3.3189e-02,  2.3803e-02,\n",
      "         -4.5741e-02,  8.7447e-02, -6.6517e-02, -3.3019e-02, -1.7010e-03,\n",
      "         -5.7431e-02,  1.6993e-02, -3.5213e-02, -6.8781e-02, -1.2256e-01,\n",
      "         -1.3517e-01, -1.1660e-02,  5.1262e-02,  1.4489e-02, -1.2760e-02,\n",
      "          6.8920e-02,  4.5433e-02,  1.8772e-02, -1.3919e-01, -7.1610e-02,\n",
      "          5.6982e-03, -1.0227e-01, -1.2145e-01,  4.3616e-02,  2.8780e-02,\n",
      "         -4.3135e-02,  2.9586e-02, -5.1684e-02, -5.6706e-02,  9.5139e-02,\n",
      "          9.8638e-02, -3.2088e-03,  4.3410e-02, -7.3501e-03,  2.0441e-02,\n",
      "         -9.1022e-02,  3.1850e-02,  5.3458e-02,  1.1044e-01,  2.4376e-02,\n",
      "          2.7065e-02,  4.5420e-02,  2.9060e-02,  7.0751e-02,  8.4390e-02,\n",
      "          1.0534e-01, -3.8853e-02, -7.3956e-02, -4.0233e-02, -4.9913e-02,\n",
      "          3.7062e-02, -6.5338e-02, -3.0278e-03, -1.2427e-01,  2.7740e-02,\n",
      "          5.8188e-02,  6.8286e-02, -1.5214e-02, -7.0305e-02, -6.0838e-02,\n",
      "          1.7772e-02,  6.4990e-02,  4.7577e-02, -5.7007e-02,  2.7011e-02,\n",
      "         -3.4719e-02,  5.6798e-02,  3.1852e-02,  5.6330e-02, -8.1418e-02,\n",
      "         -1.1081e-01, -1.1443e-02, -6.4532e-02,  1.0142e-01,  1.9060e-02,\n",
      "         -3.3644e-02, -4.6820e-02,  3.0762e-02, -6.5448e-02,  1.0451e-02,\n",
      "         -2.3868e-02,  6.1817e-03, -3.8677e-03, -3.9250e-02, -1.2432e-02,\n",
      "         -5.8452e-03,  2.1874e-02, -6.6737e-02, -8.5069e-02, -5.6035e-02,\n",
      "         -4.0944e-02, -1.8526e-02,  7.4513e-02, -3.6321e-02, -3.4804e-02,\n",
      "         -1.0403e-01, -7.5522e-02, -7.7109e-02,  7.8129e-02,  7.9582e-02,\n",
      "          3.5803e-05, -1.2429e-03, -2.2324e-02,  1.6124e-02, -3.5706e-02,\n",
      "         -1.7315e-02,  7.5204e-02,  8.1008e-03,  7.6143e-03,  9.8229e-02,\n",
      "         -1.2256e-02, -1.6352e-02, -1.5334e-02,  1.7171e-02, -7.0076e-02,\n",
      "         -7.0937e-02,  3.1748e-02, -8.6508e-02,  3.1162e-02, -1.6583e-02,\n",
      "          2.6304e-02,  7.3684e-02, -9.6682e-03, -1.5700e-01,  1.3558e-01,\n",
      "         -3.7798e-03, -5.9900e-02,  3.1956e-02,  3.9251e-02,  5.1244e-02,\n",
      "         -1.0931e-01,  4.9512e-03,  5.1732e-02,  4.1249e-02,  4.4589e-02,\n",
      "         -1.8404e-02,  7.8404e-03, -2.6485e-02, -2.4789e-02,  2.5166e-02,\n",
      "         -8.3694e-02, -5.4919e-02,  6.4539e-03, -5.0806e-02,  3.9107e-02,\n",
      "         -7.0228e-02, -6.4387e-02, -7.5033e-02,  4.5786e-02,  6.0349e-03,\n",
      "         -3.5341e-02,  1.3994e-01, -1.7185e-02, -2.6095e-02, -2.6161e-02,\n",
      "         -7.6504e-02,  1.7558e-02,  5.2655e-02,  6.2410e-02, -5.4067e-02,\n",
      "         -7.6799e-03,  1.2696e-01, -5.5177e-02, -6.9048e-02, -4.5647e-02,\n",
      "         -3.3688e-02, -2.4316e-02, -2.8612e-02, -6.3249e-02,  3.1665e-02,\n",
      "          9.7515e-03, -1.6373e-02, -2.7257e-02,  5.8053e-02,  3.9646e-02,\n",
      "          7.3409e-02, -9.2668e-02, -7.5226e-02, -3.7092e-02, -6.5094e-02,\n",
      "         -2.6736e-02, -9.7473e-02,  2.2081e-02, -7.5871e-02, -2.8199e-02,\n",
      "          9.9360e-03,  1.5857e-02,  2.4490e-02,  1.1383e-02, -4.8902e-02,\n",
      "          4.8465e-02, -1.4216e-01, -1.6387e-03,  1.2314e-01, -3.6951e-02,\n",
      "         -1.8408e-02,  5.5245e-02,  3.5687e-02,  9.9811e-03,  1.5167e-02,\n",
      "         -9.5156e-02, -2.5015e-02, -3.1672e-02, -1.9618e-02, -3.5722e-02]])\n",
      "--- torch.Size([3, 600]) tensor([[ 0.0089, -0.0088,  0.0165,  ...,  0.0118, -0.0035,  0.0082],\n",
      "        [-0.0357,  0.0385, -0.0033,  ...,  0.0174, -0.0272,  0.0341],\n",
      "        [ 0.0051, -0.0136, -0.0184,  ..., -0.0043,  0.0275,  0.0241]])\n",
      "---> tensor([[-0.0458, -0.0739, -0.0069,  ..., -0.0236, -0.0266, -0.0545],\n",
      "        [ 0.0590, -0.0098, -0.0184,  ..., -0.0620,  0.0576, -0.1038],\n",
      "        [ 0.0244, -0.0299, -0.0906,  ..., -0.0550, -0.0260, -0.0030]])\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "Embedding(99999, 200)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 16
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": true
    },
    "id": "FbkwopxoTocq",
    "colab_type": "text"
   },
   "source": [
    "## 开始训练过程"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "j8MQ0-LBTocq",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def trainOneEpoch(epoch, model:HAN, trainLoader, optimizer:Optimizer, lossFunc):\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        lossFunc = lossFunc.cuda()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    startTime = time.time()\n",
    "    for i, (x, y) in enumerate(trainLoader):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        outputs = model(x)\n",
    "        loss = lossFunc(outputs, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 99:\n",
    "            print('Epoch %d, %d/%d, loss:%f ' % (epoch, i, len(trainLoader), loss))\n",
    "        # if i > 2000:\n",
    "        #     break \n",
    "    print('Epoch %d cost time: %.3fs' % (epoch, time.time() - startTime))\n",
    "\n",
    "\n",
    "def testModel(epoch, model:HAN, testLoader):\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    startTime = time.time()\n",
    "    for i, (x, y) in enumerate(testLoader):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        outputs = model(x)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += len(y)\n",
    "        correct += predicted.data.eq(y.data).cpu().sum().numpy()\n",
    "        \n",
    "\n",
    "        if i % 50 == 49:\n",
    "            print('Epoch Test %d, %d/%d, \\npredicted:%s, \\ntarget:%s' % (epoch, i, len(testLoader), \n",
    "                                                                     str(predicted),\n",
    "                                                                     str(y)))\n",
    "        # if i > 2000:\n",
    "        #     break \n",
    "    print('Epoch Test %d cost time: %.3fs' % (epoch, time.time() - startTime))\n",
    "    print('准确率： %.3f' % (correct / total))\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def train(nepoch, model, modelSavePath, isLoad, lr=0.0002):\n",
    "    last_acc = 0\n",
    "    if isLoad: \n",
    "      model.load_state_dict(torch.load(modelSavePath))\n",
    "      last_acc = testModel(epoch, model, testLoader)\n",
    "    # optimizer=torch.optim.SGD(model.parameters(), momentum=0.9, lr=0.001)\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.0002)\n",
    "    lossFunc =torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(nepoch):\n",
    "        trainOneEpoch(epoch, model, trainLoader, optimizer, lossFunc)\n",
    "        acc = testModel(epoch, model, testLoader)\n",
    "        if last_acc < acc:\n",
    "          torch.save(model.state_dict(), modelSavePath)\n",
    "        last_acc = acc\n",
    "    \n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eJmchTfhXtPJ",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "cf591776-cdcb-4c4a-bc1b-a343385e855e",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1587904662451,
     "user_tz": -480,
     "elapsed": 1083706,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "\n",
    "train(30, model, 'EmotionAnalyzeModelData_300_600.model', True, lr=0.0002)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gzvump0AVdJY",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "50931727-ed59-4b8f-fdd0-e69f46449fda",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1587883777914,
     "user_tz": -480,
     "elapsed": 2244765,
     "user": {
      "displayName": "fuliu fuliu",
      "photoUrl": "",
      "userId": "17240005970423735187"
     }
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "\n",
    "def eval(model, modelSavePath, isLoad = True):\n",
    "    if isLoad: model.load_state_dict(torch.load(modelSavePath))\n",
    "    testModel(0, model, testLoader)\n",
    "\n",
    "eval(model, 'EmotionAnalyzeModelData_300_600.model', isLoad = True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "wKXjE4UBTodM",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "lossFunc =torch.nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "_, predicted = torch.max(input.data, 1)\n",
    "output = lossFunc(input, target)\n",
    "print('input',input, '\\n target',target, '\\n output', output)\n",
    "output = lossFunc(input, predicted)\n",
    "print('predicted',predicted, '\\n target',target, '\\n output', output)\n",
    "\n",
    "print(predicted.data.eq(target).cpu().sum())\n",
    "print(target.data.eq(predicted).cpu().sum())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "Z1zQWh6DTodQ",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# 何凯明初始化\n",
    "\n",
    "w = torch.Tensor(3, 5, 2)\n",
    "print(w)\n",
    "print(nn.init.kaiming_uniform(w))\n",
    "print(w)\n",
    "w = torch.Tensor(3, 5, 2)\n",
    "print(w)\n",
    "print(torch.nn.init.kaiming_normal(w))\n",
    "print(w)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}